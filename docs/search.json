[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science at SAMY",
    "section": "",
    "text": "1 Welcome to the Data Science Handbook\nWelcome to our Data Science Handbook, your comprehensive guide to the methods, case studies, and best practices that define our approach to data science here at SAMY. This handbook is designed to be a dynamic resource for our team, evolving with new insights, tools, and technologies.\nWithin these pages, you‚Äôll find detailed case studies showcasing our successful projects, high-level concepts that underpin our strategies, and practical coding examples to help you apply these techniques in your own work. Irrespective of your experience in data science, this handbook aims to provide valuable insights and practical guidance to enhance your skills and knowledge.\nWe believe that sharing knowledge and continuously learning are key to staying ahead in the fast-paced world of data science. As such, this handbook is not just a static document but a collaborative space where ideas are exchanged, and innovation thrives.\nHappy data sciencing!\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf you have any questions at all, ask any member of the team. Whilst this Handbook aims to be a valuable resource for self-learning, it can often be more beneficial to spend 5 minutes talking through a concept with someone on the team who may be able to describe something in a different manner to this document.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Welcome to the Data Science Handbook</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2¬† The Data Science team",
    "section": "",
    "text": "2.1 Where we sit\nCapture Intelligence is our biggest internal ‚Äúclient‚Äù as there are plenty of opportunities to offer data science led services in the research offering of Capture. But also have our own core central pipe for development that supports all agency brands.\nWhat this means is as a team we have responsibilities that range from continual development of our own tech stack to help answer specific research questions for external clients to helping empower members of the alliance to use mindful applications of emerging technologies.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science team</span>"
    ]
  },
  {
    "objectID": "summary.html#where-we-sit",
    "href": "summary.html#where-we-sit",
    "title": "2¬† The Data Science team",
    "section": "",
    "text": "The Data Science department are a fully global resource within the alliance\n\n\n\n\n\nMike Tapp: Data Director\n\nJamie Hudson: Senior Data Scientist\n\nSophie Thomas: Jr.¬†Data Scientist\n\n\nJack Penzer: Global Data Product Lead\n\nAoife Ryan: Data Scientist\n\nCheryn Tan: Jr.¬†Data Scientist",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science team</span>"
    ]
  },
  {
    "objectID": "peaks_pits_workflow.html",
    "href": "peaks_pits_workflow.html",
    "title": "3¬† Peaks and Pits",
    "section": "",
    "text": "3.1 What is the concept/project background?\nStrong memories associated to brands or products go deeper than simple positive or negative sentiment. Most of our experiences are not encoded in memory, rather what we remember about experiences are changes, significant moments, and endings. In their book ‚ÄúThe Power of Moments‚Äù, two psychologists (Chip and Dan Heath) define these core memories as Peak and Pits, impactful experiences in our lives.\nBroadly, peak moments are experiences that stand our memorable in our lives in a positive sense, whereas pit moments are impactful negative experiences.\nMicrosoft tasked us with finding a way to identify these moments in social data- going beyond ‚Äòsimple‚Äô positive and negative sentiment which does not tell the full story of consumer/user experience. The end goal is that by providing Microsoft with these peak and pit moments in the customer experience, they can design peak moments in addition to simply removing pit moments.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Peaks and Pits</span>"
    ]
  },
  {
    "objectID": "peaks_pits_workflow.html#what-is-the-conceptproject-background",
    "href": "peaks_pits_workflow.html#what-is-the-conceptproject-background",
    "title": "3¬† Peaks and Pits",
    "section": "",
    "text": "The end goal\nWith these projects the core final ‚Äòproduct‚Äô is a collection of different peaks and pits, with suitable representative verbatims and an explanation to understand the high-level intricacies of these different emotional moments.\n\n\n\nScreenshot from a Peaks and Pits project showcasing the identified Peak moments for a product at a high level\n\n\n\n\nKey features of project\n\nThere is no out-of-the-box ML model available whose purpose is to classify social media posts as either peaks or pits (i.e.¬†we cannot use a ready-made solution, we must design our own bespoke solution).\nThere is limited data available\n\nUnlike the case of spam/ham or sentiment classification, there is not a bank of pre-labelled data available for us to leverage for ‚Äòtraditional ML‚Äô.\n\nDespite these issues, the research problem itself is well defined (what are the core peak and pit moments for a brand/product), and because there are only three classes (peak, pit, or neither) which are based on extensive research, the classes themselves are well described (even if it is case of ‚Äúyou know a peak moment when you see it‚Äù).",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Peaks and Pits</span>"
    ]
  },
  {
    "objectID": "peaks_pits_workflow.html#overview-of-approach",
    "href": "peaks_pits_workflow.html#overview-of-approach",
    "title": "3¬† Peaks and Pits",
    "section": "3.2 Overview of approach",
    "text": "3.2 Overview of approach\nPeaks and pits projects have gone through many iterations throughout the past year and a half. Currently, the general workflow is to use utilise a model framework known as SetFit to efficiently train a text classification model with limited training data. This fine-tuned model is then able to run inference over large datasets to label posts as either peaks, pits, or neither. We then utilise the LLM capabilities to refine these peak and pit moments into a collection of posts we are extremely confident are peaks and/or pits. We then employ topic modelling to identify groups of similar peaks and pits to help us organise and discover hidden topics or themes within this collection of core moments.\nThis whole process can be split into six distinct steps:\n\nExtract brand/product mentions from Sprinklr (the start of any project)\nObtain project-specific exemplar posts to help fine-tune a text classification model\nPerform model fine-tuning through contrastive learning\nRun inference over all of the project specific data\nUse GPT-3.5 for an extra layer of classification on identified peaks and pits\nTurn moments into something interpretable using topic modelling\n\n\n\n\nSchematic workflow from Project 706 - Peaks and Pits in M365 Apps\n\n\n\nObtain posts for the project (Step 1)\nThis step relies on the analysts to export relevant mentions from Sprinklr (one of the social listening tools that analysts utilise to obtain social data), and therefore is not detailed much here. What is required is one dataset for each of the brands/products, so they can be analysed separately.\n\n\nIdentify project-specific exemplar peaks and pits to fine-tune our ML model (Step 2)\nThis step is synonymous with data labelling required for any machine learning project where annotated data is not already available.\nWhilst there is no one-size-fits-all for determining the amount of data required to train a machine learning model, for traditional models and tasks, the number of examples per label is often in the region of thousands, and often this isn‚Äôt even enough for more complex problems.\nThe time required to accurately label thousands of peaks and pits to train a classification model in the traditional way is sadly beyond the scope of feasibility for our projects. As such we needed an approach that did not rely on copious amounts of pre-labelled data.\nThis is where SetFit comes in. As mentioned previously, SetFit is a framework that enables us to efficiently train a text classification model with limited training data.\n\n\n\n\n\n\nHow does it do this?\n\n\n\n\n\nNote the below is directly copied from the SetFit documentation. It is so succinctly written that trying to rewrite it would not do it justice.\nEvery SetFit model consists of two parts: a sentence transformer embedding model (the body) and a classifier (the head). These two parts are trained in two separate phases: the embedding finetuning phase and the classifier training phase. This conceptual guide will elaborate on the intuition between these phases, and why SetFit works so well.\nEmbedding finetuning phase\nThe first phase has one primary goal: finetune a sentence transformer embedding model to produce useful embeddings for our classification task. The Hugging Face Hub already has thousands of sentence transformer available, many of which have been trained to very accurately group the embeddings of texts with similar semantic meaning.\nHowever, models that are good at Semantic Textual Similarity (STS) are not necessarily immediately good at our classification task. For example, according to an embedding model, the sentence of 1) ‚ÄúHe biked to work.‚Äù will be much more similar to 2) ‚ÄúHe drove his car to work.‚Äù than to 3) ‚ÄúPeter decided to take the bicycle to the beach party!‚Äù. But if our classification task involves classifying texts into transportation modes, then we want our embedding model to place sentences 1 and 3 closely together, and 2 further away.\nTo do so, we can finetune the chosen sentence transformer embedding model. The goal here is to nudge the model to use its pretrained knowledge in a different way that better aligns with our classification task, rather than making the completely forget what it has learned.\nFor finetuning, SetFit uses contrastive learning. This training approach involves creating positive and negative pairs of sentences. A sentence pair will be positive if both of the sentences are of the same class, and negative otherwise. For example, in the case of binary ‚Äúpositive‚Äù-‚Äúnegative‚Äù sentiment analysis, (‚ÄúThe movie was awesome‚Äù, ‚ÄúI loved it‚Äù) is a positive pair, and (‚ÄúThe movie was awesome‚Äù, ‚ÄúIt was quite disappointing‚Äù) is a negative pair.\nDuring training, the embedding model receives these pairs, and will convert the sentences to embeddings. If the pair is positive, then it will pull on the model weights such that the text embeddings will be more similar, and vice versa for a negative pair. Through this approach, sentences with the same label will be embedded more similarly, and sentences with different labels less similarly.\nConveniently, this contrastive learning works with pairs rather than individual samples, and we can create plenty of unique pairs from just a few samples. For example, given 8 positive sentences and 8 negative sentences, we can create 28 positive pairs and 64 negative pairs for 92 unique training pairs. This grows exponentially to the number of sentences and classes, and that is why SetFit can train with just a few examples and still correctly finetune the sentence transformer embedding model. However, we should still be wary of overfitting.\nClassifier training phase\nOnce the sentence transformer embedding model has been finetuned for our task at hand, we can start training the classifier. This phase has one primary goal: create a good mapping from the sentence transformer embeddings to the classes.\nUnlike with the first phase, training the classifier is done from scratch and using the labelled samples directly, rather than using pairs. By default, the classifier is a simple logistic regression classifier from scikit-learn. First, all training sentences are fed through the now-finetuned sentence transformer embedding model, and then the sentence embeddings and labels are used to fit the logistic regression classifier. The result is a strong and efficient classifier.\nUsing these two parts, SetFit models are efficient, performant and easy to train, even on CPU-only devices.\n\n\n\nThere is no perfect number of labelled examples to find per class (i.e.¬†peak, pit, or neither). Whilst in general more exemplars (and hence more training data) is beneficial, having fewer but high quality labelled posts is far superior than more posts of poorer quality. This is extremely important due to the contrastive nature of SetFit where it‚Äôs superpower is making the most of few, extremely good, labelled data.\nOkay so now we know why we need labelled data, and we know what the model will do with it, what is our approach for obtaining the labelled data?\nBroadly, we use our human judgement to read a post from the current project dataset, and manually label whether we think it is a peak, a pit, or neither. To avoid us just blindly reading through random posts in the dataset in the hope of finding good examples (this is not a good use of time), we can employ a couple of tricks to narrow our search region to posts that have a reasonable likelihood of being suitable examples.\n\nThe first trick is to use the OpenAI API to access a GPT model. This involves taking a sample of posts (say ~1000) and running these through a GPT model, with a prompt that asks the model to classify each post into either a peak, pit, or neither. This is possible because GPT models have learned knowledge of peaks and pits from its training on large datasets. We can therefore get a human to only sense-check/label posts that GPT also believes are peaks or pits.\nThe second trick involves using a previously created SetFit model (i.e.¬†from an older project), and running inference over a similar sample of posts (again, say ~1000).\n\nWe would tend to suggest the OpenAI route, as it is simpler to implement (in our opinion), and often the old SetFit model has been finetuned on data from a different domain so it might struggle to understand domain specific language in the current dataset. However, be aware if it not as scalable as using an old SetFit model and has the drawback of being a prompt based classification of a black-box model (as well as issues relating to cost and API stability).\nIrrespective of which approach is taken, by the end of this step we need to have a list of example posts we are confident represent what a peak or pit moment looks like for each particular product we are researching, including posts that are ‚Äúneither‚Äù.\n\n\n\n\n\n\nWhy do we do this for each project?\n\n\n\n\n\nAfter so many projects now don‚Äôt we already have a good idea of what a peak and pit moment for the purposes of model training?\nEach peak and pit project we work on has the potential to introduce ‚Äòdomain‚Äô specific language, which a machine learning classifier (model) may not have seen before. By manually identifying exemplar peaks and pits that are project-specific, this gives our model the best chance to identify emotional moments appropriate to the project/data at hand.\nThe obvious case for this is with gaming specific language, where terms that don‚Äôt necessarily relate to an ‚Äòobvious‚Äô peak or pit moment could refer to one the gaming conversation, for example the terms/phrases ‚ÄúGG‚Äù, ‚Äúcamping‚Äù, ‚Äúscrub‚Äù, and ‚Äúgoat‚Äù all have very specific meanings in this domain that differ from their use in everyday language.\n\n\n\n\n\nTrain our model using our labelled examples (Step 3)\nBefore we begin training our SetFit model with our data, it‚Äôs necessary to clean and wrangle the fine-tuning datasets. Specifically, we need to mask any mentions of brands or products to prevent bias. For instance, if a certain brand frequently appears in the training data within peak contexts, the model could erroneously link peak moments to that brand rather than learning the peak-language expressed in the text.\n\nThis precaution should extend to all aspects of our training data that might introduce biases. For example, as we now have examples from various projects, an overrepresentation of data from ‚Äògaming projects‚Äô in our ‚Äòpeak‚Äô posts within our training set (as opposed to the ‚Äòpit‚Äô posts) could skew the model into associating gaming-related language more with peaks than pits.\n\nBroadly the cleaning steps that should be applied to our data for finetuning are:\n\nMask brand/product mentions\nRemove hashtags #Ô∏è‚É£\nRemove mentions üí¨\nRemove URLs üåê\nRemove emojis üêô\n\n\n\n\n\n\n\nWhat about other cleaning steps?\n\n\n\n\n\nYou will notice here we do not remove stop words-. As explained in the previous step, part of the finetuning process is to finetune a sentence embedding model, and we want to keep stop words so that we can use the full context of the post in order to finetune accurate embeddings.\n\n\n\nAt this step, we can split out our data into training, testing, and validation datasets. A good rule of thumb is to split the data 70% to training data, 15% to testing data, and 15% to validation data. By default, SetFit oversamples the minimum class within the training data, so we shouldn‚Äôt have to worry too much about imbalanced datasets- though be aware if we have extreme imbalanced we will end up sampling the same contrastive pairs (normally positive pairs) multiple times. However, our experimentation has shown that class imbalance doesn‚Äôt seem to have a significant effect to the training/output of the SetFit model for peaks and pits.\nWe are now at the stage where we can actually fine-tune the model. There are many different parameters we can change when fine-tuning the model, such as the specific embedding model used, the number of epochs to train for, the number of contrastive pairs of sentences to train on etc. For more details, please refer to the Peaks and Pits Playbook\nWe can assess model performance on the testing dataset by looking at accuracy, precision, recall, and F1 scores. For peaks and pits, the most important metric is actually recall because in step 4 we reclassify posts using GPT, so we want to make sure we are able to provide as many true peak/pit moments as possible to this step, even if it means we also provide a few false positives.\n\n\n\n\n\n\nClick here for more info as to why recall is most important\n\n\n\n\n\nAs a refresher, precision is the proportion of positive identifications that are actually correct (it focuses on the correctness of positive predictions) whereas recall is the proportion of actual positives that are identified correctly (it focuses on capturing all relevant instances).\nIn cases where false positives need to be minimised (incorrectly predicting a non-event as an event) we need to prioritise precision - if you‚Äôve built a model to identify hot dogs from regular ol‚Äô doggos, high precision ensures that normal dogs are not misclassified as hot dogs.\nIn cases where false negatives need to be minimised (failing to detect an actual event) we need to prioritise recall - in medical diagnoses we need to minimise the number of times a patient is incorrectly told they do not have a disease when in reality they do (or worded differently, we need to ensure that all patients with a disease are identified).\nTo apply this to our problem- we want to be sure that we capture all (or as many as possible) relevant instances of peaks or pits- even if a few false positives come in (neither posts that are incorrectly classified as peaks or pits). As we use GPT to make further peak/pit identifications, it‚Äôs better to provide GPT with with a comprehensive set of potential peaks and pits, including some incorrect ones, than to miss out on critical data.\n\n\n\n\nVisualise model separation\nAs a bonus, we can actually neatly visualise how well our finetuning of the sentence transformer embedding model has gone- by seeing how well the model is able to separate our different classes in embedding space.\nWe can do this by visualising the 2-D structure of the embeddings and see how they cluster:\nThis is what it looks the embeddings space looks like on an un-finetuned model:\n\n\n\nUn-finetuned embedding model\n\n\nHere we can see that posts we know are peaks, pits, and neither all overlap and there is no real structure in the data. Any clustering of points observed are probably due to the posts‚Äô semantic similarity (c.f. the mode of transport example above). We would not be able to nicely use a classifier model to get a good mapping from this embedding space to our classes (i.e.¬†we couldn‚Äôt easily separate classes here).\nBy visualising the same posts after finetuning the embedding model, we get something more like this, where we can see that the embedding model now clearly separates posts based on their peak/pit classification (though we must be wary of overfitting!).\n\n\n\nFinetuned embedding model\n\n\nFinally, now we are happy with our model performance based on the training and validation datasets, we can evaluate the performance of this final model using our testing data. This is data that the model has never seen, and we are hoping that the accuracy and performance is similar to that of the validation data. This is Machine Learning 101 and if a refresher is needed for this there are plenty of resources online looking at the role of training, validation, and testing data.\n\n\n\nRun inference over project data (Step 4)\nIt is finally time to infer whether the project data contain peaks or pits by using our fine-tuned SetFit model to classify the posts.\nBefore doing this again we need to make sure we do some data cleaning on the project specific data.\nBroadly, this needs to match the high-level cleaning we did during fine-tuning stage:\n\nMask brand/product mentions (using RoBERTa-based model [or similar] and Rivendell functions)\nRemove hashtags #Ô∏è‚É£\nRemove mentions üí¨\nRemove URLs üåê\nRemove emojis üêô\n\n\n\n\n\n\n\nNote on social media sources\n\n\n\n\n\nCurrently all peak and pit projects have been done on Twitter or Reddit data, but if a project includes web/forum data quirky special characters, numbered usernames, structured quotes etc. should also be removed.\n\n\n\nOkay now we can finally run inference. This is extremely simple and only requires a couple of lines of code (again see the Peaks and Pits Playbook for code implementation)\n\n\nThe metal detector, GPT-3.5 (Step 5)\nDuring step 4 we obtained peak and pit classification using few-shot classification with SetFit. The benefit of this approach (as outlined previously) is its speed and ability to classify with very few labelled samples due to contrastive learning.\nHowever, during our iterations of peak and pit projects, we‚Äôve realised that this step still classifies a fair amount of non-peak and pit posts incorrectly. This can cause noise in the downstream analyses and be very time consuming for us to further trudge through verbatims.\nAs such, the aim of this step is to further our confidence in our final list of peaks and pits to be actually peaks and pits. Remember before we explained that for SetFit, we focussed on recall being the most important measure in our business case? This is where we assume that GPT-3.5 enables us to remove the false positives due to it‚Äôs incredibly high performance.\n\n\n\n\n\n\nWhy not use GPT from the start?\n\n\n\n\n\nUsing GPT-3.5 for inference, even over relatively few posts as in peaks and pits, is expensive both in terms of time and money. Preliminary tests have suggested it is in the order of magnitude of thousands of times slower than SetFit. It is for these reasons why we do not use GPT-x models from the get go, despite it‚Äôs obvious incredible understanding of natural language.\n\n\n\nWhilst prompt-based classification such as those with GPT-3.5 certainly has its drawbacks (dependency on prompt quality, prompt injections in posts, handling and version control of complex prompts, unexpected updates to the model weights rendering prompts ineffective), the benefits include increased flexibility in what we can ask the model to do. As such, in the absence of an accurate, cheap, and quick model to perform span detection, we have found that often posts identified as peaks/pits did indeed use peak/pit language, but the context of the moment was not related to the brand/product at the core of the research project.\nFor example, take the post that we identified in the project 706, looking for peaks and pits relating to PowerPoint:\n\nThis brings me so much happiness! Being a non-binary graduate student in STEM academia can be challenging at times. Despite using my they/them pronouns during introductions, emails, powerpoint presentations, name tags, etc. my identity is continuously mistaken. Community is key!\n\nThis is clearly a ‚Äòpeak‚Äô, however it is not accurate or valid to attribute this memorable moment to PowerPoint. Indeed, PowerPoint is merely mentioned in the post, but is not a core driver of the Peak which relates to feeling connection and being part of a community. This is as much a PowerPoint Peak as it is a Peak for the use of emails.\nTherefore, we can engineer our prompt to include a caveat to say that the specific peak or pit moment must relate directly to the brand/product usage (if relevant).\n\n\nTopic modelling to make sense of our data (Step 6)\nNow we have an extremely refined set of posts classified as either peak or pits. The next step is to identify what these moments actually relate to (i.e.¬†identify the topics of these moments through statistical methods).\nTo do this, we employ topic modelling via BERTopic to identifying high-level topics that emerge within the peak and pit conversation. This is done separately for each product and peak/pit dataset (i.e.¬†there will be one BERTopic model for product A peaks, another BERTopic model for product A pits, an additional BERTopic model for product B peaks etc).\nWe implement BERTopic using the R package BertopicR. As there is already good documentation on BertopicR this section will not go into any technical detail in regards to implementation.\nFrom BertopicR. we end up with a topic label for each post in our dataset, meaning we can easily quantify the size of each topics and visualise temporal patterns of topic volume etc.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Peaks and Pits</span>"
    ]
  },
  {
    "objectID": "conversation_landscape.html",
    "href": "conversation_landscape.html",
    "title": "4¬† Conversation Landscape",
    "section": "",
    "text": "4.1 Project Background\nWorking with semi-structured or unstructured high-dimensional data, such as text (and in our case, social media posts), poses significant challenges in measuring or quantifying the language used to describe any specific phenomena. One common approach to quantifying language is topic modelling, where a corpus (or collection of documents) is processed and later represented in neater and simplified format. This often involves displaying top terms, verbatims, or threads highlighting any nuances or differences within the data. Traditional topic modelling or text analysis methods, such as Latent Dirichlet Allocation (LDA), operate on the probability or likelihood of terms or n-grams belonging to a set number of topics.\nThe Conversation Landscape workflow offers a slightly different solution and one that partitions text data without a specific need for burdening the user with sifting through rows of data in order to segment documents with hopes of understanding or recognising any differences in language, which would ideally be defined more simply as topics. The is mostly achieved through sentence transforming, where documents are converted from words to numerical values, which are often referred to as ‚Äòembeddings‚Äô. These values are calculated based on their content‚Äôs semantic and syntactic properties. The transformed values are then processed again using dimension reduction techniques, making the data more suitable for visualisation. Typically, this involves reducing to two dimensions, though three dimensions may be used to introduce another layer of abstraction between our data points. The example provided throughout this chapter, represents some text data as nodes upon a two-dimensional space.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conversation Landscape</span>"
    ]
  },
  {
    "objectID": "conversation_landscape.html#project-background",
    "href": "conversation_landscape.html#project-background",
    "title": "4¬† Conversation Landscape",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis documentation will delve deeper into the core concepts of sentence transforming and dimension reduction, along with the different methods used to cluster or group topics once the overall landscape is mapped out, referring back to our illustrated real-world business use case of these techniques. We will then later look at best practices and any downstream flourishes that will help us operate within this work-stream.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conversation Landscape</span>"
    ]
  },
  {
    "objectID": "conversation_landscape.html#final-output-of-project",
    "href": "conversation_landscape.html#final-output-of-project",
    "title": "4¬† Conversation Landscape",
    "section": "4.2 Final output of project",
    "text": "4.2 Final output of project\nAn ideal output, like the one shown below should always showcase the positioning of our reduced data points onto the semantic space, along with any topic or subtopic explanations alongside, using color coding where appropriate. While we sometimes provide raw counts of documents per topic/subtopic, we always include the percentage of topic distribution across our data, occasionally referred to as Share of Voice (SOV).\n\n\n\nScreenshot Taken from the Final Output of an AI Landscape Microsoft Project - Q2 FY24",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conversation Landscape</span>"
    ]
  },
  {
    "objectID": "conversation_landscape.html#how-to-get-there",
    "href": "conversation_landscape.html#how-to-get-there",
    "title": "4¬† Conversation Landscape",
    "section": "4.3 How to get there",
    "text": "4.3 How to get there\nAs promised, we will provide some more context as well as the appropriate information surrounding the required steps taken, so that a reader may replicate and implement the methods mentioned throughout so far, providing an efficient analysis tool to use for any set of documents, regardless of domain specifics. While the example output provided displays a simplified means for visualising complex and multifaceted noisy data such as the ‚ÄòArtificial Intelligence‚Äô conversation on social, there are a number of steps that one must take carefully and be mindful of throughout, in order to create the best fit model appropriate for a typical Conversation Landscape project.\nThe broad steps would include, and as one might find across many projects within the realms of Natural Language Processing (NLP):\n\nInitial Exploratory Data Analysis (EDA): Checking that the data is relevant and fit to answer the brief.\nCleaning and Processing: Removal of spam, unhelpful or irrelevant data, and pre-processing of text variable for embedding.\nTransforming/Embedding: Turning our words into numbers which will later be transformed again before being visualised.\nDimension Reduction: Reducing our representational values of documents to a manageable state in order to visualise.\nTopic Modelling/Clustering: Scientifically modelling and defining our data into a more digestible format.\n\n\nExploratory Data Analysis (EDA):\nWhether the user is responsible for data querying/collection or not, the first steps in our workflow should always involve some high-level checks before we proceed with any of the following steps in order to save time downstream and give us confidence to carry over into the data cleaning and processing steps and beyond.\nFirst, one should always check things like the existing variables and clean or rename any where necessary. This step requires a little forward thinking as to what columns are necessary to complete each stage of the project. Once happy with our variables, we can then check for things such as missing dates, and/or if there are any abnormal distributions across columns like ‚ÄòSocial Platform‚Äô that might skew any findings or help us understand or perhaps justify the resulting topic model. Next, we can do some bespoke or project specific checks like searching for likely-to-find terms or strings within our text variable to ensure that the data is relevant and query has captured the phenomena we are aiming to model.\n\n\nData Cleaning/Processing:\nAgain, as we may not always be responsible for data collection, we can expect that our data may contain unhelpful or even problematic information which is often the result of data being unwillingly bought in by the query. Our job at this stage is to minimize the amount of unhelpful data existing in our corpus to ensure our findings are accurate as well as appropriate for the data which we will be modelling.\nOptimal procedures for spam detection and removal are covered in more detail [here]will include link when data cleaning section is complete. However, there are steps the user absolutely must take to ensure that the text variable which will be provided to the sentence transformer model is clean and concise so that an accurate embedding process can take place upon our documents. This includes the removal of:\n\nHashtags #Ô∏è‚É£\nUser/Account Mentions üí¨\nURLs or Links üåê\nEmojis üêô\nNon-English Characters üâê\n\nOften, we might also choose to remove punctuation and/or digits, however in our provided example, we have not done so. There are also things to beware of such as documents beginning with numbers that can influence the later processes, so unless we deem them necessary we should remove these where possible to ensure no inappropriate grouping of documents takes place based on these minor similarities. This is because when topic modelling, we aim to capture the pure essence of clusters which is ultimately defined by the underlying semantic meaning of documents, as apposed to any similarities across the chosen format of said documents.\n\n\nSentence Transforming/Embedding:\nOnce we are happy with the cleanliness and relevance of our data, including the processing steps we have taken with our chosen text variable, we can begin embedding our documents so that we have a numerical representation that can later be reduced and visualised for each. Typically, and in this case we have used already pre-trained sentence transformer models that are hosted on Hugging Face, such as all-mpnet-base-v2 which is the specific model we had decided to use in our AI Conversation Landscape example. This is because during that time, the model had boasted great performance scores for how lightweight it was, however with models such as these being open-source, community-lead contributions are made to further train and improve model performance which means that these performance metrics are always increasing, so one may wish to consult the Hugging Face leaderboard, or simply do some desk research before settling on an ideal model appropriate for their own specific use case.\nWhile the previous steps taken might have involved using R and Rstudio and making use of SHARE‚Äôs suite of data cleaning, processing and parsing functionality, the embedding process will need to be completed using Google Colab. This is to take advantage of their premium GPUs and high RAM option, as embedding documents can require large amounts of compute, so much so that most fairly competent machines with standard tech specs will struggle. It is also worth noting that an embedding output may depend on the specific GPU being utilized as well as the version of Python that Colab is currently running, it‚Äôs good practice to make note of both of these specifics, along with other modules and library versions that one may wish to use in the same session, such as umap-learn (you may thank yourself at a later stage for doing so). To get going with sentence transformers and for downloading/importing a model such as all-mpnet-bas-v2, there are step-by-step guides purposed to enable users with the know-how to use them and deal with model outputs upon the Hugging Face website.\n\n\nDimension Reduction:\nAt this stage, we would expect to have our data cleaned along with the representative embeddings for each document, which is output by the sentence transforming process. This next step, explains how we take this high-dimensional embeddings object and then simplify/reduce columns down enough to a more manageable size in order to map our documents onto a semantic space. Documents can then be easily represented as a node and are positioned within this abstract space based upon their nature, meaning that those more semantically similar will be situated closer together upon our two (or sometimes three-dimensional) plot, which then forms our landscape.\nThere are a number of ways the user can process an embeddings output. Each method has its own merits as well as appropriate use cases, which mostly depend whether the user intends to focus on either the local or global structure of their data. For more on the alternative dimension reduction techniques, the BERTopic documentation provides some further detail while staying relevant to the subject matter of Topic Modelling and NLP.\nOnce we have reduced our embeddings, and for the sake of staying consistent to the context of our given example, lets say we have decided to use Uniform Manifold Approximation and Projection (UMAP), a technique which is helpful for when we wish to represent both the local and global structures of our data. The output of this step should have resulted in taking our high dimensional embedding data (often 768 columns or sometimes more) and reduced these values down to just 2 columns so that we can plot them onto our semantic space (our conversation landscape plot), using these 2 reduced values as if to serve as X and Y coordinates to appropriately map each data point, we often name these two columns V1 and V2.\nAt this stage, we can use the LandscapeR package to render a static visualisation of the entire landscape, and we can select the desired colour of our nodes by making use of the fill_colour parameter. In this instance, we‚Äôve mapped our documents onto the semantic space represented as nodes using columns V1 and V2 and coloured them a dark grey.\n\ndata %&gt;% \n  LandscapeR::ls_plot_static(x_var = V1,\n                             y_var = V2,\n                             fill_colour = \"#808080\")\n\n\n\n\nGrey Colourless Landscape Plot from an AI Landscape Microsoft Project - Q2 FY24\n\n\nIt‚Äôs worth pointing out, that there are a number of ways for the user to interactively explore the landscape at this stage by scanning over each node, checking the documents contents. This helps the user to familiarise with each region of the landscape before clustering. The plotly package serves as a user friendly means for this purpose, helping us gather a ‚Äòlay of the land‚Äô and identify the dense and not so dense sections of our data.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis shows just a 20K sample from our data, which is done only to comply with data size limits on GitHub and to be more conservative with compute and memory usage. Here, we also use a message column with breaks every 10 words to ensure the visual is neater.\n\n\n\n\ndata %&gt;% plotly::plot_ly(\n  x = ~V1,\n  y = ~V2,\n  type = 'scatter',\n  mode = 'markers',\n  marker = list( color = '#808080', size = 1),\n  text = ~paste('Message: ', message_with_breaks),\n  hoverinfo = 'text'\n)\n\n\n\n\n\n\n\nTopic Modelling/Clustering:\nThe final steps taken are arguably the most important, this is where we will define our documents and simplify our findings byway of scientific means, in this case using Topic Modelling.\nThere are a number of algorithms that serve this purpose, but the more commonly used clustering techniques are KMeans and HDBSCAN. However, the example we have shown uses KMeans, where we define the number of clusters that we would expect to find beforehand and perform clustering on either the original embeddings object output by the sentence transformer model, or we can reduce those embeddings to something much smaller like 10 dimensions and cluster documents based on those. If we were to opt for HDBSCAN however, we would allow the model to determine how many clusters were formed based on some input parameters such as min_cluster_size which are provided by the user. For more on these two techniques and when/how to use them in a topic modelling setting, we can consult the BERTopic documentation once more.\nIt‚Äôs also worth noting that this step requires a significant amount of human interpretation, so the user can definitely expect to partake in an iterative process of trial and error, trying out different values for the clustering parameters which determine the models output, with hopes of finding the model of best fit, which they feel accurately represents the given data.\nIn practise, this visualisation can be derived using our original data object with topic/cluster information appended, as well as the original V1 and V2 coordinates that we had used previously. To ensure our topics are coloured appropriately, we can create and use a named character vector and some additional ggplot2 syntax to manually assign topics with specific hex codes or colours.\n\n# assign colours to topics\ntopic_colours &lt;- c(\"Ethics & Regulation\" = \"#C1E1C1\",\n                   \"Technological Innovations\" = \"#6e88db\", \n                   \"AI in an Artistic Domain\" = \"#7e2606\",\n                   \"Cultural & Social Impact\" = \"#ff6361\",\n                   \"Business & Wider-Markets\" = \"#063852\",\n                   \"Future of Learning & Personal Growth\" = \"#ffa600\",\n                   \"Future of Work & Security\" = \"#9e1ad6\"\n                   )\n\n\ndata %&gt;% \n  LandscapeR::ls_plot_group_static(x_var = V1,\n                                   y_var = V2,\n                                   group_var = topic_name) +\n  ggplot2::scale_colour_manual(values = topic_colours) # colour nodes manually\n\n\n\n\nSegmented Colourful Landscape Plot from an AI Landscape Microsoft Project - Q2 FY24",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conversation Landscape</span>"
    ]
  },
  {
    "objectID": "conversation_landscape.html#downstream-flourishes",
    "href": "conversation_landscape.html#downstream-flourishes",
    "title": "4¬† Conversation Landscape",
    "section": "4.4 Downstream Flourishes",
    "text": "4.4 Downstream Flourishes\nWith the basics of each step covered, we will now touch on a few potentially beneficial concepts worth grasping that may help us overcome anything else that may occur when working within the Conversation Landscape project domain.\n\nModel Saving & Reusability:\nOccasionally, a client may want us to track topics over time or perform a landscape change analysis. In these cases, we need to save both our Dimension Reduction and Clustering models so that new data can be processed using these models, tp produce consistent and comparable results.\nThis requires careful planning. When we initially reduce embeddings and perform clustering, we use the .fit() method from sklearn when either reducing the dimensions of or clustering on the original embeddings. This ensures that the models are trained on the data they are intended to represent, making future outputs comparable.\nWe had earlier, mentioned, that it is crucial to document the versions of the modules and Python interpreter used. When we reduce or cluster new data using our pre-fitted models, it is essential to do so with the exact same versions of important libraries and Python. The reason is that the internal representations and binary structures of the models can differ between versions. If we attempt to load and apply previously saved models with different versions, we risk encountering incompatibility errors. By maintaining version control and documenting the environment in which the models were created, we can ensure the reusability of our models. Overall, this practice allows for us to be accurate when tracking and comparing topics and noting any landscape changes.\n\n\nEfficient Parameter Tuning:\nWhen we‚Äôre performing certain steps within this workflow, more specifically the Dimension Reduction with likes of UMAP, or if we were to decide we‚Äôd want to cluster using HDBSCAN for example, being mindful of and efficient with tuning the different parameters at each step will definitely improve the outcome of our overall model. Therefore, understanding these key parameters and how they can interact will significantly enhance the performance of the techniques being used here.\n\nDimension Reduction with UMAP:\nn_neighbors: This parameter controls the local neighborhood size used in UMAP. A smaller value focuses more on capturing the local structure, while a larger value considers more global aspects. Efficiently tuning this parameter involves considering the nature of your data and the scale at which you want to observe patterns.\nmin_dist: The min distance argument determines quite literally how tight our nodes are allowed to be positioned together within our semantic space, a lower value for this will mean nodes will be tightly packed together, whereas a higher number will ensure larger spacing of data points.\nn_components: Here is where we decide how many dimensions we wish to reduce our high-dimensional embeddings object down to, for visualisation we will likely set this parameter to a value of 2.\n\n\nKMeans CLustering\nn_clusters: KMeans is a relatively simple algorithm compared to other methods and components, requiring very little input. Here we just provide a value for the number of clusters we wish to form, this will either be clusters in the embeddings or a smaller, more manageable reduced embeddings object as mentiioned previously.\n\n\nHDBSCAN Clustering:\nmin_samples: This parameter defines the minimum number of points required to form a dense region. It helps determine the density threshold for clusters and can determine how conservative we want the clustering model to be. Put simply, a higher value can lead to fewer, larger clusters, while a lower value can result in more, smaller clusters.\nmin_cluster_size: This parameter sets the minimum size of clusters. Like min_samples it can directly influence the granularity of the clustering results. In this case, smaller values allow the formation of smaller clusters, while larger values prevent the algorithm from identifying any small clusters(or those below the size of the provided value). It‚Äôs worth noting that the relationship between min_samples and min_cluster_size is crucial. min_samples should generally be less than or equal to min_cluster_size. Adjusting these parameters in tandem helps us to control the sensitivity of HDBSCAN, and for us to define what qualifies as a cluster.\n\n\nTip: Try starting with the default value for all of these parameters, and incrementally adjust based on the desired granularity or effect of any that we wish to amend.\n\n\n\nSupporting Data Visualisation:\nOnce we have our landscape output, as shown in Final output of project, we will inevitably need to display some further information regarding our topics, most commonly; Volume over Time (VOT) and Sentiment Distribution for each.\nWhen doing so, we would ideally keep some formatting consistencies when plotting, as we mentioned previously, the colouring of our topics must remain the same throughout so that they match up with any previous representations in existing visuals such as the landscape output. We would also want to ensure that any plot we create orders our topics by volume or at least in the same order throughout our project. We can order our topics in terms of volume easily with just a few lines of code.\nFirst, we‚Äôll make sure to set the factor levels of our topics by using dplyr::count() on the topic_name column, and setting the levels feature of the factor() base function based on the counted output.\n\n# sort topics by order of volume\ntopic_order &lt;- data %&gt;% dplyr::count(topic_name, sort = TRUE)\n# set levels determined by volume of topic, this orders the group variable for plotting\ndata &lt;- data %&gt;% \n  dplyr::mutate(topic_name = factor(topic_name, levels = topic_order$topic_name))\n\n\nTopic Volume over Time\nStarting with volume over time, we often choose to render a faceted plot that includes all topics and their VOT for comparison. We can do so by using functionality found in packages such as JPackage for this.\n\n# plot topic volume over time using 'plot_group_vol_time()' function\ndata %&gt;% \n  JPackage::plot_group_vol_time(group_var = topic_name,\n                                date_var = date,\n                                unit = \"day\",\n                                nrow = 2) +\n  ggplot2::scale_fill_manual(values = topic_colours) # apply colours manually!\n\n\n\n\n\n\n\n\n\n\nTopic Sentiment Distribution\nNext, we might want/need to break each of our topics out by their sentiment distribution to help shine light on any of particular interest or to help us tell a more refined story using our topic model. This can be done by using the dr_plot_sent_group() function of the DisplayR package.\n\ndata %&gt;% \n  DisplayR::dr_plot_sent_group(group_var = topic_name,\n                               sentiment_var = sentiment,\n                               \"percent\", bar_labels = \"none\", \n                               sentiment_colours = c(\"POSITIVE\" = \"darkgreen\",\n                                                     \"NEGATIVE\" = \"darkred\"))\n\n\n\n\n\n\n\n\n\n\nAlternative Visualisations\nWhile the two visuals we have displayed so far are relatively basic and commonly used, this does not mean that we won‚Äôt require alternative methods to display topic-level information. Often, we may render n-grams per topic to display the relationships that exist between terms/phrases, and we may create plots to showcase things such as data source or social network/platform distributions across topics.\nFinally, it‚Äôs worth noting that the need for specific data visualisation methods entirely depends on the project domain and brief, as well as any outcomes/findings derived throughout. This means we ought to be flexible in our approach to utilising any technique that may assist with strengthening our understanding of the data and/or supporting our analyses.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conversation Landscape</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html",
    "href": "spam_project_case_study.html",
    "title": "5¬† Spam Classifier",
    "section": "",
    "text": "5.1 Introduction\nIn the Peaks and Pits, and Conversation Landscape case studies, we walked through some of our fundamental project offerings. Here we walk through a different type of project, where the goal of the research was twofold:\nThis document covers 2. The deck we presented to the client can be found here",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html#introduction",
    "href": "spam_project_case_study.html#introduction",
    "title": "5¬† Spam Classifier",
    "section": "",
    "text": "To produce thought leadership-style guidance for preserving the quality of data on social.\nTo create a model, or a set of models & heuristics, to identify high-quality data on social.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html#spam-classifier",
    "href": "spam_project_case_study.html#spam-classifier",
    "title": "5¬† Spam Classifier",
    "section": "5.2 Spam Classifier",
    "text": "5.2 Spam Classifier\nThe vast majority of our work is centred around answering research questions which are proscribed to us by stakeholders. To achieve this aim, we try to extract, understand, and represent the organic opinions of real people. Our work is good in proportion to how accurately we can do this: the more organic & accurate data that we extract, the better our answers, and vice versa.\nAs practitioners we have all lived through the frustration of arriving at the end of a research project only to find an overlooked cluster of spam is skewing our results; sending us back to the beginning of the process to clean the data and repeat the analysis. When this happens it can be draining on both time and motivation, and if we‚Äôre pressed for either than we may end up producing work that isn‚Äôt accurate, or isn‚Äôt as accurate as it could be.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html#motivation",
    "href": "spam_project_case_study.html#motivation",
    "title": "5¬† Spam Classifier",
    "section": "5.3 Motivation",
    "text": "5.3 Motivation\nTake the following sentiment distribution:\n\n\n\n\n\nFigure 1. Sentiment Distribution\n\n\n\n\nBriefly, Simpson‚Äôs paradox demonstrates how important differences between groups can be obscured by aggregation.\nThe chart indicates an approximately even split between Positive and Negative, and a large proportion of Neutral. This distribution is common and unremarkable. We could make some reasonable inferences about the data based on this distribution. However, lurking under the surface is a problem similar to Simpson‚Äôs Paradox.\nFor example, if we separate our dataset into groups of ‚ÄòSpam‚Äô and ‚ÄòNot Spam‚Äô, the balance between Positive and Negative disappears entirely. The ‚ÄòSpam‚Äô section of the dataset has ~4x as many Positive mentions than Negative, whereas the ‚ÄòNot Spam‚Äô section has ~2x as many Negative than Positive; an ~8x swing.\n\n\n\n\n\nFigure 1.1. Removing ‚ÄòSpam‚Äô has a dramatic effect on the distribution of sentiment in a dataset.\n\n\n\n\nPractically speaking, failure to remove spam could be the difference between deriving an accurate picture of reality and not, or, a correctly-timed strategic intervention and continuing as-is, assuming everything is ok.\nWe also want to:\n\nReduce time spent cleaning data\nIncrease consistency of output\nTry our best to comply with the assumptions of the algorithms we use, e.g.¬†for clustering or topic modelling",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html#what-is-spam",
    "href": "spam_project_case_study.html#what-is-spam",
    "title": "5¬† Spam Classifier",
    "section": "5.4 What is Spam?",
    "text": "5.4 What is Spam?\nBroadly speaking, we can define spam as ‚ÄòUnwanted, irrelevant, or unsolicited mentions sent to large audiences.‚Äô However, whether something is unwanted or irrelevant is open to interpretation and varies with context; so precisely stating what each of these words means is difficult. Despite this difficulty, we will mostly tend to agree with one another when we actually see spam. There is a certain ‚Äòknow it when I see it‚Äô aspect of spam.\nIt should be quite clear which of the following two documents is spam and which is not:\n\n\n\n‚ÄúNeed help with online exams, assignments, research projects, or dissertations? Look no further! I can assist with proofreading, personal statements, and more. Let‚Äôs tackle #MachineLearning, #DataScience, #Python, #Cybersecurity, #BigData, #AI, #IoT, #DeepLearning, #NLP together!‚Äù\n\n\n\n‚ÄúI think one of the big open questions is whether anyone will challenge the order in court. It uses presidential emergency powers to require red-teaming of foundation models. Initially that may only affect companies like OpenAI and Anthropic that have been asking for regulation.‚Äù",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html#methodology",
    "href": "spam_project_case_study.html#methodology",
    "title": "5¬† Spam Classifier",
    "section": "5.5 Methodology",
    "text": "5.5 Methodology\nWith our loose definition of ‚ÄòSpam‚Äô in place, we used its ‚Äòknow it when I see it‚Äô property to curate a corpus of ‚ÄòSpam‚Äô and ‚ÄòNot Spam‚Äô mentions. We then used a Transfer Learning1 approach to fine-tune a foundational Language Model to classify data into ‚ÄòSpam‚Äô and ‚ÄòNot Spam‚Äô.\nWhen labelling ‚ÄòSpam‚Äô and ‚ÄòNot Spam‚Äô we added optional labels as the need arose. For example, we saw a lot of spam regarding wallet scams, memecoins, crypto price updates, sustainabile coins, and a whole lot more relating to cryptocurrencies, so we added a label for ‚ÄòCrypto‚Äô.\n\n\n\n\n\n\nCreating Labels on the Fly\n\n\n\n\n\nIn a project like this, creating new labels as we went was inevitable, because we were discovering/learning about the nature of Spam as we labelled, and as part of the research brief we were to create a taxonomy of spam - meaning we needed to identify groups of spam from the data.\nThat said, if we introduce a new label at data point 1,000, are we really going to go back and check which of the previous 1,000 data points may fit into that label? The answer is most likely no. Depending on what you‚Äôre using these labels for, this may or may not be harmful. For our cases, the additional labels were of secondary importance, so we could tolerate them not being consistent throughout the dataset. We also set out to identify cases off LLM-generated slop 2\nTo be clear, the important label for the model was ‚ÄòSpam‚Äô vs ‚ÄòNot Spam‚Äô.\nVisit the Data Labelling Strategy document for more information on strategies for curating datasets.\n\n\n\nCreating the additional labels served two main purposes:\n\nPriors for a Taxonomy of Spam\nDiscrete variables to systematically improve model performance\n\nWe were delivering the Taxonomy as a scoped commitment, and we will talk about 2. in more detail in the Systematically Improving a Model Section.\nWhen selecting models to test we needed to consider the following constraints:\n\nWeights are open-source\nPermissive License\nCapable of performing the task at hand\nCan be fine-tuned on consumer hardware\nSimple to deploy and run inference at scale\n\nWe tested a variety of candidate models and ultimately opted for a fine-tune of ‚ÄòRoberta-base‚Äô as it performed the best in all metrics.\n[TODO: get gt table for initial results]",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html#corpus",
    "href": "spam_project_case_study.html#corpus",
    "title": "5¬† Spam Classifier",
    "section": "5.6 Corpus",
    "text": "5.6 Corpus\nEach tab houses a selection of the texts which have been assigned that label inside the corpus, click through the tabs to start building a mental model for the type of data that will be removed by the classifier.\n\n\n\n\n\n\nWarning\n\n\n\nIt is crucial when using the model that you understand what you are removing, and that you are happy that what you are removing is not indispensable for your research question(s). Never blindly remove data.\n\n\n\nPromotionSlopCryptoArticle LinkSEOAnnouncementEventReportQuote\n\n\nSomething worth detailing here is that in the early labels, before it was clear that people sharing their AI Generations might be their own type of spam, I was considering links to AI images as a form of promotion. If the goal was to separate promotions from AI generations this would be problematic.\nHowever, this corpus is for separating ‚ÄòSpam‚Äô from ‚ÄòNot Spam‚Äô, so we can accept some fuzziness between categories.\n\n\n\n\n\n\n\n\nIt is not always possible to tell whether something was written by an AI, a person, or a person using AI. Some slop, however, is much more obvious than others.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSEO will tend to be generic content (sometimes including links) with excessive use of keywords, or hashtags to boost content visibility.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuite similar to ‚Äòarticle_link‚Äô except reports are mainly the content of reports, they have often been truncated (ending in ‚Ä¶) by Sprinklr‚Äôs scrapers. They are potentially tricky to deal with because they will often not look like spam.\n\n\n\n\n\n\n\n\nWe started out collecting these because quotes can have unwanted effects in many of our downstream tasks, and then settled on texts where people are just parroting a quote being Spam. I strongly suspect that some of these quotes will not be spam, and should be revisited.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html#data-challenges",
    "href": "spam_project_case_study.html#data-challenges",
    "title": "5¬† Spam Classifier",
    "section": "5.7 Data Challenges",
    "text": "5.7 Data Challenges\nNow that you have seen some of the data, let‚Äôs talk about a few of the data-related challenges we encountered. We include this section only to reassure that you will encounter problems, so try not to get too worried when you inevitably do.\n\nArticles, Links, Announcements\nA particular challenge we faced was how to separate Articles, Reports, and Announcements that are included by a user as part of a discussion or organic conversation, from those that are merely a subset of promotion, or those that are a mix of both. Ultimately we elected to relabel a subset (~500) of the mentions of these classes.\nWe made the choice to relabel because 1) we had found it difficult to be consistent in our labelling, which meant that the model was struggling to learn to distinguish these classes, and 2) after discussing the issue as a group we planned to explore how we could bracket ‚ÄòNot Spam‚Äô mentions into distinct categories.\nAlthough laborious, the net impact of revisiting labels was positive: we ended up with a clearer idea of what constitutes spam, and our classifier‚Äôs performance improved.\n\n\n\n\n\n\nTip\n\n\n\nIt‚Äôs important to make space for self-correcting mechanisms along the way.\n\n\n\n\nLong Mentions\nIt takes a long time to read the blog-length AI-infused mentions from social_network == \"WEB\". Many of these are half-written by AI, or contain a lot of SEO, others have paragraphs full of adverts. If we knew in advance which portion of the text would contain the advert then we could remove, but we don‚Äôt so we are forced to remove the whole mention. Whilst this may seem undesirable, the adverts are often identical or nearly identical, and may feature 100s, or 1,000s of times per dataset, skewing our downstream analyses if not treated with care.\nTo add to this difficulty, the roberta-base model that we ended up using has a maximum token limit of 512. Any token after the 512th token will be truncated. Tokens are not 1:1 mappable to words, each word will tend to be $$2-5 tokens 3. It follows from this that a document which has no spam signals until the 513th token, but many thereafter, will be labelled as ‚ÄòSpam‚Äô but the model will classify it as ‚ÄòNot Spam‚Äô.\nOne solution would be to use a model with a larger context window, but this would present other difficulties. Another solution would be to break up each mention into parts with length &lt; 512, however this again presents other difficulties. Ultimately we choose to accept that some documents will be classified incorrectly due to length.\n\n\nSlop\nSome slop is instantly recognisable: repeated structures/constructs, stock terms & phrases, unnatural rhythm/musicality, or clear register mismatch. 4 Other slop is difficult to identify, and we risk a high % of false positives if we‚Äôre not careful in how we label.\nSeparate to the difficult cases, we have to read a lot of slop to start recognising the patterns. Varying exposure to slop across labellers is inevitable, and will inevitably lead to some inconsistency in labelled data. Again, it‚Äôs important to introduce self-correcting mechanisms later in the pipeline, see the Modelling Outputs Logits & Uncertainty section for more information.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html#data-labelling-strategy",
    "href": "spam_project_case_study.html#data-labelling-strategy",
    "title": "5¬† Spam Classifier",
    "section": "5.8 Data & Labelling Strategy",
    "text": "5.8 Data & Labelling Strategy\nFrom a combination of the Spam Classifier project and other tangentially-related porjects e.g.¬†‚ÄòGeneralised Peaks & Pits Classifier‚Äô, we have created an entire document. We expect this document to grow as we continue to learn new things about data labelling strategies.\nAt a very high level:\n\nData is the most important part of any machine learning project.\nWe aim for a high quantity of high-quality data.\nIt will often take from \\(\\approx\\) 2 to 5x the amount of data to halve the number of errors, so plan accordingly.\nWe expect to revisit our data labelling strategy throughout the project as we learn new things.\n\nFor information on the practical steps to actually labelling data, visit the Data Labelling Stack document",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html#training-the-model",
    "href": "spam_project_case_study.html#training-the-model",
    "title": "5¬† Spam Classifier",
    "section": "5.9 Training the Model",
    "text": "5.9 Training the Model\n[TODO: Pending merge with Aoife‚Äôs document]",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html#systematically-improving-a-model",
    "href": "spam_project_case_study.html#systematically-improving-a-model",
    "title": "5¬† Spam Classifier",
    "section": "5.10 Systematically Improving a Model",
    "text": "5.10 Systematically Improving a Model\nSimilarly to the Data & Labelling strategy document, we have created a separate document regarding systematically improving a machine learning model, particularly for text classification.\nAt a very high level:\n\nLook at your data, a lot.\nBreak your data into groups (discrete variables) and calculate metrics for each group\nTarget (find more data) high-frequency, or under-performing groups\nCalculate the loss for each training sample and identify incorrect labels, or grave model errors\nIdentify uncertainty in the model with the logits or the softmax‚Äôd logits (0.4 - 0.6 = quite uncertain)\nContinuously monitor and improve your model by starting at step 0 and working through the process.\nData work will never be over - but when you start to exhaust the opportunities for improvement from data, you can allocate more time to model selection, hyper-parameters etc. to squeeze out the last few \\(\\frac{1}{10^{ths}}\\) of performance gains.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html#final-results",
    "href": "spam_project_case_study.html#final-results",
    "title": "5¬† Spam Classifier",
    "section": "5.11 Final Results",
    "text": "5.11 Final Results\n[TODO: Pending the gt table from Aoife]",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html#reflections",
    "href": "spam_project_case_study.html#reflections",
    "title": "5¬† Spam Classifier",
    "section": "5.12 Reflections",
    "text": "5.12 Reflections",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "spam_project_case_study.html#footnotes",
    "href": "spam_project_case_study.html#footnotes",
    "title": "5¬† Spam Classifier",
    "section": "",
    "text": "Taking a model which has already been trained for general purposes, and training it on a specific task.‚Ü©Ô∏é\n‚Äú‚Ä¶slop generated by large language models, written by no one to communicate nothing.‚Äù, Robyn Speer, WordFreq maintainer‚Ü©Ô∏é\nwe can tokenise each document and count the tokens if we want to be precise‚Ü©Ô∏é\nWhere the language used is not appropriate for the thing being described. For example, when describing a calendar app: ‚ÄúThis trailblazing calendar app is going to turn the meeting scheduling world upside down!‚Äù‚Ü©Ô∏é",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Spam Classifier</span>"
    ]
  },
  {
    "objectID": "project_management.html",
    "href": "project_management.html",
    "title": "6¬† A Data Science project",
    "section": "",
    "text": "6.1 What is a Data Science project?\nAside from the obvious definition of a project (a piece of work planned and executed to achieve a particular aim- in this case facilitate a client‚Äôs needs), what this section is referring to is the structure and usage of a coding project.",
    "crumbs": [
      "Data Science project basics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>A Data Science project</span>"
    ]
  },
  {
    "objectID": "project_management.html#where-are-projects-savedlocated",
    "href": "project_management.html#where-are-projects-savedlocated",
    "title": "6¬† A Data Science project",
    "section": "6.2 Where are projects saved/located?",
    "text": "6.2 Where are projects saved/located?\nAll projects need to be saved onto the Google Drive. We have our own Data Science section, where we save our project and internal work (code, data, visualisations etc), which is in the filepath:\nShare_Clients/data_science_project_work/\nYou should get access to this directory straight away.\nWithin the data_science_project_work directory there are subdirectories of all of our clients, such as data_science_project_work/microsoft, data_science_project_work/dyson etc.\n\n\n\nScreenshot of the data_science_project_work/ directory with client-specific subdirectories\n\n\n\n\n\n\n\n\nFile paths\n\n\n\n\n\nYou will see that we refer to the location of directories mainly by their filepath, with the above screenshot of the Google Drive just for full transparency and clarity.\nThere are no two ways about it, getting familiar with working with filepaths in the command line (or in a script) is non-negotiable, but will become second nature and you will be tab-completing filepaths in no time at all!",
    "crumbs": [
      "Data Science project basics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>A Data Science project</span>"
    ]
  },
  {
    "objectID": "project_management.html#rstudio-projects",
    "href": "project_management.html#rstudio-projects",
    "title": "6¬† A Data Science project",
    "section": "6.3 RStudio Projects",
    "text": "6.3 RStudio Projects\nWe are primarily an R focused team, and as such we utilise RStudio projects to help keep all the files associated with a given project together in one directory.\nTo create a RStudio Project, click File &gt; New Project and then follow the below steps, but call the directory the name of the project (if a Microsoft project, appended by the project number) rather than ‚Äòr4ds‚Äô. Be sure to make sure the option ‚ÄòCreate project as subdirectory of‚Äô is the client directory on the Drive (in the case of Microsoft, this is Share_Clients/data_science_project_work/microsoft/project_work/).\n\n\n\nSteps to create a new project, taken from R for Data Science (2e) https://r4ds.hadley.nz/workflow-scripts.html#projects\n\n\nOnce this process is complete, there should be a new project folder in the client directory, with a .Rproj file within it.\nIf this is your first time using RStudio Projects, we recommend reading this section within the R for Data Science book, to familiarise yourself with some more intricacies of Project work within R (such as relative and absolute paths) which we would not do justice summarising here.",
    "crumbs": [
      "Data Science project basics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>A Data Science project</span>"
    ]
  },
  {
    "objectID": "project_management.html#components-of-a-ds-project",
    "href": "project_management.html#components-of-a-ds-project",
    "title": "6¬† A Data Science project",
    "section": "6.4 Components of a DS project",
    "text": "6.4 Components of a DS project\nDS projects consist of a parent project directory, with an associated .Rproj file, and three compulsory subdirectories code, data, and viz (all of which are made manually).\n\n\n\n\n\nWhilst there are no prizes for what goes in each subdirectory, it can be useful to have a structure in place to facilitate workflow ease.\n\ncode\nWithin the code subdirectory is where all scripts should be kept. We utilise .Rmd (R Markdown) documents rather than basic .R scripts for our code.\nWe do this for a few reasons, but the main benefits include:\n\nIt acts as an environment in which to do data science- we can capture not only what we did, but importantly why we did it\nWe can easily build and export a variety of different output formats from a R Markdown document (PDF, HTML, slideshow etc)\n\nAs part of our commitment to literate programming, there are some good practices that we can implement at this level of abstraction.\nFirstly, do not have extremely long .Rmd documents, as this is no good for anybody. Instead split up your documents into different sections based on the purpose of the code.\nWhilst this can be a bit subjective, a good rule of thumb is to have a separate .Rmd for each aspect of a workflow. For example, we might have one .Rmd for reading in raw data, another for cleaning the data, another for EDA, and another for performing topic modelling etc.\nWe should also follow the tidyverse style guide in the naming of files, which states:\n\nIf files should be run in a particular order, prefix them with numbers\n\nTherefore it makes sense to prefix our files, as we must load in the raw data before we can clean the data, and we must clean the data before we can perform certain analyses etc.\nSo we might have something like 00_load_data.Rmd, 01_clean_data.Rmd, 02_topic_modelling.Rmd.\n\n\ndata\ndata is where we save any data file that comes from a project.\nThe vast majority of projects will involve analysing an export from a social listening platform, such as Sprinklr. Analysts will save the export in the form of .csv or .xlsx files on the Drive (not within the Data Science section). As Sprinklr limits its exports to 10k rows of data per export file, we often are presented with 10s/100s of files with raw mentions. Therefore once we read these files into R, it is a good opportunity to save them as an .Rds in the code folder using the function write_rds() to avoid having to reread the raw excel or csv files in again.\nIt is within data where you would also save cleaned datasets and the outputs of different analyses (not visualisations though). This is not limited to .Rds files, but could also be word documents, excel spreadsheets etc.\nAs projects get more complex with many analyses, it can be easy to clutter this subdirectory. As such, it is recommended to make folders within data to help maintain structure. This means it is easy to navigate where cleaned data is because it will be in a folder such as data/cleaned_data and a dataframe with topic labels would be in data/topic_modelling.\n\n\n\n\n\n\nSave liberally\n\n\n\n\n\nGenerally speaking, space is cheaper than time. If in doubt, save an intermediate dataframe after an analysis if you think you‚Äôll need it in the future. It is better to run an analysis once and save the output to never look at it again, than to run an analysis, not save the output, and then need to rerun the analysis the following week.\n\n\n\n\n\nviz\nAny visualisation that is made throughout the project should be saved here. Again, this directory should be split into separate folders to keep different analyses separate, navigable, and clear. This is especially useful if there is are visualisations being made of the same analysis mapped over different variables or parameters, or if the project involves the analysis of separate products or brands.\nFor example, the below shows a screenshot of a viz folder for a project that looked at three products. Within viz the plots for each brand are in their own folder, and within each brand (chatgpt, copilot, gemini) there are further folders to split up the type of visualisations created (area_charts, eda etc), with even a third level of subdirectory (area_charts/peaks and area_charts/pits).\n\n\n\n\n\n\n\nExample viz directory hierarchy for a Peaks and Pits project",
    "crumbs": [
      "Data Science project basics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>A Data Science project</span>"
    ]
  },
  {
    "objectID": "project_players.html",
    "href": "project_players.html",
    "title": "7¬† Project Key Players",
    "section": "",
    "text": "7.1 Insights Analyst\nAnalysts add the bit of human-insight sparkle to our projects. They work closely with the client and stakeholder to help frame our findings so they are suitable for the clients business needs. At a high level, we may say ‚ÄúOur clustering analysis identified five distinct regions of conversation based on the semantics of the language used‚Äù whereas an analyst would translate that to ‚ÄúWe have five key conversational themes that can be targeted with tailored marketing strategies to boost product reach on socials‚Äù. Though this does vary on a project by project basis and we often have to act as a conduit between the science and the client too.\nInsight analysts are who work closely with Sprinklr and other social listening platforms to obtain the data we analyse. They will craft queries to pull the relevant data from a variety of sources and save the data on the Drive for us to access and do science on.",
    "crumbs": [
      "Data Science project basics",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Project Key Players</span>"
    ]
  },
  {
    "objectID": "project_players.html#account-manager",
    "href": "project_players.html#account-manager",
    "title": "7¬† Project Key Players",
    "section": "7.2 Account Manager",
    "text": "7.2 Account Manager\nAccount Managers (AMs) are the point of contact between our business and the client, bridging the gap between the technical teams (in our cases DS or Insights) and the client/stakeholder. They will arrange meetings with the client, help us understand the client‚Äôs needs and business objectives, and coordinate project logistics, timelines, and deliverables. As project deadlines approach, account managers will help QA our deliveries (normally in the form of a PowerPoint or Keynote presentation), providing valuable opinion from a non-technical background (it can be easy for us to get stuck in the weeds and forget that stakeholders do not know as much about data as we do). Broadly, AMs make sure both us as a company, and the client, are held accountable for the work we are contracted to do.",
    "crumbs": [
      "Data Science project basics",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Project Key Players</span>"
    ]
  },
  {
    "objectID": "project_players.html#datainsight-director",
    "href": "project_players.html#datainsight-director",
    "title": "7¬† Project Key Players",
    "section": "7.3 Data/Insight Director",
    "text": "7.3 Data/Insight Director\nDepending on the project, there will be a Data Director or Insight Director involved on the project too. You will notice that they will normally be resources on Float Whilst not working on the nitty gritty of the project, they are there to help steer the project in the appropriate direction based on the clients business needs. They will also be checking the final delivery as it is created, making sure the deliverables and story we have thread is suitable and valid.",
    "crumbs": [
      "Data Science project basics",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Project Key Players</span>"
    ]
  },
  {
    "objectID": "python_environments.html",
    "href": "python_environments.html",
    "title": "8¬† Python Environments",
    "section": "",
    "text": "Use this page for the info on Python environments",
    "crumbs": [
      "Data Science project basics",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Python Environments</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "9¬† Data Cleaning",
    "section": "",
    "text": "9.1 Dataset-level cleaning\nGoal: Ensure the dataset as a whole is relevant and of high quality\nThe main steps that we take for this level of cleaning is spam removal, uninformative content removal and deduplication",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html#dataset-level-cleaning",
    "href": "data_cleaning.html#dataset-level-cleaning",
    "title": "9¬† Data Cleaning",
    "section": "",
    "text": "Spam Removal\nWe use the term ‚Äúspam‚Äù quite loosely in our data pre-processing workflows. Whilst the strict definition of ‚Äúspam‚Äù could be something like ‚Äúunsolicited, repetitive, unwanted content‚Äù, we can think of it more broadly any post that displays irregular posting patterns or is not going to provide analytical value to our research project.\n\nHashtag filtering\nThere are multiple ways we can identify spam to remove it. The simplest is perhaps something like hashtag spamming, where an excessive number of hashtags, often unrelated to the content of the post, can be indicative of spam.\nWe can identify posts like this by counting the number of hashtags, and then filtering out posts that reach a certain (subjective) threshold.\n\ncleaned_data &lt;- data %&gt;% \n  mutate(extracted_hashtags = str_extract_all(message_column, \"#\\\\S+\"),\n         number_of_hashtags = lengths(extracted_hashtags)) %&gt;% \n  filter(number_of_hashtags &lt; 5)\n\nIn the example above we have set the threshold to be 5 (so any post that has 5 or more hashtags will be removed), however whilst this is a valid starting point, it is highly recommend to treat each dataset uniquely in determining which threshold to use.\n\n\nSpam-grams\nOften-times spam can be identified by repetitive posting of the same post, or very similar posts, over a short period of time.\nWe can identify these posts by breaking down posts into n-grams, and counting up the number of posts that contain each n-gram. For example, we might find lots of posts with the 6-gram ‚ÄúClick this link for amazing deals‚Äù, which we would want to be removed.\nTo do this, we can unnest our text data into n-grams (where we decide what value of n we want), count the number of times each n-gram appears in the data, and filter out any post that contains an n-gram above this filtering threshold.\nThankfully, we have a function within the LimpiaR package called limpiar_spam_grams() which aids us with this task massively. With this function, we can specify the value of n we want and the minimum number of times an n-gram should occur to be removed. We are then able to inspect the different n-grams that are removed by the function (and their corresponding post) optionally changing the function inputs if we need to be more strict or conservative with our spam removal.\n\nspam_grams &lt;- data %&gt;% \n  limpiar_spam_grams(text_var = message_column,\n                     n_gram = 6,\n                     min_freq = 6)\n\n# see remove spam_grams\nspam_grams %&gt;% \n  pluck(\"spam_grams\")\n\n# see deleted posts\nspam_grams %&gt;% \n  pluck(\"deleted\")\n\n# save 'clean' posts\nclean_data &lt;- spam_grams %&gt;% \n  pluck(\"data\")\n\n\n\nFilter by post length\nDepending on the specific research question or analysis we will be performing, not all posts are equal in their analytical potential. For example, if we are investigating what specific features contribute to the emotional association of a product with a specific audience, a short post like ‚ÄúI love product‚Äù (three words) won‚Äôt provide the level of detail required to answer the question.\nWhile there is no strict rule for overcoming this, we can use a simple heuristic for post length to determine the minimum size a post needs to be before it is considered informative. For instance, a post like ‚ÄúI love product, the features x and y excite me so much‚Äù (12 words) is much more informative than the previous example. We might then decide that any post containing fewer than 10 words (or perhaps 25 characters) can be removed from downstream analysis.\nOn the other end of the spectrum, exceedingly long posts can also be problematic. These long posts might contain a lot of irrelevant information, which could dilute our ability to extract the core information we need. Additionally, long posts might be too lengthy for certain pipelines. Many embedding models, for example, have a maximum token length and will truncate posts that are longer than this, meaning we could lose valuable information if it appears at the end of the post. Also, from a practical perspective, longer posts take more time to analyse and require more cognitive effort to read, especially if we need to manually identify useful content (e.g.¬†find suitable verbatims).\n\n# Remove posts with fewer than 10 words\ncleaned_data &lt;- data %&gt;% \n  filter(str_count(message_column, \"\\\\w+\") &gt;= 10)\n\n# Remove posts with fewer than 25 characters and more than 2500 characters\ncleaned_data &lt;- data %&gt;% \n  filter(str_length(message_column) &gt;= 25 & str_length(message_column) &lt;= 2500)\n\n\n\n\nDeduplication\nWhile removing spam often addresses repeated content, it‚Äôs also important to handle cases of exact duplicates within our dataset. Deduplication focuses on eliminating instances where entire data points, including all their attributes, are repeated.\nA duplicated data point will not only have the same message_column content but also identical values in every other column (e.g., universal_message_id, created_time, permalink). This is different from spam posts, which may repeat the same message but will differ in attributes like universal_message_id and created_time.\nAlthough the limpiar_spam_grams() function can help identify spam through frequent n-grams, it might not catch these exact duplicates if they occur infrequently. Therefore, it is essential to use a deduplication step to ensure we are not analysing redundant data.\nTo remove duplicates, we can use the distinct() function from the dplyr package, ensuring that we retain only unique values of universal_message_id. This step guarantees that each post is represented only once in our dataset.\n\ndata_no_duplicates &lt;- data %&gt;% \n  distinct(universal_message_id, .keep_all = TRUE)",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html#document-level-cleaning",
    "href": "data_cleaning.html#document-level-cleaning",
    "title": "9¬† Data Cleaning",
    "section": "9.2 Document-level cleaning",
    "text": "9.2 Document-level cleaning\nGoal: Prepare each individual document (post) for text analysis.\nAt a document-level (or individual post level), the steps that we take are more small scale. The necessity to perform each cleaning step will depend on the downstream analysis being performed, but in general the different steps that we can undertake are:\n\nRemove punctuation\nOften times we will want punctuation to be removed before performing an analysis because they tend to not be useful for text analysis. This is particularly the case with more ‚Äòtraditional‚Äô text analytics, where an algorithm will assign punctuation marks a unique numeric identify just like a word. By removing punctuation we create a cleaner dataset by reducing noise.\n\n\n\n\n\n\nWarning on punctuation\n\n\n\n\n\nFor more complex models, such as those that utilise word or sentence embeddings, we often keep punctuation in. This is because punctuation is key to understanding a sentences context (which is what sentence embeddings can do).\nFor example, there is a big difference between the sentences ‚ÄúLet‚Äôs eat, Grandpa‚Äù and ‚ÄúLet‚Äôs eat Grandpa‚Äù, which is lost if we remove punctuation.\n\n\n\n\n\nRemove stopwords\nStopwords are extremely common words such as ‚Äúand,‚Äù ‚Äúthe,‚Äù and ‚Äúis‚Äù that often do not carry significant meaning. In text analysis, these words are typically filtered out to improve the efficiency of text analytical models by reducing the volume of non-essential words.\nRemoving stopwords is particularly useful in our projects for when we are visualising words, such as a bigram network or a WLO plot, as it is more effective if precious informative space on the plots is not occupied by these uninformative terms.\n\n\n\n\n\n\nWarning on stopword removal\n\n\n\n\n\nSimilarly to the removal of punctuation, for more complex models (those that utilise word or sentence embeddings) we often keep stopwords in. This is because these stopwords can be key to understanding a sentences context (which is what sentence embeddings can do).\nFor example, imagine if we removed the stopword ‚Äúnot‚Äù from the sentence ‚ÄúI have not eaten pizza‚Äù- it would become ‚ÄúI have eaten pizza‚Äù and the whole context of the sentence would be different.\nAnother time to be aware of stopwords is if a key term related to a project is itself a stopword. For example, the stopwords list SMART treats the term ‚Äúone‚Äù as a stopword. If we were studying different Xbox products, then the console ‚ÄúXbox One‚Äù would end up being ‚ÄúXbox‚Äù and we would lose all insight referring to that specific model. For this reason it is always worth double checking which stopwords get removed and whether it is actually suitable.\n\n\n\n\n\nLowercase text\nConverting all text to lowercase standardises the text data, making it uniform. This helps in treating words like ‚ÄúData‚Äù and ‚Äúdata‚Äù as the same word, and is especially useful when an analysis requires an understanding of the frequency of a term (we rarely want to count ‚ÄúData‚Äù and ‚Äúdata‚Äù as two different things) such as bigram networks.\n\n\nRemove mentions\nMentions (e.g., @username) are specific to social media platforms and often do not carry significant meaning for text analysis, and in fact may be confuse downstream analyses. For example, if there was a username called @I_love_chocolate, upon punctuation remove this might end up confusing a sentiment algorithm. Removing mentions therefore helps in focusing on the actual content of the text.\n\n\n\n\n\n\nRetaining mentions, sometimes\n\n\n\n\n\nWe often perform analyses that involve network analyses. For these, we need to have information of usernames because they appear when users are either mentioned or retweeted. In this case we do not want to remove the @username completely, but rather we can store this information elsewhere in the dataframe.\nHowever, broadly speaking if the goal is to analyse the content/context of a paste, removing mentions is very much necessary.\n\n\n\n\n\nRemove URLs\nURLs in posts often point to external content and generally do not provide meaningful information for text analysis. Removing URLs helps to clean the text by eliminating these irrelevant elements.\n\n\nRemove emojis/special characters\nEmojis and special characters can add noise to the text data. While they can be useful for certain analyses (like emoji-specific sentiment analysis - though we rarely do this), they are often removed to simplify text and focus on word-based analysis.\n\n\nStemming/Lemmatization\nStemming and lemmatization are both techniques used to reduce words to their base or root form and act as a text normalisation technique.\nStemming trims word endings to their most basic form, for example changing ‚Äúclouds‚Äù to ‚Äúcloud‚Äù or ‚Äútrees‚Äù to ‚Äútree‚Äù. However, sometimes stemming reduces words to a form that doesn‚Äôt make total sense such as ‚Äúlittle‚Äù to ‚Äúlittl‚Äù or ‚Äúhistories‚Äù to ‚Äúhistori‚Äù.\nLemmatization considers the context and grammatical role when normalising words, producing dictionary definition version of words. For example ‚Äúhistories‚Äù would become ‚Äúhistory‚Äù, and ‚Äúcaring‚Äù would become ‚Äúcar‚Äù (whereas for stemming it would become ‚Äúcar‚Äù).\nWe tend to use lemmatization over stemming- despite it being a bit slower due to a more complex model, the benefit of lemmitization outweighs this. Similar to lowercasing the text, lemmitization is useful when we need to normalise text where having distinct terms like ‚Äúchange‚Äù, ‚Äúchanging‚Äù, ‚Äúchanges‚Äù, and ‚Äúchanged‚Äù isn‚Äôt necessary and just ‚Äúchange‚Äù is suitable.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html#conclusion",
    "href": "data_cleaning.html#conclusion",
    "title": "9¬† Data Cleaning",
    "section": "9.3 Conclusion",
    "text": "9.3 Conclusion\nDespite all of these different techniques, it is important to remember these are not mutually exclusive, and do not always need to be performed. It may very well be the case where a specific project actually required us to mine through the URLs in social posts to see where users a linking too, or perhaps keeping text as all-caps is important for how a specific brand or product is mentioned online. Whilst we can streamline the cleaning steps by using the ParseR function above, it is always worth spending time considering the best cleaning steps for each specific part of a project. It is much better spending more time at the beginning of the project getting this right, than realising that the downstream analysis are built on dodgy foundations and the data cleaning step needs to happen again later in the project, rendering intermediate work redundant.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "modelling_overview.html",
    "href": "modelling_overview.html",
    "title": "10¬† Setting The Scene",
    "section": "",
    "text": "10.1 Understanding the Problem: Defining the Research Question\nBefore diving into data collection or whirring up a GPU for model training, it‚Äôs crucial to have a clear and well-defined research question from the client.The question provided by the client will shape every aspect of the project, from the selection of data sources to the design of labelling schemes and, ultimately, the type of model that is used\nCassie Kozyrkov (former Chief Decision Scientist at Google) brilliantly describes machine learning as ‚ÄúA thing labeller‚Äù - a tool that helps make a series of small decisions within the data. For example: Is this email spam? Should we invest more in this campaign? If our task involves labelling, categorizing, or making a decision based on patterns in data, then machine learning may be a good fit. However, if the correct answer can be looked up each time, or if decisions don‚Äôt involve subtle patterns in large datasets, ML is likely unnecessary.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Setting The Scene</span>"
    ]
  },
  {
    "objectID": "modelling_overview.html#sec-needs-ml",
    "href": "modelling_overview.html#sec-needs-ml",
    "title": "10¬† Setting The Scene",
    "section": "10.2 Does the Task Need Machine Learning?",
    "text": "10.2 Does the Task Need Machine Learning?\nOne of the most common misconceptions is that ML is a universal solution, often seen as ‚Äúmagical‚Äù by those unfamiliar with its limitations. However, ML isn‚Äôt always the right tool for the job. Before we consider building models, we need to be able to clearly articulate what success looks like. If the goals and metrics are not defined, or if there isn‚Äôt a need for learning from complex patterns in large amounts of data, ML may not be the answer. ML is not magic, though in many businesses (and non data savvy people, unlike yourself) it is considered okay to skip thinking about what it means to do the task well.\nFor ML to add value, you should be able to answer the following questions before starting:\n\nWhat does it mean to do the task correctly? If the task‚Äôs outcome isn‚Äôt measurable or there‚Äôs no clear definition of what success looks like, an ML solution will struggle to be effective.\nWhich mistakes are worse than which other mistakes? In many ML tasks, errors are inevitable, but not all errors are equally harmful. Before building a model, it‚Äôs essential to rank the mistakes and decide which ones are more tolerable than others.\nHow will we measure the performance at scale? Imagine 1,000 units of work are completed, some of them imperfectly. You need a well-defined method to evaluate the overall performance of that work. Without an evaluation metric, it‚Äôs impossible to gauge success or failure meaningfully.\n\n\nBest Practices for Deciding Whether ML is Needed\n1.  Is the problem deterministic?\nIf a task can be solved by looking up known answers or using rule-based logic, you don‚Äôt need ML. ML is beneficial when patterns need to be learned from data, particularly when those patterns are complex, subtle, or change over time. 2. Can a non-ML solution address the problem efficiently? Consider whether simpler approaches such as basic statistics, heuristics, or existing automation tools can solve the problem. ML should only be used when it offers a clear advantage over these simpler methods. 3. Do we have enough data, and is it labelled correctly? ML models require data, and lots of it, especially for tasks like classification. Moreover, labelled data (with clear examples of the desired output) is crucial for supervised learning. If the data is scarce or poorly labelled, you may need to reconsider whether ML is a viable approach. 4. Can we measure success? Defining evaluation metrics is a must before starting any ML project. If it‚Äôs impossible to clearly measure how well a model performs (e.g., accuracy, F1 score, precision-recall, etc.), reconsider whether ML is necessary or if the problem is well-formed.\nautomating the ineffable",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Setting The Scene</span>"
    ]
  },
  {
    "objectID": "data_labelling_strategy.html",
    "href": "data_labelling_strategy.html",
    "title": "11¬† Data Labelling and Collection",
    "section": "",
    "text": "11.1 Setting Goals\nSo you‚Äôre sure you need machine learning and you‚Äôve understood the research question. Now is the time to establish your modelling goals, as your data labelling and collection strategy should flow directly from these goals.\nAnswer the following questions to help you establish your goals:\nFrom your answers to these questions, you could establish goals like:\nEstablishing these goals will help keep you on track, and inform you when it‚Äôs time to stop collecting more data.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Data Labelling and Collection</span>"
    ]
  },
  {
    "objectID": "data_labelling_strategy.html#setting-goals",
    "href": "data_labelling_strategy.html#setting-goals",
    "title": "11¬† Data Labelling and Collection",
    "section": "",
    "text": "What does appropriate performance look like in terms of accuracy, i.e.¬†what is an acceptable error rate?\nDoes the end user expect perfect accuracy? If they do, do not use Machine Learning.\nIs one class ‚Äòmore important‚Äô than the other?\nIs one type of mistake (false positive, false negative) more important than the other, and does this differ across classes? i.e.¬†if we are creating a classifier for detecting extremely abusive content, we are going to prioritise minimising the number of false negatives, accepting that we will have a higher rate of false positives.\n\n\n\nAccuracy must be 80% - or 1/5 error rate\nFalse positives & negatives are equally important, so we want precision and recall to be within 2% of each other for each class.\n\n\n\n\n\n\n\n\nNote\n\n\n\nInformation on our recommended stack for labelling can be found in the Data Labelling Stack document.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Data Labelling and Collection</span>"
    ]
  },
  {
    "objectID": "data_labelling_strategy.html#achieving-our-goals",
    "href": "data_labelling_strategy.html#achieving-our-goals",
    "title": "11¬† Data Labelling and Collection",
    "section": "11.2 Achieving our Goals",
    "text": "11.2 Achieving our Goals\nSomething that we cannot stress enough, is that data is the single most important factor in any Machine Learning project. We need to find a high enough quantity of high quality data to achieve our modelling goals\\(^{\\text{tm}}\\).\nThis raises two questions:\n\nHow much data is enough?\nWhat is quality when it comes to data?\n\nThere is a circularity to the answer of both questions. Enough data is the amount of data that it takes you to achieve your modelling goals1. Quality data is data that helps you achieve your modelling goals2. Despite this circularity, we put evaluations in place that tell us whether our changes are positively impacting our ability to achieve our modelling goals, or not.\n\nQuantity\nThankfully, for text classification tasks that involve foundational models & Transfer Learning, the models already understand language so we do not have to teach them to understand it. This would require a lot of data. For our purposes, we tend to need \\(1,000 \\lesssim x \\lesssim 10,000\\) samples to get the required performance.\nAnother rule of thumb is that if we want to halve the error rate, we need to double the sample size. Clearly the required sample size grows exponentially as we get closer to 100% accuracy, so we have to balance model performance versus allocated resources.\n\n\n\n\n\n\nWarning\n\n\n\nUpdate: more research suggests we will often need 4-5x as much data to halve errors.\n\n\n\n\n\n\n\n\n\n\n\nClearly, if we need 2x (or more) as much data to halve errors, the amount of data we need is going to grow very quickly. As you progress through the project, keep in mind that any information you can acquire about this relationship is valuable. One strategy for understanding the relationship is to train a model on progressively larger samples. Record the accuracy for [100, 200, 500, 1000, 2000] samples and visualise, what effect is increasing sample size having on accuracy? Extrapolate the trend and you have a hypothesis for a scaling law which you can test.\n\n\n\n\n\n\nMore rows \\(\\neq\\) more data\n\n\n\n\n\nIt‚Äôs easy to conflate the number of rows, or samples, with the quantity of data. 1,000,000 identical mentions contain the same amount of data as just 1 of those mentions. Accidentally filling your dataset with a load of duplicates or near duplicates will mislead you into thinking you have more data than you do in fact have. This in turn may mislead you on how well the scaling laws are holding - i.e.¬†you are adding ‚Äòdata‚Äô but not seeing increases in your metrics.\n\n\n\n\n\nClass Balance\nIf your classification task is binary - it has two possible labels - you should aim for an approximately 50/50 split. If you have 3 labels aim for a 33% split, irrespective of the distribution of the dataset that you will eventually be classifying. This is a very important rule of thumb that you should only break when you really know what you are doing. Even if you have a reasonable expectation that the true probability distribution will skew in one label or another‚Äôs favour, balancing the number of labels will make the learning 3 process smoother and more robust. Smooth and robust learning processes prevent many unnecessary headaches.\n\n\nQuality\nDovetailing with quantity, we need quality data. How do we define quality? Quality data is data that improves your model. It‚Äôs somewhat circular but that‚Äôs really all there is to it.\nQuality data will tend to be varied - combined it will cover the full range of ‚Äòthings‚Äô the model needs to perform the task at hand, e.g.¬†it covers the range of vocabulary the model will encounter when deployed, and it covers the types of syntactic sequence, or structure, that lead to one classification over another.\n\n\n\n\n\n\nSyntactic Sequences and Pattern Recognition\n\n\n\n\n\nAs humans we can make reasonable inferences about texts very quickly. For example, it takes us very little time (essentially it‚Äôs instant) to understand what a sentence in our native language is about, what intention the sentence was written with and a host of other things. We can do this because we have extreme capacity for pattern recognition.\nWhen classifying and analysing texts, we pay attention to details like word order, or punctuation, to infer meaning, consider a canonical example:\n\n‚ÄúLet‚Äôs eat, John.‚Äù\n‚ÄúLet‚Äôs eat John.‚Äù\n\nWe can do this because we have learnt the rules associated with commas. Most models for text classification would not, without further instruction, know the difference between 1. and 2. If this difference is important for our task, then we need to go out and find instances of this pattern to teach the model.\nWe should be able to determine whether a particular difference is important according to how frequently we find it in our training data when labelling.\n\n\n\nIf, once trained, the model is going to receive inputs with emojis, special characters, numbers, punctuation etc. then it should have examples of these in both the positive (spam) and negative (not spam) labels. This is very important. If the model only has emojis in the positive labels, it will learn that emoji = spam. This is undesirable.\n\n\nStaying within Distribution\nOne way of understanding modern machine learning models is that they compress patterns in their training data 4. Another way of thinking about machine learning models is to think of them as programs, and sub-programs. These views are complementary, and together they imply one of the most important challenges we have to confront when building Machine Learning models, namely, that they do not generalise well.\nGiven this, we should aim to train our model on the type of data that it will encounter at test time (when you deploy the model and use it for inference on new data), and we should expect our model to perform worse on data in proportion to how different the data is to our training data.\n\n\nIteration\nAs with everything in Data Science, the process of acquiring data and labelling data is fundamentally iterative. At each step of the modelling process you will uncover some new fact about your data, or some problem in your data, which sends you back to the beginning of the process. You then collect more data to solve this problem, and test the effect this data has on your model.\nThis process will require looking at a lot of data - you can think of data here as your input documents, your model‚Äôs predictions, your\nHow do you know when you‚Äôre done?\nYou have exhausted the available time, you‚Äôre out of ideas for how to improve your model, your model is performing at, or above, the targets you set out to achieve.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Data Labelling and Collection</span>"
    ]
  },
  {
    "objectID": "data_labelling_strategy.html#footnotes",
    "href": "data_labelling_strategy.html#footnotes",
    "title": "11¬† Data Labelling and Collection",
    "section": "",
    "text": "Gee, thanks‚Ü©Ô∏é\nGee, thanks x2‚Ü©Ô∏é\nIn terms of the model‚Äôs learning process‚Ü©Ô∏é\n Minimum Description Length‚Ü©Ô∏é",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Data Labelling and Collection</span>"
    ]
  },
  {
    "objectID": "data_labelling_stack.html",
    "href": "data_labelling_stack.html",
    "title": "12¬† Data Labelling Stack",
    "section": "",
    "text": "12.1 Doccano - AWS\nTo this aim, we now have a Doccano EC2 instance hosted on AWS at a company-specific domain. Hosting the instance makes collaboration while labelling significantly smoother and data is backed up.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Data Labelling Stack</span>"
    ]
  },
  {
    "objectID": "data_labelling_stack.html#doccano---aws",
    "href": "data_labelling_stack.html#doccano---aws",
    "title": "12¬† Data Labelling Stack",
    "section": "",
    "text": "Note\n\n\n\nIf you need access, message the team for details. You will be given the URL and login details.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Data Labelling Stack</span>"
    ]
  },
  {
    "objectID": "data_labelling_stack.html#doccano---local",
    "href": "data_labelling_stack.html#doccano---local",
    "title": "12¬† Data Labelling Stack",
    "section": "12.2 Doccano - Local",
    "text": "12.2 Doccano - Local\nSetting up a tool like Doccano requires:\n\nManaging a Python environment\nA working Docker installation\nSetting up a Doccano-specific Docker container from your Python environment\n\n\nPython Environments\n\n\n\n\n\n\nNote\n\n\n\nIf you feel confident setting up and managing Python environments skip to the Docker and Doccano section\n\n\nIf you are unfamiliar with Python environments and have come from an R background, you are in for a tiny treat. We can often get very far in R without knowing anything about virtual environments. This is primarily due to CRAN - R‚Äôs package management archive. CRAN adds a layer of friction to uploading and updating packages which help to protect the ecosystem, trading off developer productivity for usability. It does a great job of ensuring backwards compatibility and compatibility between packages.\nPyPi on the other hand, is not so fussed about ensuring backwards compatibility, and there are significantly fewer hoops to jump through to submit a package to PyPi than there are for CRAN. Whilst this is beneficial for the rate of innovation, the lack of guardrails can be painful for us as users.\nThe consequences of failing to manage your Python environment range from ‚Äòminor‚Äô to ‚Äòactually quite severe‚Äô. It is not exceptionally rare to need to delete an entire Python installation, or your whole Operating System to resolve Python environment issues! So with that in mind, let‚Äôs set ourselves up for success. We‚Äôll look at venv and miniconda.\n\n\n\n\n\n\nNote\n\n\n\nIf you are used to something more modern like poetry or uv then please do add a tab to the steps outlined below.\n\n\n\nvenvminiconda\n\n\nBefore following these steps - consider reading the official docs and building the environment from first principles.\nvenv (virtual env) is a built-in Python module. When using venv to management our environments, we are using a specific Python interpreter - the one associated with our environment - to run Python code. Under the hood this is achieved by adding the environment to our Python path, visit the official docs for more detail.\nCreating the environment:\n\nCreate a folder for your project\nNavigate to your your project folder cd &lt;~/root/to/folder&gt;\nCreate an environment named ‚Äò.venv‚Äô in the terminal: python -m venv .venv\nActivate the environment: source .venv/bin/activate\nCheck the environment is running with: echo $VIRTUAL_ENV\nNow you can install packages using pip (or other package management software).\n\n\nIf inheriting a project and the provider has included a ‚Äòrequirements.txt‚Äô file then install from requirements with: pip install -r requirements.txt\nIf you did not receive a ‚Äòrequirements.txt‚Äô file, or are creating the project yourself and adding dependencies, pip freeze &gt; requirements.txt\nList all installed packages with pip list\n\nEvery time you want to use this virtual environment, return to its folder, activate it and run your code.\n\n\nIf you intend to use conda to manage your environments, we recommend installing miniconda over conda because it is a lightweight version which plays nicely with MacOS. Unlike venv, miniconda environments are portable so we can use them across projects more easily. However, it‚Äôs still recommended to create specific environments for projects.\nAs ever, before following the steps below - consider reading the official docs: - Miniconda installation - Minconda docs\nCreating the environment:\n\nCreate a ‚Äòpretendenv‚Äô environment with Python version 3.10: conda create --name pretendenv python=3.10\nActivate the environment conda activate pretendenv\nInstall packages:\n\n\nWith conda: conda install pandas\nSometimes you‚Äôll need to use pip when working with conda: conda install pip &gt; pip install pandas\n\n\nList installed packages: conda list\nList environments: conda env list\n\nWhen working with conda we save our project‚Äôs requirements to a .yml file: 6. conda env export &gt; environment.yml\nThen new users can install our requirements: i. conda env create -f environment.yml\nYou can deactivate the environment with: conda deactivate pretendenv\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nCreate a folder and a virtual environment for your pending Doccano installation\n\n\n\n\nInstalling Docker and Doccano\nSteps for installing Docker (desktop) and Doccano\n\nFollow the official Docker documentation installation steps\nStart Docker (either in the terminal or via the desktop). Check you have it installed with docker --version, open /Applications/Docker.app\nIf you haven‚Äôt already, open up a terminal and execute: docker pull doccano/doccano\nSet up a Python environment\n\n\nname your environment ‚Äòdoccano‚Äô\nactivate it\npip install doccano or conda install doccano. If conda doesn‚Äôt find doccano: conda install pip -&gt; pip install doccanp\n\n\nCreate a Docker container for Doccano in the terminal:\n\n\n  docker container create --name doccano \n  -e \"ADMIN_USERNAME=admin\" \n  -e \"ADMIN_EMAIL=your.email@sharecreative.com\" \n  -e \"ADMIN_PASSWORD=yourpassword\" \n  -v doccano-db:/data \n  -p 8000:8000 doccano/doccano\n  \nThis will take a minute or two.\nCheck the image is built with docker images or docker images | grep doccano and if the return is empty, you don‚Äôt! Try again.\ndocker container start doccano\n\n\nopen http://localhost:8000\nLog in (top right) with the ADMIN_USERNAME and ADMIN_PASSWORD you set. Doccano has a fully-functional Django backend which takes care of authorisation (among other things).\n\nTo kill the container: docker kill doccano\n\n\n\n\n\n\nTip\n\n\n\ndocker --help will give you the tools to navigate Docker from the terminal.\n\n\n\n\nUsing Doccano\nAs ever, first read the official doccano-mentation. Next, read through Doccano‚Äôs official tutorial, this will show you how to create projects, add datasets, define labels, add members to a project, and annotate data.\nFor the next steps you will need to have Doccano and R open.\n\n\n\n\n\n\nTask\n\n\n\n\nSet up a Doccano project for text classification\n\n\n\nDoccano accepts data in the following formats: TextFile, TextLine, CSV , fastText, JSON, JSONL. If unfamiliar with any format, it‚Äôs simple enough to look up examples or read the specifications online, so we‚Äôll focus on the most common filetype that we use - CSV, showing how to import.\nLet‚Äôs create a .csv with one row and one string of text placed in a column named ‚Äòtext‚Äô.\n\ntibble(message = \"test\") %&gt;%\n  write_csv(\"~/Downloads/doccano_insufficient.csv\")\n\n\n\n\n\n\n\nTask\n\n\n\n1. Set up a Doccano project for text classification 2. Try importing your ‚Äòdoccano_insufficient.csv‚Äô file, how does Doccano tell you that something is wrong?\n\n\n\n\n\nAnswer\n\nDirectly above the ‚Äòimport‚Äô button we should see a table rendered with ‚ÄòFilename‚Äô, ‚ÄòLine‚Äô, ‚ÄòMessage‚Äô columns. This table should have 2 rows, which tell us: ‚ÄòColumn text not found in the file‚Äô and ‚ÄòColumn label not found in the file‚Äô.\n\nTo fix the issue directly in Doccano, note that Doccano allows us to input fields for both Data, and Label. We can change the text in the ‚ÄòColumn Data‚Äô field to ‚Äòmessage‚Äô this shows Doccano the structure of our data, and provided we have added some labels, we‚Äôll be able to start labelling our data.=\n\n\n\n\n\n\nTask\n\n\n\n1. Set up a Doccano project for text classification 2. Try importing your ‚Äòdoccano_insufficient.csv‚Äô file, how does Doccano tell you that something is wrong? 3. Add labels for ‚ÄòPositive‚Äô, ‚ÄòNegative‚Äô, and ‚ÄòNeutral‚Äô via the ‚ÄòLabel‚Äô tab in the sidebar. 4. Click ‚ÄòStart Annotation‚Äô and label the sample. 5. Export your data and check that the label column is present.\n\n\nWhen using the .csv filetype, everything except ‚Äòtext‚Äô and ‚Äòlabel‚Äô will be stored under ‚Äòmetadata‚Äô. These columns will be present when we label.\n\ntibble(text = c(\"text number 1\", \"text number 2\"),\n       label = \"\", # R will broad cast this to every row\n       id = c(\"xx1\", \"xx2\"),\n       sentiment = c(\"neutral\", \"neutral\")\n       ) %&gt;%\n  write_csv(\"~/Downloads/doccano_extra_fields.csv\")\n\nTry labelling the 2 samples and then export the data into a .csv, what happens to the metadata?\nWe should see that our export does not have the meta data. If we want the metadata to be included, we need to upload a .jsonl file, with our additional columns (id and sentiment) in a metadata node.\n\ntibble(text = c(\"text number 1\", \"text number 2\"),\n       label = \"\", # R will broad cast this to every row\n       id = c(\"xx1\", \"xx2\"),\n       sentiment = c(\"neutral\", \"neutral\")\n       ) %&gt;%\n  nest(metadata = c(\"id\", \"sentiment\")) %&gt;%\n  jsonlite::toJSON()\n\nYou can then save the file with the jsonlite::stream_out function, which requires a file() connection rather than a file path as input.\nImport the following data into Doccano, label it, and then export the data as .jsonl.\n\ntibble(text = c(\"text number 1\", \"text number 2\"),\n       label = \"\", # R will broad cast this to every row\n       id = c(\"xx1\", \"xx2\"),\n       sentiment = c(\"neutral\", \"neutral\")\n       ) %&gt;%\n  nest(metadata = c(\"id\", \"sentiment\")) %&gt;%\n  jsonlite::stream_out(file(\"~/Downloads/doccano_jsonl_meta.jsonl\"))\n\n\n\n\n\n\n\nTip\n\n\n\nLocal versions of Doccano export data to your ~/Downloads as a .zip file, and then with ‚Äòadmin‚Äô, ‚Äòadmin 1‚Äô, ‚Äòadmin 2‚Äô and so on. Rename your files as they are exported to avoid later confusion, and then move them to the appropriate folder.\n\n\nWhen you read this data back in, you should now have the original ID column, text, original sentiment label, and a label column requiring only minimal cleaning:\n\njsonlite::stream_in(file(\"~/Downloads/doccano_jsonl_meta_labelled.jsonl\")) %&gt;%\n  tibble() %&gt;%\n  unnest(label) %&gt;%\n  select(-Comments)",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Data Labelling Stack</span>"
    ]
  },
  {
    "objectID": "model_finetuning.html",
    "href": "model_finetuning.html",
    "title": "13¬† Model Training and Fine-Tuning",
    "section": "",
    "text": "Now that you have your data, the next step is selecting the appropriate model. When working with predefined categories (e.g.¬†sentiment), starting with out-of-the-box (OOTB) models that has already been fine-tuned by others to perform the task at hand might be sufficient. However, given the nature of both social media and our specific research, these models may not fully understand specific nuances (e.g., sarcasm, platform-specific language) without additional fine-tuning.\nFor this section, let‚Äôs assume we need to perform sentiment classification, and the reason we cannot use an OOTB sentiment classification model is because we are classifying data from a novel social media platform that has very unique language style (for example, this could be from a gaming forum where the phrase ‚Äúthat is sick‚Äù is actually positive, or from a thread on a new song that is described as ‚Äúsavage‚Äù). Whilst these are overly simplified example, the sentiment (ü•Å) still stands.\nWe am going to explore three distinct approaches to text classification, each offering different levels of complexity, resource requirements, and suitability based on the size of your dataset and the task at hand.\n\nLogistic Regression: A simple, interpretable classification model that is trained from scratch using basic feature extraction techniques like TF-IDF or Bag-of-Words. Logistic regression is well-suited for straightforward classification tasks and small datasets where interpretability is important.\nSetFit: A framework designed for few-shot learning, where limited labelled data is available. SetFit leverages pre-trained sentence transformers to generate embeddings, fine-tuning a lightweight classifier on top of these embeddings, making it ideal when you need quick results with minimal data.\nVanilla Fine-Tuning with Hugging Face Trainer: The most powerful of the three, this approach fine-tunes large pre-trained language models like BERT on task-specific datasets. It‚Äôs best used when you have access to larger datasets and need high accuracy and deep contextual understanding.\n\n\nFine-Tuning a Model\nFine-tuning involves taking a pre-trained model and retraining it on your specific dataset. How we go about this can be approached differently depending on the amount of labelled data and the complexity of the problem.\n\n\nChoosing the Right Pre-Trained Model\nSelecting an appropriate starting point is crucial.\n\nModel Selection: Choose models known to perform well in NLP tasks (e.g., BERT, RoBERTa).\nDomain Relevance: Prefer models pre-trained on data similar to your domain if available.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Model Training and Fine-Tuning</span>"
    ]
  },
  {
    "objectID": "logistic_regression.html",
    "href": "logistic_regression.html",
    "title": "14¬† Method 1: Traditional Machine Learning with textual features",
    "section": "",
    "text": "14.1 How to Train a Classifier on Text Features?\nSometimes we might find that fine-tuning a whole LLM or transformer model is overkill. In such cases, combining the knowledge from pre-trained language models with traditional machine learning algorithms can be a powerful approach. This involves generating features from the text (such as embeddings) to train a traditional classifier, such as logistic regression.\nAt a high level, the process involves:\nThe core idea behind using these traditional models for text classification is to find the best-fitting model to describe the relationship between a categorical dependent variable and one or more independent variables (features extracted from text).\nBenefits\nLimitations:\nWhen to Use These Methods?\nLet‚Äôs dive into training a traditional model on text features. For this walkthrough, we will training a logistic regression model on embeddings generated from a pre-trained sentence transformer. This section aims to get you started quickly. Feel free to run the code, experiment, and learn by doing. After this walkthrough, we‚Äôll provide a more detailed explanation of each step.\nSo, let‚Äôs get started! We will first install the required packages/modules‚Ä¶\n!pip install sentence-transformers scikit-learn datasets\n‚Ä¶ before loading in our dataset. For this example, let‚Äôs use the IMDb dataset for binary sentiment classification. This dataset contains 25,000 highly polar movie reviews (polar as in sentiment, not just Arctic based films) for training, and 25,000 for testing.\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"imdb\")",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Method 1: Traditional Machine Learning with textual features</span>"
    ]
  },
  {
    "objectID": "logistic_regression.html#how-to-train-a-classifier-on-text-features",
    "href": "logistic_regression.html#how-to-train-a-classifier-on-text-features",
    "title": "14¬† Method 1: Traditional Machine Learning with textual features",
    "section": "",
    "text": "Prepare the data\nNext we will split the dataset into training and testing sets, and take a sample for the sake of computing resources/time efficiencies.\n\n# Use a smaller subset for quicker execution\ntrain_dataset = dataset[\"train\"].shuffle(seed=42).select(range(2000))\nvalidation_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n\n# Extract texts and labels\ntrain_texts = train_dataset[\"text\"]\ntrain_labels = train_dataset[\"label\"]\nvalidation_texts = validation_dataset[\"text\"]\nvalidation_labels = validation_dataset[\"label\"]\n\n\n\nGenerating Embeddings\nLoad in a pre-trained sentence transformer and generate embeddings for the data. In this case we will use a popular model called all-MiniLM-L6-v2\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load the pre-trained model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings\ntrain_embeddings = model.encode(train_texts, convert_to_tensor=True)\nvalidation_embeddings = model.encode(validation_texts, convert_to_tensor=True)\n\n\n\nTraining a Model\nTrain a logistic regression model using the embeddings.\n\nLogistic RegressionSupport Vector Machine (SVM)Random ForestNaive BayesGradient Boosting\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Initialize the classifier\nclassifier = LogisticRegression(max_iter=1000, random_state=42)\n\n# Fit the model\nclassifier.fit(train_embeddings.cpu().numpy(), train_labels)\n\n\n\n\nfrom sklearn.svm import SVC\n\n# Initialize the classifier\nsvm_classifier = SVC(kernel='linear', probability=True)\n\n# Fit the model\nsvm_classifier.fit(train_embeddings.cpu().numpy(), train_labels)\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize the classifier\nrf_classifier = RandomForestClassifier(n_estimators=100)\n\n# Fit the model\nrf_classifier.fit(train_embeddings.cpu().numpy(), train_labels)\n\n\n\n\nfrom sklearn.naive_bayes import GaussianNB\n\n# Initialize the classifier\nnb_classifier = GaussianNB()\n\n# Fit the model\nnb_classifier.fit(train_embeddings, train_labels)\n\n\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Initialize the classifier\ngb_classifier = GradientBoostingClassifier(random_state=42)\n\n# Fit the model\ngb_classifier.fit(train_embeddings, train_labels)\n\n\n\n\n\n\nEvaluating the Model\nAssess the model‚Äôs performance on the validation set.\n\nLogistic RegressionSupport Vector Machine (SVM)Random ForestNaive BayesGradient Boosting\n\n\n\nfrom sklearn.metrics import classification_report\n\n# Make predictions\nvalidation_predictions = classifier.predict(validation_embeddings.cpu().numpy())\n\n# Print evaluation metrics\nprint(classification_report(validation_labels, validation_predictions, target_names=[\"Negative\", \"Positive\"]))\n\n\n\n\nfrom sklearn.metrics import classification_report\n\n# Make predictions\nsvm_predictions = svm_classifier.predict(validation_embeddings.cpu().numpy())\n\n# Print evaluation metrics\nprint(classification_report(validation_labels, svm_predictions, target_names=[\"Negative\", \"Positive\"]))\n\n\n\n\nfrom sklearn.metrics import classification_report\n\n# Make predictions\nrf_predictions = rf_classifier.predict(validation_embeddings.cpu().numpy())\n\n# Print evaluation metrics\nprint(classification_report(validation_labels, rf_predictions, target_names=[\"Negative\", \"Positive\"]))\n\n\n\n\nfrom sklearn.metrics import classification_report\n\n# Make predictions\nnb_predictions = nb_classifier.predict(validation_embeddings.cpu().numpy())\n\n# Print evaluation metrics\nprint(classification_report(validation_labels, nb_predictions, target_names=[\"Negative\", \"Positive\"]))\n\n\n\n\nfrom sklearn.metrics import classification_report\n\n# Make predictions\ngb_predictions = gb_classifier.predict(validation_embeddings.cpu().numpy())\n\n# Print evaluation metrics\nprint(classification_report(validation_labels, gb_predictions, target_names=[\"Negative\", \"Positive\"]))\n\n\n\n\n\n\nMaking Predictions on New Data\nYou can now use the model to predict sentiments of new texts.\n\nLogistic RegressionSupport Vector Machine (SVM)Random ForestNaive BayesGradient Boosting\n\n\n\nnew_texts = [\n    \"I absolutely love the polar express, it's easily Tom Hanks' best movie.\",\n    \"The Marvel film was boring and too long.\",\n]\n\n# Generate embeddings for new texts\nnew_embeddings = model.encode(new_texts, convert_to_tensor=True)\n\n# Predict\nnew_predictions = classifier.predict(new_embeddings.cpu().numpy())\n\n# Map predictions to labels\nlabel_mapping = {0: \"Negative\", 1: \"Positive\"}\npredicted_labels = [label_mapping[pred] for pred in new_predictions]\n\nfor text, label in zip(new_texts, predicted_labels):\n    print(f\"Text: {text}\\nPredicted Sentiment: {label}\\n\")\n\n\n\n\nnew_texts = [\n    \"I absolutely love the polar express, it's easily Tom Hanks' best movie.\",\n    \"The Marvel film was boring and too long.\",\n]\n\n# Generate embeddings for new texts\nnew_embeddings = model.encode(new_texts, convert_to_tensor=True)\n\n# Predict\nnew_predictions = svm_classifier.predict(new_embeddings.cpu().numpy())\n\n# Map predictions to labels\nlabel_mapping = {0: \"Negative\", 1: \"Positive\"}\npredicted_labels = [label_mapping[pred] for pred in new_predictions]\n\nfor text, label in zip(new_texts, predicted_labels):\n    print(f\"Text: {text}\\nPredicted Sentiment: {label}\\n\")\n\n\n\n\nnew_texts = [\n    \"I absolutely love the polar express, it's easily Tom Hanks' best movie.\",\n    \"The Marvel film was boring and too long.\",\n]\n\n# Generate embeddings for new texts\nnew_embeddings = model.encode(new_texts, convert_to_tensor=True)\n\n# Predict\nnew_predictions = rf_classifier.predict(new_embeddings.cpu().numpy())\n\n# Map predictions to labels\nlabel_mapping = {0: \"Negative\", 1: \"Positive\"}\npredicted_labels = [label_mapping[pred] for pred in new_predictions]\n\nfor text, label in zip(new_texts, predicted_labels):\n    print(f\"Text: {text}\\nPredicted Sentiment: {label}\\n\")\n\n\n\n\nnew_texts = [\n    \"I absolutely love the polar express, it's easily Tom Hanks' best movie.\",\n    \"The Marvel film was boring and too long.\",\n]\n\n# Generate embeddings for new texts\nnew_embeddings = model.encode(new_texts, convert_to_tensor=True)\n\n# Predict\nnew_predictions = nb_classifier.predict(new_embeddings)\n\n# Map predictions to labels\nlabel_mapping = {0: \"Negative\", 1: \"Positive\"}\npredicted_labels = [label_mapping[pred] for pred in new_predictions]\n\nfor text, label in zip(new_texts, predicted_labels):\n    print(f\"Text: {text}\\nPredicted Sentiment: {label}\\n\")\n\n\n\n\nnew_texts = [\n    \"I absolutely love the polar express, it's easily Tom Hanks' best movie.\",\n    \"The Marvel film was boring and too long.\",\n]\n\n# Generate embeddings for new texts\nnew_embeddings = model.encode(new_texts, convert_to_tensor=True)\n\n# Predict\nnew_predictions = gb_classifier.predict(new_embeddings.cpu().numpy())\n\n# Map predictions to labels\nlabel_mapping = {0: \"Negative\", 1: \"Positive\"}\npredicted_labels = [label_mapping[pred] for pred in new_predictions]\n\nfor text, label in zip(new_texts, predicted_labels):\n    print(f\"Text: {text}\\nPredicted Sentiment: {label}\\n\")\n\n\n\n\nCongratulations! You‚Äôve successfully trained a classification model on embeddings for sentiment analysis. Feel free to tweak the code, try different datasets, and explore further.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Method 1: Traditional Machine Learning with textual features</span>"
    ]
  },
  {
    "objectID": "logistic_regression.html#extra-details",
    "href": "logistic_regression.html#extra-details",
    "title": "14¬† Method 1: Traditional Machine Learning with textual features",
    "section": "14.2 Extra details",
    "text": "14.2 Extra details\nWhereas the other tutorials for fine-tuning (see SetFit and vanilla fine-tuning pages) have required as to do a deep dive to each step of the training process because each step has many moving parts, training a traditional classifier using text features does not have so many moving parts.\nHowever,\n\nHandling Class Imbalance\nIn real-world datasets, you may encounter class imbalance, where some classes are underrepresented and others overrepresented. This can lead to biased models that perform poorly on minority classes.\nThere are a couple of things we can do to in our attempt to alleviate this issue:\n\nAssign Class Weights: We could adjust class weights by modifying the training algorithm to give more weight to the minority classes. This effectively penalises the model more for making errors on minority classes and less for making errors on majority classes.\nResampling Techniques: We could oversample the minority class or undersample the majority class.\n\nI personally prefer the first approach. In our projects labelled data tends to be scarce, and oversampling the minority class could feasibly lead to a lack of diversity in that class, and undersampling the majority class leads to us not using precious labelled data.\nImplementing Class Weights:\n\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Compute class weights\nunique_classes = np.unique(train_labels)\nclass_weights = compute_class_weight(class_weight = 'balanced', classes = [0, 1],  = train_labels)\nclass_weight_dict = dict(zip(unique_classes, class_weights))\n\n# Initialize the classifier with class weights\nclassifier = LogisticRegression(max_iter = 1000, random_state=42, class_weight = class_weight_dict)\n\n# Fit the model\nclassifier.fit(train_embeddings.cpu().numpy(), train_labels)\n\n\n\nInterpreting Results\nEvaluating a fine-tuned model often requires more than just accuracy. We also need metrics like precision, recall, and F1-score to understand how well the model handles different classes.\n\nPlease read the corresponding section in the handbook on Model Evaluation for more information on the theory and rationale behind these metrics, and when to chose one over the other.\n\nWe can implement these additional metrics by using the evaluate library\n\nfrom evaluate import load\n\naccuracy_metric = load(\"accuracy\")\nprecision_metric = load(\"precision\")\nrecall_metric = load(\"recall\")\nf1_metric = load(\"f1\")\n\naccuracy = accuracy_metric.compute(predictions=validation_predictions, references=validation_labels)\nprecision = precision_metric.compute(predictions=validation_predictions, references=validation_labels, average='weighted')\nrecall = recall_metric.compute(predictions=validation_predictions, references=validation_labels, average='weighted')\nf1 = f1_metric.compute(predictions=validation_predictions, references=validation_labels, average='weighted')\n\nprint(f\"Accuracy: {accuracy['accuracy']}\")\nprint(f\"Precision: {precision['precision']}\")\nprint(f\"Recall: {recall['recall']}\")\nprint(f\"F1 Score: {f1['f1']}\")\n\nWe can also visualise a confusion matrix to see where the model is making errors:\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(validation_labels, validation_predictions)\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Method 1: Traditional Machine Learning with textual features</span>"
    ]
  },
  {
    "objectID": "logistic_regression.html#hyperparameter-tuning",
    "href": "logistic_regression.html#hyperparameter-tuning",
    "title": "14¬† Method 1: Traditional Machine Learning with textual features",
    "section": "14.3 Hyperparameter Tuning",
    "text": "14.3 Hyperparameter Tuning\nMuch like training an LLM using Hugging Face Trainer or SetFit, there are hyperparameters associated with traditional statistical models that can affect model performance. For Logistic Regression these are not that critical, but we can still tune them if we would like. They include:\n\nsolver: The algorithm to use in the optimisation problem. In other words, it‚Äôs what is used to find the optimal parameters (coefficients) that minimise the cost function of the model. There are five different solvers that can be used:\n\nliblinear: A solver that uses the coordinate descent algorithm. It is efficient for small to medium-sized datasets and for problems where the data is sparse (many zeros in the data). It works well for binary and multi-class classification.\nsag: Stands for ‚ÄúStochastic Average Gradient Descent‚Äù. It is an iterative algorithm that works well for large datasets and is computationally efficient. It‚Äôs faster than other solvers for large datasets, when both the number of samples and the number of features are large.\nsaga: Similar to ‚Äúsag,‚Äù but it also handles elastic net regularization. It‚Äôs the solver of choice for sparse multinomial logistic regression and it‚Äôs also suitable for very large datasets.\nlbfgs: Stands for ‚ÄúLimited-memory Broyden-Fletcher-Goldfarb-Shanno‚Äù. It is an optimization algorithm suitable for small to medium-sized datasets and performances relatively well compared to other methods and it saves a lot of memory, however, sometimes it may have issues with convergence.\nnewton-cg: A solver that uses a Newton-Conjugate Gradient approach. It‚Äôs useful for larger datasets but can be computationally expensive for very large problems.\n\npenalty: This intends to reduce model generalisation error, and is meant to discourage and regulate overfitting.\nC: This parameter controls the penalty strength and works with penalty. Smaller values specify stronger regularization and high value tells the model to give high weight to the training data.\nclass_weight: As discussed above, it is worth trying to see where no class weights, or balanced weights, make a difference to performance\n\nUsing Grid Search:\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Define parameter grid\nparam_grid = {\n    'solver': ['liblinear', 'sag', 'saga', 'lbfgs', 'newton-cg'],\n    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n    'C': [0.01, 0.1, 1, 10],\n    'class_weight': [None, 'balanced']\n}\n\n# Initialize grid search\ngrid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='f1_weighted')\n\n# Fit grid search\ngrid_search.fit(train_embeddings.cpu().numpy(), train_labels)\n\n# Best parameters\nprint(\"Best parameters:\", grid_search.best_params_)\n\n# Use the best estimator\nclassifier = grid_search.best_estimator_\n\n\nSaving and Loading the Model\nPersisting models allows you to reuse them without retraining.\nSaving model\n\nimport joblib\n\n# Save the logistic regression model\njoblib.dump(classifier, 'logistic_regression_model.pkl')\n\nLoading model\n\n# Load the logistic regression model\nclassifier = joblib.load('logistic_regression_model.pkl')\n\n\n\nFeature Importance and Interpretability\n[ ] To do: SHAP values\nUnderstanding which features contribute to the predictions can be valuable.\nUsing SHAP Values:",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Method 1: Traditional Machine Learning with textual features</span>"
    ]
  },
  {
    "objectID": "setfit.html",
    "href": "setfit.html",
    "title": "15¬† Method 2: SetFit",
    "section": "",
    "text": "15.1 How to fine-tune a model with SetFit?\nLet‚Äôs dive into fine-tuning a model using SetFit. This section will get you started quickly. Feel free to run the code, experiment, and learn by doing. After this walkthrough, we‚Äôll provide a more detailed explanation of each step.\nStart by installing the required packages/modules‚Ä¶\n!pip install setfit datasets\n‚Ä¶ before loading in our dataset. For this example, we‚Äôll use the sst2 (Stanford Sentiment Treebank) dataset, which is great for sentiment analysis as it is single sentences extracted from movie reviews that have been annotated as either positive or negative.\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"SetFit/sst2\")",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Method 2: SetFit</span>"
    ]
  },
  {
    "objectID": "setfit.html#how-to-fine-tune-a-model-with-setfit",
    "href": "setfit.html#how-to-fine-tune-a-model-with-setfit",
    "title": "15¬† Method 2: SetFit",
    "section": "",
    "text": "Prepare the data\nNow we have loaded in the data, let‚Äôs prepare it for the SetFit framework. The benefit of SetFit is being able to perform model fine-tuning with very few labelled data. As such, we will load in data from the SetFit library, but will sample it so we only keep 8 (yes 8!) instances of each label for fine-tuning to simulate a few-shot learning scenario. Note the dataset provided is already split up into training, testing, and validation sets (and it is the training set we will be sampling). The testing set is left unaffected for better evaluation.\n\n# Use 8 examples per class for training\ntrain_dataset = sample_dataset(dataset[\"train\"], label_column=\"label\", num_samples=8)\n\n# Obtain the validation and test datasets\nvalidation_dataset = dataset[\"validation\"]\ntest_dataset = dataset[\"test\"]\n\n\n\nLoading a Pre-trained SetFit Model\nThen initialise a SetFit model using a Sentence Transformer model of our choice. For this example we will use BAAI/bge-small-en-v1.5:\n\nfrom setfit import SetFitModel\n\nmodel = SetFitModel.from_pretrained(\"BAAI/bge-small-en-v1.5\",\nlabels=[\"negative\", \"positive\"])\n\nNow we prepare TrainingArguments for training- the most frequently used arguments (hyperparamters) are num_epochs and max_steps which affect the number of total training steps. We then initialise the Trainer and perform the training\n\nfrom setfit import TrainingArguments\n\nargs = TrainingArguments(\n    batch_size=32,\n    num_epochs=10,\n)\n\nfrom setfit import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    metric=\"accuracy\",\n    column_mapping={\"sentence\": \"text\", \"label\": \"label\"}\n)\n\ntrainer.train()",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Method 2: SetFit</span>"
    ]
  },
  {
    "objectID": "setfit.html#evaluating-the-model",
    "href": "setfit.html#evaluating-the-model",
    "title": "15¬† Method 2: SetFit",
    "section": "15.2 Evaluating the Model",
    "text": "15.2 Evaluating the Model\nAfter training, evaluate the model on the validation dataset.\n\nmetrics = trainer.evaluate()\nprint(metrics)\n\nFinally once we are happy with model performance based on the validation data, we can evaluate using the testing dataset.\n\ntrainer.evaluate(test_dataset)\n\nNow we can save (or load) the model as needed\n\nmodel.save_pretrained(\"setfit-bge-small-v1.5-sst-8-shot\") # Save to a local directory\n\nmodel = SetFitModel.from_pretrained(\"setfit-bge-small-v1.5-sst-8-shot\") # Load from a local directory\n\nOnce a SetFit model has been trained, it can be used for inference straight away using SetFitModel.predict()\n\ntexts = [\n    \"I love this product! It's fantastic.\",\n    \"Terrible customer service. Very disappointed.\",\n]\n\npredictions = trainer.model.predict(texts)\nprint(predictions)\n\nCongratulations! You‚Äôve fine-tuned a SetFit model for sentiment analysis. Feel free to tweak the code, try different datasets, and explore further.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Method 2: SetFit</span>"
    ]
  },
  {
    "objectID": "setfit.html#detailed-overview",
    "href": "setfit.html#detailed-overview",
    "title": "15¬† Method 2: SetFit",
    "section": "15.3 Detailed overview",
    "text": "15.3 Detailed overview\n\nSetFit is a few-shot learning method that fine-tunes sentence embeddings to match the desired classification task. It uses contrastive learning to train embeddings such that examples of the same class are pulled closer in the embedding space, while examples from different classes are pushed apart. After fine-tuning an embedding model (sentence transformer), a lightweight classifier like logistic regression is trained on these embeddings, allowing for efficient and accurate classification with minimal labeled data.\n\n\nWhat is Contrastive Learning?\nThe goal of contrastive learning is to learn an embedding space where similar pairs of samples are positioned close together, while dissimilar pairs are kept far apart.\nPut simply, the model learns by comparing examples: it is trained to pull similar items closer in the representation space and push dissimilar items further apart.\n\n\n\n\n\n\nHow is this achieved?\n\n\n\n\n\nNote the below is directly copied from the SetFit documentation. It is so succinctly written that trying to rewrite it would not do it justice.\nEvery SetFit model consists of two parts: a sentence transformer embedding model (the body) and a classifier (the head). These two parts are trained in two separate phases: the embedding finetuning phase and the classifier training phase. This conceptual guide will elaborate on the intuition between these phases, and why SetFit works so well.\nEmbedding finetuning phase\nThe first phase has one primary goal: finetune a sentence transformer embedding model to produce useful embeddings for our classification task. The Hugging Face Hub already has thousands of sentence transformer available, many of which have been trained to very accurately group the embeddings of texts with similar semantic meaning.\nHowever, models that are good at Semantic Textual Similarity (STS) are not necessarily immediately good at our classification task. For example, according to an embedding model, the sentence of 1) ‚ÄúHe biked to work.‚Äù will be much more similar to 2) ‚ÄúHe drove his car to work.‚Äù than to 3) ‚ÄúPeter decided to take the bicycle to the beach party!‚Äù. But if our classification task involves classifying texts into transportation modes, then we want our embedding model to place sentences 1 and 3 closely together, and 2 further away.\nTo do so, we can finetune the chosen sentence transformer embedding model. The goal here is to nudge the model to use its pretrained knowledge in a different way that better aligns with our classification task, rather than making the completely forget what it has learned.\nFor finetuning, SetFit uses contrastive learning. This training approach involves creating positive and negative pairs of sentences. A sentence pair will be positive if both of the sentences are of the same class, and negative otherwise. For example, in the case of binary ‚Äúpositive‚Äù-‚Äúnegative‚Äù sentiment analysis, (‚ÄúThe movie was awesome‚Äù, ‚ÄúI loved it‚Äù) is a positive pair, and (‚ÄúThe movie was awesome‚Äù, ‚ÄúIt was quite disappointing‚Äù) is a negative pair.\nDuring training, the embedding model receives these pairs, and will convert the sentences to embeddings. If the pair is positive, then it will pull on the model weights such that the text embeddings will be more similar, and vice versa for a negative pair. Through this approach, sentences with the same label will be embedded more similarly, and sentences with different labels less similarly.\nConveniently, this contrastive learning works with pairs rather than individual samples, and we can create plenty of unique pairs from just a few samples. For example, given 8 positive sentences and 8 negative sentences, we can create 28 positive pairs and 64 negative pairs for 92 unique training pairs. This grows exponentially to the number of sentences and classes, and that is why SetFit can train with just a few examples and still correctly finetune the sentence transformer embedding model. However, we should still be wary of overfitting.\nClassifier training phase\nOnce the sentence transformer embedding model has been finetuned for our task at hand, we can start training the classifier. This phase has one primary goal: create a good mapping from the sentence transformer embeddings to the classes.\nUnlike with the first phase, training the classifier is done from scratch and using the labelled samples directly, rather than using pairs. By default, the classifier is a simple logistic regression classifier from scikit-learn. First, all training sentences are fed through the now-finetuned sentence transformer embedding model, and then the sentence embeddings and labels are used to fit the logistic regression classifier. The result is a strong and efficient classifier.\nUsing these two parts, SetFit models are efficient, performant and easy to train, even on CPU-only devices.\nVisual example\nFor example, if we naively used an ‚Äúuntrained‚Äù transformer model to embed data it may look like this in 2D:\n\n\n\nEmbeddings representation of training data with untrained model\n\n\nHowever after fine-tuning the embedding model through contrastive learning, we force similar posts (similar via our classification definition) to be nearer each other, we end up with the below:\n\n\n\nEmbeddings representation of training data with fine-tuned model\n\n\n\n\n\nTo get the most out of SetFit, I am a firm believer in being able to conceptualise what is going on behind the scenes. The SetFit documentation on the Hugging Face website is extremely good, and I think the ‚ÄúConceptual Guides‚Äù pages on SetFit- what is written above and Sampling Strategies are absolute gold dust, and really must be read and understood to get a proper appreciate of SetFit.\nOkay, now let‚Äôs dive deeper into each step.\n\n\nSetting Up the Environment\nDespite SetFit being lightweight, we still recommend you running it in a cloud environment like Google Colab to access the GPUs\nAs such, make sure you are connected to a GPU, we recommend T4 as it‚Äôs a good balance between speed and cost.\n\n\n\n\n\n\nHow do I do this?\n\n\n\n\n\nTo use a GPU in Colab, go to Runtime &gt;Change runtime type and select a GPU under the hardware accelerator option\n\n\n\n\n\nInstall the required packages and modules\n\n%%capture\n# Install  necessary packages\n!pip install setfit datasets evaluate\n\n# Imports \nfrom datasets import load_dataset\nfrom setfit import SetFitModel, Trainer, TrainingArguments\n\n\n\nLoad in the data\nIn the example above, we loaded in data that was already saved on the Hugging Face Hub and within the Hugging Face datasets library. However, we can also load in our own data that we have as a .csv file:\n\ndataset = load_dataset('csv', data_files = path/to/csv)\n\nBy using load_dataset, the dataframe should be read into your environment and be converted into a dataset dictionary (DatasetDict). You can inspect it by running the following command:\n\ndataset\n\nThis will output something like:\nDatasetDict({\n    train: Dataset({\n        features: ['universal_message_id', 'text', 'label'],\n        num_rows: 100\n    })\n})\nYou might notice that this looks slightly different from when we loaded the tweet-eval dataset earlier. In the tweet-eval dataset, the DatasetDict included splits for train, validation, and test. However, in our custom dataset, we only have a train split. This is because we haven‚Äôt explicitly created additional splits like validation or test, so all our data currently resides under the train key.\nThere are two ways we can approach this, we can either split up our dataset using train_test_split (which splits into random train and test subsets) or do splitting outside of python (say in R) and read in the already split datasets individually.\n\ntrain_test_split method:\n\nThis method requires you to first split the data into training and testing data, before then further splitting the training data into training and validation. Because of this some maths is required to workout the split proportions.\n\n# Load your dataset (or create one)\ndataset = load_dataset('csv', data_files=path/to/csv)\n\n# Split dataset into train (70%) and test+validation (30%)\ntrain_test_split = dataset['train'].train_test_split(test_size=0.3, seed=42)\n\n# Split the remaining 30% into validation (15%) and test (15%)\nvalidation_test_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n\ntrain_dataset = train_test_split['train'] # 70% of data\nvalidation_dataset = validation_test_split['train'] # 15% of data\ntest_dataset = validation_test_split['test'] # 15% of data\n\n\nExternal splitting:\n\nIf you have split up the dataset into train, validation, and test splits already, we can read these in individually.\n\ntrain_dataset = load_dataset('csv', data_files = path/to/csv)\nvalidation_dataset = load_dataset('csv', data_files = path/to/csv)\ntest_dataset = load_dataset('csv', data_files = path/to/csv)\n\nWhichever approach you prefer, you can then bring the individual splits together into a single DatasetDict if needed.\n\ncomplete_dataset = DatasetDict({\n    'train': train_dataset['train'],\n    'test': validation_dataset['train'],\n    'validation': test_dataset['train']\n})\n\nNote that by default the dictionary key when you load in a dataset this way is train, which is why for each of train_dataset, validation_dataset and test_dataset are subset by ['train'].\nNow is a good time to verify the datasets we have read in\n\n# Verify the datasets\nprint(\"Train set:\")\nprint(complete_dataset['train'])\n\nprint(\"Validation set:\")\nprint(complete_dataset['validation'])\n\nprint(\"Test set:\")\nprint(complete_dataset['test'])\n\nFor the purposes of this more detailed tutorial, let‚Äôs again read in the sst2 dataset that we will work on:\n\ndataset = load_dataset(\"SetFit/sst2\"\")\n\n\n\n\n\n\n\nWhat about Tokenization?\n\n\n\n\n\nWith SetFit, the tokenization step is abstracted away within the SetFitTrainer class- so unlike Vanilla fine-tuning using the Trainer API we do not need to explicitly create a tokenizer class. Neat!\n\n\n\n\n\nLoading the SetFit Model\nWe load a pre-trained model that is suitable for SetFit:\n\nmodel = SetFitModel.from_pretrained(\"BAAI/bge-small-en-v1.5\",\nlabels=[\"negative\", \"positive\"])\n\nSetFitModel is a wrapper that combines a pre-trained body from sentence-transformers (i.e.¬†the embedding layer) with a classification head (such as a Logistic Regression model).\n\n\nDefine metrics for evaluation\nEvaluating a fine-tuned model often requires more than just accuracy. We also need metrics like precision, recall, and F1-score to understand how well the model handles different classes.\n\nPlease read the corresponding section in the handbook on Model Evaluation for more information on the theory and rationale behind these metrics, and when to chose one over the other.\n\nUnlike the example above where we only evaluate using accuracy and loss, we need to define a function (which we will call compute_metrics()) that will enable the calculation of the necessary evaluation metrics. The code below provides per-class metrics and a weighted F1-score (which is useful for handling imbalanced datasets which we often obtain)\n\nfrom sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n\n    # Calculate precision, recall, and f1 for each label\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=None, labels=[0, 1, 2])\n    # Calculate accuracy\n    accuracy = accuracy_score(labels, predictions)\n    # Calculate weighted and macro f1 scores\n    f1_weighted = f1_score(labels, predictions, average='weighted')\n    f1_macro = f1_score(labels, predictions, average='macro')\n\n    # Prepare the metrics dictionary\n    metrics = {\n        'accuracy': accuracy,\n        'f1_weighted': f1_weighted,\n        'f1_macro': f1_macro\n    }\n\n    class_names = [\"negative\", \"neutral\", \"positive\"]\n    \n    for i, label in enumerate(class_names):\n        metrics[f'precision_{label}'] = precision[i]\n        metrics[f'recall_{label}'] = recall[i]\n        metrics[f'f1_{label}'] = f1[i]\n\n    return metrics\n\n\n\nTraining arguments\nWe set up the training arguments by creating a TrainingArguments class which contains all the hyperparameters you can tune as well:\n\nargs = TrainingArguments(\n    batch_size=16,\n    num_epochs=4,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n\nLet‚Äôs go through each of these hyperparameters/arguments step by step, explaining what they are and how we can choose an appropriate value (where relevant)\n\nbatch_size: Sets the batch size. Sets the batch size, which is the number of samples processed before the model updates its weights during training. Larger batch sizes (e.g., 32, 64) lead to faster training but require more memory (RAM) and may lead to poorer model performance at generalising over unseen data. Smaller batch sizes (e.g.¬†8, 16) are slower but can help when memory is limited or for more stable training, however if it is too small the gradient estimation will be noisy and not converge. In our use cases 16 or 32 tends to work fine.\nnum_epochs: Specifies the number of complete passes through the training dataset (an epoch). We find that fewer epochs (1-3) are suitable for fine-tuning when you‚Äôre only adjusting the final classification head, or if the model is large and already well-trained. More epochs (5-10) may be needed when we‚Äôre training with less data or starting with a less well-trained pre-trained model. We can implement ‚Äúearly stopping‚Äù so that if the model starts to drop in performance after a certain number of epochs, training halts to avoid overfitting.\neval_strategy: Specifies when to run evaluation (validation) on the dataset during training. The values this hyperparameter can take are epoch (runs evaluation at the end of each epoch- common for most training tasks), steps (runs evaluation at a set number of steps, which is sometimes useful for longer training runs or when training for many epochs), or no (don‚Äôt evaluate- do not chose this!). We find that epoch is usually sufficient, but recommend trying steps too if you‚Äôd like more control over evaluation visualisations.\nsave_strategy: Specifies when to save the model checkpoint. Similar to eval_strategy the argument takes epoch (saves a model checkpoint at the end of every epoch- which is ideal for most fine-tuning tasks) or steps (saves checkpoints every set number of steps, useful for longer training runs).\nload_best_model_at_end: Automatically loads the best model (based on the evaluation metric) after training completes. We would have this as true.\n\n\ntrainer = Trainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    metric=compute_metrics,\n    column_mapping={\"masked_context\": \"text\", \"label\": \"label\"} # Map dataset columns to text/label expected by trainer\n)\n\nUnderstanding the Parameters:\nloss_class: The loss function to use. CosineSimilarityLoss works well for similarity tasks. metric: The evaluation metric. Here, we use accuracy. batch_size: Number of samples per batch during training. num_iterations: Controls how many text pairs are generated for contrastive learning. num_epochs: Number of times to iterate over the generated pairs.\n\n\nTraining the Model\nStart the fine-tuning process.\n\ntrainer.train()\n\n\n\nWhat Happens During Training:\nPair Generation: Generates pairs of sentences that are similar or dissimilar.\nContrastive Learning: Fine-tunes the model to bring similar sentences closer in the embedding space and push dissimilar ones apart.\nClassifier Training: Trains a classification head on the embeddings.\n\n\nEvaluating the Model\nAssess the model‚Äôs performance on the validation dataset.\n\nmetrics = trainer.evaluate()\n\nmetrics\n\nFinally once we are happy with model performance based on the validation data, we can evaluate using the testing dataset.\n\ntrainer.evaluate(test_dataset)",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Method 2: SetFit</span>"
    ]
  },
  {
    "objectID": "setfit.html#hyperparameter-optimisation",
    "href": "setfit.html#hyperparameter-optimisation",
    "title": "15¬† Method 2: SetFit",
    "section": "15.4 Hyperparameter Optimisation",
    "text": "15.4 Hyperparameter Optimisation\nBecause of the speed we can train SetFit models, they are suitable for hyperparameter optimisation to help select the best hyperparameters.\nThe hyperparameter optimisation section in the SetFit documentation goes through this in great detail, outlining what is required to do this.\nOur advice would be:\n\nTo only do this as a final step, a cherry on top if you will. The improvements from hyperparameter optimisation will not improve a model built on poor data quality, and should really only be used to eke out some final extra percentage points in model performance.\nFocus on num_epochs, max_steps, and body_learning_rate as the most important hyperparameters for the contrastive learning process.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Method 2: SetFit</span>"
    ]
  },
  {
    "objectID": "setfit.html#extras",
    "href": "setfit.html#extras",
    "title": "15¬† Method 2: SetFit",
    "section": "15.5 Extras",
    "text": "15.5 Extras\nThere are many additional features of SetFit to explore, from multilabel classification to Aspect-Based Sentiment Analysis (ABSA). Since this framework is constantly evolving, it‚Äôs impossible to cover all potential implementations within this handbook. Therefore, we recommend regularly checking online resources and experimenting with different facets of SetFit as needed. You can find well-written guidance on the SetFit website. With experience, you‚Äôll see how the knowledge, understanding, and skills developed through fine-tuning with Hugging Face can be applied across different approaches, including SetFit and traditional fine-tuning.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Method 2: SetFit</span>"
    ]
  },
  {
    "objectID": "setfit.html#footnotes",
    "href": "setfit.html#footnotes",
    "title": "15¬† Method 2: SetFit",
    "section": "",
    "text": "See the page on data labelling for our advice on knowing when you have limited data - rule of thumb here is &lt; 100 posts per class‚Ü©Ô∏é",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Method 2: SetFit</span>"
    ]
  },
  {
    "objectID": "vanilla_finetuning.html",
    "href": "vanilla_finetuning.html",
    "title": "16¬† Method 3: Hugging Face Trainer API",
    "section": "",
    "text": "16.1 How to fine-tune a model?\nLet‚Äôs dive straight into fine-tuning a model using the Hugging Face Trainer API. This section is designed to get you coding quickly. Don‚Äôt worry if everything doesn‚Äôt make sense right away‚Äîfocus on running the code, experimenting with changes, and learning by trial and error. It‚Äôs okay to break things! The idea here is that learning by doing is the fastest way to grasp new concepts. After this quick walkthrough, you‚Äôll find a more detailed explanation of each step to solidify your understanding, with code that is more suited for a real research project.\nSo, let‚Äôs get started! We will first install the required packages/modules‚Ä¶\n!pip install datasets transformers[torch] evaluate\n‚Ä¶ before loading in our dataset. For this example, we‚Äôll use the tweet-eval dataset (from CardiffNLP), which is perfect for sentiment analysis (it‚Äôs a bunch of tweets with associated sentiment labels). We‚Äôll also use BERT as our model for fine-tuning.\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\nBefore fine-tuning, we need to tokenize the text so that it can be processed by the model. We‚Äôll also pad and truncate the text to ensure all sequences have the same length. This can be done in one step using the map function.\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\nSince we want to get started quickly, let‚Äôs use a smaller subset of the data for training and evaluation to speed things up:\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\nNow that our data is tokenized, we‚Äôre ready to load the model and specify the number of labels (in this case, three: ‚Äúpositive‚Äù, ‚Äúnegative‚Äù, and ‚Äúneutral‚Äù).\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels = 3)\nNext we define our TrainingArguments, where you can adjust hyperparameters and training settings. For simplicity, we‚Äôll stick with the default hyperparameters but specify the directory to save checkpoints and the evaluation strategy (which evaluates after each epoch).\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(output_dir=\"./test_trainer\", eval_strategy=\"epoch\")\nHow do we know whether training the model is actually making improvements? We need to evaluate it! To evaluate the model during training, we‚Äôll need a function that calculates and reports our chosen metric (in this case, accuracy). We‚Äôll use the evaluate library for this, and define a function called compute_metrics to compute accuracy.\nimport numpy as np\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\nNow, we can bring everything together by creating a Trainer object. This combines the model, training arguments, datasets, and evaluation function for fine-tuning.\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\nFinally, to fine-tune the model, simply call train():\ntrainer.train()\nCongratulations! You‚Äôve successfully fine-tuned BERT for sentiment analysis. Feel free to experiment by tweaking the code, trying different parameters, and seeing how the model behaves. It‚Äôs all about exploring and understanding what each step does!",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Method 3: Hugging Face Trainer API</span>"
    ]
  },
  {
    "objectID": "vanilla_finetuning.html#how-to-fine-tune-a-model",
    "href": "vanilla_finetuning.html#how-to-fine-tune-a-model",
    "title": "16¬† Method 3: Hugging Face Trainer API",
    "section": "",
    "text": "You will see a warning about some of the pretrained weights not being used and some weights being randomly initialized. Don‚Äôt worry, this is completely normal! The pretrained head of the BERT model is discarded, and replaced with a randomly initialized classification head. You will fine-tune this new model head on your sequence classification task, transferring the knowledge of the pretrained model to it.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nYou‚Äôll see a warning about some of the pre-trained weights not being used and others being randomly initialized. Don‚Äôt worry‚Äîthis is expected! The pre-trained head of the BERT model is replaced with a randomly initialized classification head, which we will fine-tune for our sentiment analysis task, transferring the knowledge of the pretrained model to it.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Method 3: Hugging Face Trainer API</span>"
    ]
  },
  {
    "objectID": "vanilla_finetuning.html#detailed-overview",
    "href": "vanilla_finetuning.html#detailed-overview",
    "title": "16¬† Method 3: Hugging Face Trainer API",
    "section": "16.2 Detailed overview",
    "text": "16.2 Detailed overview\nNow that you have some familiarity with the approach, let‚Äôs go into each step in a bit more detail.\n\nWhere are we working?\nThe example code above could be run pretty effectively locally (say in VS Code) because we were not using much training data (remember we took a sample of 1000 posts) and also we were using default hyperparameters (specifically batch size and number of epochs). However, when you come to fine-tune a model for real, we will want more data and more complex training steps, and therefore we will need to be able to utilise GPU within Google Colab. As such, make sure you are connected to a GPU (we recommend T4 as it‚Äôs a good balance between speed and cost, whereas A100 is quicker but more expensive).\n\n\n\n\n\n\nHow do I do this?\n\n\n\n\n\nTo use a GPU in Colab, go to Runtime &gt;Change runtime type and select a GPU under the hardware accelerator option\n\n\n\n\n\nInstall the required packages and modules\n\n%%capture\n# Install  necessary packages\n!pip install datasets sentence-transformers transformers[torch] accelerate evaluate\n\n# Imports \nfrom datasets import load_dataset, DatasetDict\nimport pandas as pd\nimport numpy as np\nimport evaluate\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom transformers import BertTokenizer\n\n\n\nLoad in the data\nIn the example above, we loaded in data that was already saved on the Hugging Face Hub and within the Hugging Face datasets library. However, we can also load in our own data that we have as a .csv file:\n\ndataset = load_dataset('csv', data_files = path/to/csv)\n\nBy using load_dataset, the dataframe should be read into your environment and be converted into a dataset dictionary (DatasetDict). You can inspect it by running the following command:\n\ndataset\n\nThis will output something like:\nDatasetDict({\n    train: Dataset({\n        features: ['universal_message_id', 'text', 'label'],\n        num_rows: 10605\n    })\n})\nYou might notice that this looks slightly different from when we loaded the tweet-eval dataset earlier. In the tweet-eval dataset, the DatasetDict included splits for train, validation, and test. However, in our custom dataset, we only have a train split. This is because we haven‚Äôt explicitly created additional splits like validation or test, so all our data currently resides under the train key.\nThere are two ways we can approach this, we can either split up our dataset using train_test_split (which splits into random train and test subsets) or do splitting outside of python (say in R) and read in the already split datasets individually.\n*train_test_split method\nThis method requires you to first split the data into training and testing data, before then further splitting the training data into training and validation. Because of this some maths is required to workout the split proportions.\n\n# Load your dataset (or create one)\ndataset = load_dataset('csv', data_files=path/to/csv)\n\n# Split dataset into train (70%) and test+validation (30%)\ntrain_test_split = dataset['train'].train_test_split(test_size=0.3, seed=42)\n\n# Split the remaining 30% into validation (15%) and test (15%)\nvalidation_test_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n\ntrain_dataset = train_test_split['train'] # 70% of data\nvalidation_dataset = validation_test_split['train'] # 15% of data\ntest_dataset = validation_test_split['test'] # 15% of data\n\n\nExternal splitting\n\nIf you have split up the dataset into train, validation, and test splits already, we can read these in individually.\n\ntrain_dataset = load_dataset('csv', data_files = path/to/csv)\nvalidation_dataset = load_dataset('csv', data_files = path/to/csv)\ntest_dataset = load_dataset('csv', data_files = path/to/csv)\n\nWhichever approach you prefer, you can then bring the individual splits together into a single DatasetDict if needed.\n\ncomplete_dataset = DatasetDict({\n    'train': train_dataset['train'],\n    'test': validation_dataset['train'],\n    'validation': test_dataset['train']\n})\n\nNote that by default the dictionary key when you load in a dataset this way is train, which is why for each of train_dataset, validation_dataset and test_dataset are subset by ['train'].\nNow is a good time to verify the datasets we have read in\n\n# Verify the datasets\nprint(\"Train set:\")\nprint(complete_dataset['train'])\n\nprint(\"Validation set:\")\nprint(complete_dataset['validation'])\n\nprint(\"Test set:\")\nprint(complete_dataset['test'])\n\nFor the purposes of this more detailed tutorial, let‚Äôs again read in the tweet-eval dataset that we will work on:\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Method 3: Hugging Face Trainer API</span>"
    ]
  },
  {
    "objectID": "vanilla_finetuning.html#prepare-the-data",
    "href": "vanilla_finetuning.html#prepare-the-data",
    "title": "16¬† Method 3: Hugging Face Trainer API",
    "section": "16.3 Prepare the data",
    "text": "16.3 Prepare the data\nNow we have loaded in the data, let‚Äôs prepare it for fine-tuning. Sentiment tasks involve assigning a label (positive, negative, neutral) to a given text. These labels need to be converted into integers for the model to process them. Hugging Face provides a ClassLabel feature to make this mapping easy and consistent:\nStep 1: Define Class Names and Convert Labels to Integers\nWe‚Äôll first define the sentiment class names and use ClassLabel to convert these labels into integers. This ensures that each class (e.g., ‚Äúpositive‚Äù) is assigned a specific numeric value. Note from the dataset card we can see the order of the labels (i.e.¬†0, 1, 2 corresponds to ‚Äúnegative‚Äù, ‚Äúneutral‚Äù, and ‚Äúpositive‚Äù respectively), so we know what order to assign class_names.\n\nfrom datasets import ClassLabel\n\nclass_names = [\"negative\", \"neutral\", \"positive\"]\n\n# Define the ClassLabel feature\nlabel_feature = ClassLabel(names=class_names)\n\n# Cast the 'label' column to ClassLabel\ncomplete_dataset = complete_dataset.cast_column(\"label\", label_feature)\n\n# Let's verify the mapping and the features of the dataset\nfeatures = complete_dataset['train'].features\nprint(features)\n\nStep 2: Verify the Label Mappings\nOnce we‚Äôve cast the label column to the ClassLabel format, it‚Äôs essential to verify that the mapping between the labels and integers is correct. This ensures that the model is trained with consistent and accurate labels.\n\nprint(label_feature.int2str(0))  # Should return 'negative'\nprint(label_feature.int2str(1))  # Should return 'neutral'\nprint(label_feature.int2str(2))  # Should return 'positive'\n\nStep 3: Create and Verify id2label and label2id Mappings\nFor further robustness, we can explicitly define the mappings between label names and integers using id2label and label2id. This is useful when initializing the model and ensuring consistency across all stages of fine-tuning and inference.\n\n# Create a mapping from integers to labels\nid2label = {i: features[\"label\"].int2str(i) for i in range(len(features[\"label\"].names))}\nprint(id2label)  # e.g., {0: 'negative', 1: 'neutral', 2: 'positive'}\n\n# Create the reverse mapping (from labels to integers)\nlabel2id = {v: k for k, v in id2label.items()}\nprint(label2id)  # e.g., {'negative': 0, 'neutral': 1, 'positive': 2}\n\nStep 4: Verify the Distribution of Labels in the Dataset\nBefore proceeding, it‚Äôs helpful to check the distribution of labels in the dataset. This helps us understand whether the data is balanced or if certain classes (like ‚Äúpositive‚Äù) dominate the dataset.\n\n# Count the number of instances of each label in the dataset\nlabel_distribution = complete_dataset['train'].to_pandas()[\"label\"].value_counts()\nprint(label_distribution)\n\nBy checking the label distribution, you can decide if you need to handle any class imbalance issues (e.g., by applying class weights).",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Method 3: Hugging Face Trainer API</span>"
    ]
  },
  {
    "objectID": "vanilla_finetuning.html#tokenization",
    "href": "vanilla_finetuning.html#tokenization",
    "title": "16¬† Method 3: Hugging Face Trainer API",
    "section": "16.4 Tokenization",
    "text": "16.4 Tokenization\nLLMs operate on tokenized text, meaning the raw text needs to be converted into smaller chunks that the model can understand. This is done via a Tokenizer in Hugging Face, and each model will have it‚Äôs own Tokenizer.\nIn addition to breaking down text into a numerical format, tokenization also handles things like:\n\nSpecial tokens - The tokenizer adds special tokens such as ([CLS], [SEP]) to the input to help the model further understand the specific structure of the input (e.g.¬†CLS provides signals for when a sequence starts).\nTruncating the input - Transformer models have a maximum input length (512 tokens for BERT). If the input sequence (after tokenization) is longer than this limit, truncation ensures only the first 512 tokens are kept, and the rest are discarded. This can be applied by setting truncation = True.\nPadding text - Transformer models require fixed-length inputs. If sequences are shorter than the maximum length, padding ensures the input matches the required length by adding padding tokens ([PAD]) to the end of the sequence. This can be applied by setting padding=\"max_length\".\n\n\nfrom transformers import AutoTokenizer\n\n# Load the pre-trained tokenizer for BERT via the relevant checkpoint\nbert_model_ckpt = \"google-bert/bert-base-uncased\"\ntokenizer_bert = AutoTokenizer.from_pretrained(bert_model_ckpt)\n\n# Define a tokenization function\ndef bert_tokenize_text(text):\n    return tokenizer_bert(text[\"text\"], padding = \"max_length\", truncation = True)\n  \n# Apply tokenization to the dataset\ntokenized_dataset = complete_dataset.map(bert_tokenize_text, batched=True)\n\n\n\n\n\n\n\nClass Imbalance Issues\n\n\n\n\n\nOften times we will find our labelled training data displays class imbalance, where one or more classes are under-represented. This can lead to poor classification performance (often being biased towards the majority class) and give us untrustworthy evaluation metrics.\nTo address class imbalance, we can adjust the training process to pay more attention to the minority classes. One common approach is to assign class weights, which penalise the model more for making errors on minority classes and less for making errors on majority classes.\nHere is a step-by-step guide for calculating class weights and integrating them into the Hugging Face Trainer API.\nStep 1: Calculating Class Weights\nWe first calculate class weights based on the inverse of the class distribution. The idea is to give more weight to underrepresented classes and less to overrepresented classes.\n\n# Calculate class weights based on the distribution of the classes\nclass_weights = (1 - (complete_dataset['train'].to_pandas()[\"label\"].value_counts().sort_index() / len(complete_dataset['train'].to_pandas()))).values\n\nclass_weights\n\nStep 2: Print Class Weights for Verification\nLet‚Äôs print the class weights and their corresponding class names to verify that everything is correct.\n\n# Print the weights and their corresponding class names\nfor idx, weight in enumerate(class_weights):\n    print(f\"Class {label_feature.int2str(idx)} (Index {idx}): Weight = {weight}\")\n\nStep 3: Convert Class Weights to a PyTorch Tensor\nNow, we convert the class weights into a format that the Trainer API can use‚Äîspecifically, a PyTorch tensor.\n\nimport torch\n\n# Convert class weights to a PyTorch tensor and move it to the GPU\nclass_weights = torch.from_numpy(class_weights).float().to(\"cuda\")\nclass_weights\n\nNote that this is not the only approach to address class imbalance, you could also look at\n\nOversampling the minority class\nUndersampling the majority class\nSMOTE (Synethetic Minority Over-sampling Technique) [generate synthetic data]\n\nHowever, based on our experience, using class weights offers the best balance between time and payoff for our tasks.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Method 3: Hugging Face Trainer API</span>"
    ]
  },
  {
    "objectID": "vanilla_finetuning.html#define-metrics-for-evaluation",
    "href": "vanilla_finetuning.html#define-metrics-for-evaluation",
    "title": "16¬† Method 3: Hugging Face Trainer API",
    "section": "16.5 Define metrics for evaluation",
    "text": "16.5 Define metrics for evaluation\nEvaluating a fine-tuned model often requires more than just accuracy. We also need metrics like precision, recall, and F1-score to understand how well the model handles different classes.\n\nPlease read the corresponding section in the handbook on Model Evaluation for more information on the theory and rationale behind these metrics, and when to chose one over the other.\n\nJust like the example above, we need to define a function (which we will call compute_metrics()) that will enable the calculation of the necessary evaluation metrics. The code below provides per-class metrics and a weighted F1-score (which is useful for handling imbalanced datasets which we often obtain)\n\nfrom sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n\n    # Calculate precision, recall, and f1 for each label\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=None, labels=[0, 1, 2])\n    # Calculate accuracy\n    accuracy = accuracy_score(labels, predictions)\n    # Calculate weighted and macro f1 scores\n    f1_weighted = f1_score(labels, predictions, average='weighted')\n    f1_macro = f1_score(labels, predictions, average='macro')\n\n    # Prepare the metrics dictionary\n    metrics = {\n        'accuracy': accuracy,\n        'f1_weighted': f1_weighted,\n        'f1_macro': f1_macro\n    }\n\n    class_names = [\"negative\", \"neutral\", \"positive\"]\n    \n    for i, label in enumerate(class_names):\n        metrics[f'precision_{label}'] = precision[i]\n        metrics[f'recall_{label}'] = recall[i]\n        metrics[f'f1_{label}'] = f1[i]\n\n    return metrics\n\nWhilst this looks like overkill, and probably is for what you present to a client when reporting evaluation metrics, having all of this information when understanding model performance is extremely useful- it is better to have too much information here than too little.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Method 3: Hugging Face Trainer API</span>"
    ]
  },
  {
    "objectID": "vanilla_finetuning.html#instantiate-the-trainer",
    "href": "vanilla_finetuning.html#instantiate-the-trainer",
    "title": "16¬† Method 3: Hugging Face Trainer API",
    "section": "16.6 Instantiate the Trainer",
    "text": "16.6 Instantiate the Trainer\nNow we have loaded in our data, prepared it for fine-tuning, and created our function for model evaluation, it is time to load in the model we will be fine-tuning and instantiate our Trainer.\nLet‚Äôs start by loading in the pre-trained BERT model:\n\nmodel = AutoModelForSequenceClassification.from_pretrained(bert_model_ckpt, num_labels=len(class_names), id2label = id2label, label2id = label2id)\n\nBut what does this actually do or mean?\n\nAutoModelForSequenceClassification: is a class from the transformers library. It‚Äôs a pre-defined architecture that uses the pre-trained model and adds a classification head (a fully connected layer) on top of it. The classification head is responsible for predicting which class a piece of text belongs to. Note BERT models actually have a synonymous class called BertForSequenceClassification, however using AutoModelForSequenceClassification means we could in theory swap out bert_model_ckpt for another transformer model (note this bert_model_ckpt comes from our tokenization step).\nfrom_pretrained(model_name): this method loads a pre-trained version of the model. Remember we are not training a deep neural network from scratch- we‚Äôre leveraging a pre-trained model that already understands language structures.\nnum_labels=len(class_names): this specifies the number of classes or categories that the model needs to classify text into. num_labels ensures that the final layer of the model has the correct number of outputs corresponding to the number of possible classes.\nid2label = id2label & label2id = label2id: ensures consistent handling of labels in the model- this makes it much easier to understand the predictions as they can be translated between integers and labels (rather than just integers).\n\nAnd then we set up the training arguments by creating a TrainingArguments class which contains all the hyperparameters you can tune as well:\n\ntraining_args = TrainingArguments(output_dir = \"./results\",\n                                num_train_epochs = 5,\n                                learning_rate = 1e-5,\n                                per_device_train_batch_size = 16,\n                                per_device_eval_batch_size = 16,\n                                weight_decay = 0.01,\n                                warmup_steps = 600,\n                                eval_strategy = \"epoch\",\n                                save_strategy = \"epoch\",\n                                logging_steps = 100,\n                                logging_dir = \"./logs\",\n                                fp16 = True,\n                                load_best_model_at_end = True,\n                                metric_for_best_model = \"eval_loss\",\n                                push_to_hub = False)\n\nLet‚Äôs go through each of these hyperparameters/arguments step by step, explaining what they are and how we can choose an appropriate value (where relevant)\n\noutput_dir: Directory where model checkpoints and training outputs will be saved.\nnum_train_epochs: Specifies the number of complete passes through the training dataset (an epoch). We find that fewer epochs (1-3) are suitable for fine-tuning when you‚Äôre only adjusting the final classification head, or if the model is large and already well-trained. More epochs (5-10) may be needed when we‚Äôre training with less data or starting with a less well-trained pre-trained model. We can implement ‚Äúearly stopping‚Äù so that if the model starts to drop in performance after a certain number of epochs, training halts to avoid overfitting.\nlearning_rate: Controls how much to change the model in response to the estimated error each time the model weights are updated. Smaller values such as 1e-5 and 1e-6 are preferred when fine-tuning an entire model or working with a large dataset, as it ensures more gradual learning (a higher learning rate could destroy the pre-learned features). However, if you are only tuning the final classification head (while keeping other layers froze), you can use higher learning rates such as 1e-3, as it allows the model to adjust faster.\nper_device_train_batch_size: Sets the batch size, which is the number of samples processed before the model updates its weights during training. Larger batch sizes (e.g., 32, 64) lead to faster training but require more memory (RAM) and may lead to poorer model performance at generalising over unseen data. Smaller batch sizes (e.g.¬†8, 16) are slower but can help when memory is limited or for more stable training, however if it is too small the gradient estimation will be noisy and not converge. In our use cases 16 or 32 tends to work fine.\nper_device_train_batch_size: Sets the batch size for evaluation (validation) on each device. Best to keep it to the same as the per_device_train_batch_size.\nweight_decay: Weight decay is a regularisation technique that helps prevent overfitting by penalizing large weights in the model. It reduces the model‚Äôs complexity, and values around 0.01 are common. If you notice overfitting it might be worth trying larger values (e.g.¬†0.1) or if you notice underfitting it might be worth smaller values (e.g.¬†0.001 or even 0).\nwarmup_steps: These gradually increase the learning rate at the beginning of training before settling at the specified rate. This helps stabilise training and prevents the model from making large adjustments too early. Typically you might want to set this as ~10% of the total training steps, though 600 isn‚Äôt too bad.\neval_strategy: Specifies when to run evaluation (validation) on the dataset during training. The values this hyperparameter can take are epoch (runs evaluation at the end of each epoch- common for most training tasks), steps (runs evaluation at a set number of steps, which is sometimes useful for longer training runs or when training for many epochs), or no (don‚Äôt evaluate- do not chose this!). We find that epoch is usually sufficient, but recommend trying steps too if you‚Äôd like more control over evaluation visualisations.\nsave_strategy: Specifies when to save the model checkpoint. Similar to eval_strategy the argument takes epoch (saves a model checkpoint at the end of every epoch- which is ideal for most fine-tuning tasks) or steps (saves checkpoints every set number of steps, useful for longer training runs).\nlogging_steps: Specifies how often (in terms of steps) to log training information such as loss. Smaller values (e.g.¬†10-50 steps) provide more frequent updates by may slow down training due to excessive logging, whereas larger values (e.g.¬†200-500 steps) provide less frequent updates but are useful for faster training. We would suggest 100 steps is a balanced choice, giving regular feedback without overwhelming the logs.\nlogging_dir: Directory where logs are saved\nfp16: This boolean enables mixed precision training using 16-bit floating point (FP16) arithmetic. This speeds up training and reduces memory usage without sacrificing model performance.\nload_best_model_at_end: Automatically loads the best model (based on the evaluation metric) after training completes. We would have this as true.\nmetric_for_best_model: Specifies the evaluation metric used to determine which model is ‚Äúbest‚Äù during training. eval_loss is common for classification tasks, and therefore in conjunction with load_best_model_at_end it means the model with the lowest validation loss will be saved. You can also specify other metrics like accuracy, F1-score, precision, etc., depending on your task and what is most important for evaluation.\npush_to_hub: Determines whether the model and results shoul dbe uploaded to the Hugging Face Hub.\n\nNow, finally, we can instantiate our Trainer. Here we provide our Trainer class all the information required to train and evaluate the model, namely the model itself, the training arguments, the training dataset, the validation dataset, and the function to compute evaluation metrics\n\ntrainer = Trainer(\n    model = model,                         # The pre-trained model\n    args = training_args,                  # Training arguments\n    train_dataset = tokenized_dataset['train'],  # Training dataset\n    eval_dataset = tokenized_dataset['validation'],  # Evaluation dataset\n    compute_metrics = compute_metrics,     # Custom metrics\n)\n\nNow our Trainer is instantiated (as trainer), we can call the trainer and train it using train(). Note that this will take quite a while (~20/30 mins) even on a GPU as we are training on\n\ntrainer.train()\n\nNow, depending on our training arguments, you will see an output printed after every epoch or n steps, which shows our evaluation metrics as below\n\n\n\nExample output from fine-tuning a model for peaks and pits. Note this training is not very successful, as seen by the poor scores for the labels. However the take home message is more the structure of the output.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Method 3: Hugging Face Trainer API</span>"
    ]
  },
  {
    "objectID": "vanilla_finetuning.html#model-evaluation",
    "href": "vanilla_finetuning.html#model-evaluation",
    "title": "16¬† Method 3: Hugging Face Trainer API",
    "section": "16.7 Model evaluation",
    "text": "16.7 Model evaluation\nAfter training, we can evaluate the model on the validation set:\n\ntrainer.evaluate()\n\nThis will provide the evaluation metrics defined earlier for our best performing model.\nFinally, once we are happy with our model (See Model Evaluation page first!), we may want to evaluate the model on the held out test dataset. It is these values that we would report when reporting final model performance.\n\ntest_metrics = trainer.evaluate(tokenized_dataset['test'])\n\ntest_metrics",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Method 3: Hugging Face Trainer API</span>"
    ]
  },
  {
    "objectID": "vanilla_finetuning.html#footnotes",
    "href": "vanilla_finetuning.html#footnotes",
    "title": "16¬† Method 3: Hugging Face Trainer API",
    "section": "",
    "text": "See the page on data labelling for our advice on knowing when you have ‚Äúenough‚Äù - spoiler, it‚Äôs difficult‚Ü©Ô∏é",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Method 3: Hugging Face Trainer API</span>"
    ]
  },
  {
    "objectID": "model_evaluation.html",
    "href": "model_evaluation.html",
    "title": "17¬† Model Evaluation",
    "section": "",
    "text": "17.1 Evaluation metrics\nThere are myriad evaluation metrics that can be used when assessing model performance. They all have pros and cons, and different projects will require us to focus on different evaluation metrics. For example, the evaluation metric we use for a classification task will be different then one used for regression or clustering, and even appropriate metrics will differ between different classification tasks.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "model_evaluation.html#evaluation-metrics",
    "href": "model_evaluation.html#evaluation-metrics",
    "title": "17¬† Model Evaluation",
    "section": "",
    "text": "Metrics for classification tasks\nClassification models are the models we will most often fine-tune and need to evaluate.\n\n\n\n\n\n\nüê∫ The Boy Who Cried Wolf üê∫\n\n\n\nHere me out.\nThroughout this page, we‚Äôll use the story of the Boy Who Cried Wolf to discuss confusion matrices and in particular false positives, false negatives, true positives, and true negatives.\nWith the Boy Who Cried Wolf, we can consider the boy‚Äôs perspective in the same way as how the model would be reporting results to us. For example, we can visualise a confusion matrix of the story:\n\n\n\n\n\n\n\nTrue Value\n\n\n\n\nWolf\nNo Wolf\n\n\nPredicted Value\nWolf\nTrue Positive\nFalse Positive\n\n\nNo Wolf\nFalse Negative\nTrue Negative\n\n\n\nIn this case, we can see that a:\nTrue Positive (TP) is when the boy claims there to be a wolf (i.e.¬†our model has said that a positive event has occurred) AND there actually is a wolf\nFalse Positive (FP) is when the boy claims there to be a wolf (i.e.¬†our model has said that a positive event has occurred) AND there actually is no wolf. Known as Type 1 Error\nTrue Negative (TN) is when the boy claims there to be no wolf (i.e.¬†our model has said that a negative event has occurred) AND there actually is no wolf\nFalse Negative (FN) is when the boy claims there to be no wolf (i.e.¬†our model has said that a negative event has occurred) AND there actually is a wolf. Known as Type 2 Error\nNotice that the total in each row gives all predicted positives (TP + FP) and all predicted negatives (FN + TN), regardless of validity. The total in each column, meanwhile, gives all real positives (TP + FN) and all real negatives (FP + TN) regardless of model classification.\nFor the purposes of the examples below, let‚Äôs assume our boy has spent 100 days in the field, and has therefore made 100 cries as to whether a wolf was present or not. Our confusion matrix could look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrue Value\n\n\n\n\n\n\nWolf\nNo Wolf\n\n\nPredicted Value\nWolf\n20\n(True Pos)\n5\n(False Pos)\n\n\nNo Wolf\n15\n(False Neg)\n60\n(True Neg)\n\n\n\n\n\n\nAccuracy\nMeasures the proportion of correct predictions out of the total number of predictions. It is calculated as:\n\\[\nAccuracy = \\dfrac{TP + TN}{(TP + TN + FP + FN) }\n\\]\nTherefore by plugging in our values from the above Confusion Matrix, we get a score of:\n\\[\n0.8 = \\dfrac{20 + 60}{(20 + 60 + 5 + 15) }\n\\]\nAdvantages:\n\nVery easy to interpret and understand (especially to a non-techinical stakeholder)\nWorks well when the classes are balanced\n\nDisadvantages:\n\nIs a poor metric for imbalanced data (especially when classifying a rare class label is important!). Imagine a model that predicts whether a social media post contains hate speech. If hate speech is only present in 1 in 1000 posts (a made up figure!), then a model that always predicts a post as ‚Äúnot containing hate speech‚Äù will have an accuracy of 99.9%- despite always incorrectly classifying posts that truly contain hate speech, failing at the model‚Äôs primary goal.\n\n\n\nPrecision\nThe proportion of positive identifications that were actually correct. Precision focuses on the correctness of positive predictions\n\\[\nPrecision = \\dfrac{TP}{(TP + FP) }\n\\]\nAgain, plugging in our Boy Who Cried Wolf values, we end up with a Precision score of:\n\\[\n0.8 = \\dfrac{20}{(20 + 5) }\n\\]\nPrecision measures the extent of error caused by False Positives (first wolf cry in the story)\nAdvantages:\n\nPrecision is particularly useful when false positives are costly (e.g., if you‚Äôve built a model to identify hot dogs from regular ol‚Äô doggos, high precision ensures that normal dogs are not misclassified as hot dogs).\nFocuses on the quality of positive predictions and can indicate trustworthiness (a high precision score means that when the model predicts a positive class, it is likely to be correct).\n\nDisadvantages:\n\nIt ignores false negatives, so doesn‚Äôt provide a complete picture of model performance (especially when missing positive instances is important)\nMay lead to overly conservative models- if we focus too much on precision then we may potentially miss many true positives (say we are classifying for sentiment, and we know that 40 out of 100 posts are positive, if our model only predicts 5 as being positive, but was correct each time, then we end up with 100% precision however we know we‚Äôve actually missed out on 35 other positive posts)\n\n\n\nRecall\nThe proportion of actual positives that were identified correctly. Recall focuses on capturing all relevant instances, and is also known as true positive rate (TPR) or Sensitivity\n\\[\nRecall = \\dfrac{TP}{(TP + FN) }\n\\]\nAgain, plugging in our Boy Who Cried Wolf values, we end up with a Recall score of:\n\\[\n0.57 = \\dfrac{20}{(20 + 15) }\n\\]\nRecall measures the extent of error caused by False Negatives (second wolf cry in the story)\nNote this score is lower than Precision (0.8), suggesting that a fair amount of times the boy says there is no wolf when there actually is a wolf! Indeed, it means the boy is more likely to to say there is no wolf and get it wrong than say there is a wolf and get it wrong.\nAdvantages:\n\nUseful when it‚Äôs critical to catch as many positive cases as possible (e.g., when the cost of false negatives is high). This is often the case in rare events, where missing a positive case can have serious consequences. This is often the case in medical diagnoses, where a false negative (predicting someone does not have a disease, when in reality they do) is bad\n\nDisadvantages:\n\nCan be misleading if used alone, as it doesn‚Äôt consider false positives. A model can have high recall by simply predicting all cases as positive.\n\n\n\n\n\n\n\nPrecision vs Recall trade-off\n\n\n\n\n\nAs with many of the fun things in life, there is often a trade-off between precision and recall.\nBroadly we can think of precision as a measure of quality and recall a measure of quantity.\nThe trade-off occurs because increasing the threshold for classification will result in fewer false positives (increasing precision) but also more false negatives (decreasing recall), and vice versa. You cannot eliminate both false positives and false negatives simultaneously without the presence of a perfect classifier, so improving one often comes at the cost of the other.\n\n\n\n\n\nSpecificity\nSpecificity, also referred to as the True Negative Rate measures the proportion of true negative cases out of all the actual negative cases.\n\\[\nSpecificity = \\dfrac{TN}{(TN + FP) }\n\\]\nApplying to our Boy Who Cried Wolf model we would end up with a value of:\n\\[\n0.92 = \\dfrac{60}{(60 + 5) }\n\\] Advantages:\n\nA useful metric in cases where false positives are a concern (e.g., it would be costly if every time the boy cried wolf (irrespective of the true value) the villagers had to stop what they were doing and round up all their sheep and lock their doors).\nIt can provide a nice complement to recall to give a fuller picture of the model‚Äôs performance.\n\nDisadvantages:\n\nNot often used alone, as it doesn‚Äôt account for the model‚Äôs ability to identify positive cases (which is captured by recall).\n\n\n\nF1 Score\nThe F1 score is the harmonic mean of precision and recall. It aims to balance the trade-off between the two, and as it considers both precision and recall, it accounts for both False Positives and False Negatives.\n\\[\nF1 Score = 2  \\times \\dfrac{Precision \\times Recall}{(Precision + Recall) }\n\\]\n\n\n\n\n\n\nHarmonic mean vs arithmetic mean\n\n\n\n\n\nThe harmonic mean is calculated by dividing the number of observations, or entries in the series, by the reciprocal of each number. In contrast, the arithmetic mean is simply the sum of a series of numbers divided by the count of numbers in that series.\nThe harmonic mean is often used to calculate the average of the ratios or rates. It is the most appropriate measure for ratios and rates because it equalises the weights of each data point. In this case, the F1 score calculates the average of precision and recall (themselves both ratios ranging from 0 to 1). It ensures that a low value in either precision or recall has a significant impact on the overall F1 score, thus incentivising a balance between the two.\nFor example, imagine a model with a Precision score of 0.2 and a Recall score of 0.8, the arithmetic mean would be:\n\\[\n0.5 = \\dfrac{0.8 + 0.2}{2}\n\\]\nwhereas the harmonic mean (F1 score) would be lower:\n\\[\n0.32 = 2  \\times \\dfrac{0.2 \\times 0.8}{(0.2 + 0.8) }\n\\]\n\n\n\nNow we can plug in our Boy Who Cried Wolf values as before to calculate F1:\n\\[\n0.67 = 2  \\times \\dfrac{0.8 \\times 0.57}{(0.8 + 0.57) }\n\\] Advantages:\n\nUseful when there is a need to balance precision and recall, particularly in imbalanced datasets.\nThere is a lot to be said about providing a single metric that combines both precision and recall for a client for simplicity.\n\nDisadvantages:\n\nWhilst a single metric can be a positive, it also means we have limited information in understanding our model. For this reason it‚Äôs highly advised to look at additional metrics during model evaluation, but perhaps only presenting F1 to a client (if relevant)\nIt assumes precision and recall are equally important- which we know from above is not always the case.\n\n\n\nROC-AUC (Receiver Operating Characteristic - Area Under the Curve)\n\nThe output of a binary classification model (i.e.¬†it only outputs a positive or negative class, ‚Äúwolf‚Äù vs ‚Äúnot wolf‚Äù) is typically derived from a regression model, like logistic regression, that predicts a probability between 0 and 1. This value represents the probability that a wolf is present. For example, a prediction of 0.50 indicates a 50% likelihood of a wolf being present, while a prediction of 0.80 suggests a 80% likelihood.\nBut how do we decide when the model should predict ‚Äúwolf‚Äù or ‚Äúnot wolf‚Äù? To do this, we need to chose a classification threshold. Predictions with probabilities above this threshold are assigned to the positive class (‚Äúwolf‚Äù), while those below it are assigned to the negative class (‚Äúnot wolf‚Äù).\nAlthough 0.5 may seem like an intuitive threshold, it might not be suitable when the cost of one type of misclassification is higher than the other, or when the classes are imbalanced. Different thresholds lead to different outcomes in terms of true positives, false positives, true negatives, and false negatives. Therefore, selecting an appropriate threshold depends on understanding which type of misclassification‚Äîfalse positives or false negatives‚Äîhas the greater cost or impact in your specific context.\n\nThe ROC curve is a graph that shows how well a classification model performs, and allows us to see how the model makes decisions at different levels of classification threshold. It plots the true positive rate (recall) against the false positive rate (1 - specificity) at different threshold levels. The AUC (Area Under the Curve) represents the likelihood that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. A rough rule of thumb is that the accuracy of tests with AUCs between 0.50 and 0.70 is low; between 0.70 and 0.90, the accuracy is moderate; and it is high for AUCs over 0.90.\nAdvantages:\n\nProvides a holistic view of model performance across various thresholds.\nEffective for comparing models without needing to choose a specific threshold.\n\nCons:\n\nMay be overly optimistic for highly imbalanced datasets. Even if a model performs well overall, it may still perform poorly for minority classes, which AUC may not fully capture.\n\n\n\nPrecision-recall curve\nAUC and ROC work well for comparing models when the dataset is roughly balanced between classes. When the dataset is imbalanced, precision-recall curves (PRCs) and the area under those curves may offer a better comparative visualization of model performance. Precision-recall curves are created by plotting precision on the y-axis and recall on the x-axis across all thresholds.\n\n\n\nLoss Function - Binary Cross Entropy (BCE)\nBCE alias ‚ÄòLog Loss‚Äô is the default loss function for text classification tasks. Intuitively, BCE penalises confidently wrong predictions, so if the true class is ‚Äòspam‚Äô and our model predicts ‚Äò0.99 spam‚Äô then the loss is very low. On the other hand, if the true label is ‚Äònot spam‚Äô and the model predicts ‚Äò0.99 spam‚Äô then the loss is very high.\nLet‚Äôs visualise the relationship between confidence, correctness, and loss to build the intuition for why BCE is an appropriate loss function for text classification. Hover over points to see the precise 1 values for loss and predicted probabilities:\nInterpreting the chart: 1. For true label == 1: - Loss increases dramatically as predictions approach 0 - Minimal loss when predictions are close to 1 - Loss approaches infinity as probability approaches 0\n\nFor true label == 0:\n\nLoss increases dramatically as predictions approach 1\nMinimal loss when predictions are close to 0\nLoss approaches infinity as probability approaches 1\n\n\n\n\n\n\n\n\nFormally, BCE is:\n\n\n\n\n\nFor each prediction (p), and each true label (y): \\[BCE(y, p) = - \\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(p_i) +(1-y_i) \\log(1 - p_i)]\\] Where:  \\(N\\) = Number of samples  \\(y_i\\) = True label for the \\(i_{th}\\) sample  \\(p_i\\) = Prediction for the \\(i_{th}\\) sample  \\(\\log = ln\\) = the natural logarithm (log with a base of e)\n\n\n\nAdvantages:\n\nTakes into account the predicted probabilities, not just the final classification.\nEncourages models to output well-calibrated probabilities rather than just hard predictions.\n\nDisadvantages:\n\nCan be difficult to interpret compared to more intuitive metrics like accuracy or precision.\nMore sensitive to poorly calibrated models.\n\n\nOn Multi-class classification\nThe information provided above all relate to binary classification, where the output is one of two classes (positive or negative, spam or not spam, wolf or no wolf). In many cases however, we need to perform multi-class classification. This is treated as an extension of binary classification. If each data point can only be assigned to one class, then the classification problem can be handled as a binary classification problem, where one class contains one of the multiple classes, and the other class contains all the other classes put together. The process can then be repeated for each of the original classes.\nFor example, in a three-class multi-class classification problem (i.e.¬†sentiment), where you‚Äôre classifying examples with the labels Positive, Neutral, and Negative, you could turn the problem into two separate binary classification problems. First, you might create a binary classifier that categorizes examples using the label Positive + Neutral and the label Negative. Then, you could create a second binary classifier that reclassifies the examples that are labelled Positive + Neutral using the label Positive and the label Neutral.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "model_evaluation.html#footnotes",
    "href": "model_evaluation.html#footnotes",
    "title": "17¬† Model Evaluation",
    "section": "",
    "text": "to 3 dp‚Ü©Ô∏é",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "systematic_model_improvement.html",
    "href": "systematic_model_improvement.html",
    "title": "18¬† Systematically Improving a Model",
    "section": "",
    "text": "Data & Discrete Variables\nWe are going to keep saying this because it is so important: start with data. Everything else you do will have a severely-reduced and limited effect without enough data, without enough variance within your data, or without the patterns you want the model to learn.\nAnd yet it is common when starting out in Data Science to focus on model/algorithm selection, hyper-parameter tuning, optimisers, dropout & regularisation, or anything else except the thing we should be focusing on: the data. Whilst those things are important, keep in mind that the data determines what the model can learn, algorithms & hyper-parameters etc. determine how efficiently the model learns what‚Äôs in the data.\nWith that in mind, we should prioritise data strategy above everything else. This means spending the majority of our time collecting, cleaning, and labelling data. Barring elementary mistakes and training run errors, normally your model is not as good as it could be because:\nWhen thinking about 1., keep in mind that this is likely to be true both at the overall dataset level, and within the groups present in your data. With some groups needing more data than others. You can assess whether a group needs more data as a function of 1) how well the model performs on that group, and 2) how frequent the group is expected to be when you deploy the model.\nIf the model performs extremely well on a group, you should prioritise finding data for other groups. If a model is performing poorly, but the expected frequency of that group is low, prioritise finding data for other groups. Where possible we want to target the highest impact areas.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Systematically Improving a Model</span>"
    ]
  },
  {
    "objectID": "systematic_model_improvement.html#stepping-into-the-great-unknown",
    "href": "systematic_model_improvement.html#stepping-into-the-great-unknown",
    "title": "18¬† Systematically Improving a Model",
    "section": "18.1 Stepping into the Great Unknown",
    "text": "18.1 Stepping into the Great Unknown\nBeyond these systematic steps, there is more advanced (but shaky) ground that we can explore. However, it becomes increasingly harder to predict which of these steps are going to be fruitful - we need to isolate them and experiment with each of them, and react accordingly to the experimental data.\n\nWas training stable at the start or do I need to add some warm up?\nDo the training & loss curves suggest under or overfitting? If you‚Äôre not slightly overfitting, train for longer!\nWill using a bigger model significantly impact performance?\nAm I using the right activation function? Would switching to ReLU, or Leaky ReLU have an impact? What about Sigmoid?\nIs my loss function optimal?\nShould I be prioritising Precision or Recall for one class over another? Could I optimise the probability cut-off for classifications to achieve this?\nWhat optimiser am I using, can I do better than the default (AdamW)?\nIs my learning rate too high? How does the learning rate scheduler work in my optimiser?\nIs batch size having any impact on learning?\nShould I be early stopping? Am I saving the best model?\nWould ensembling help? 3\nIs there any post-processing I can do to move the needle?\n\n\n\n\n\n\n\nTip\n\n\n\nTraining neural networks is an art form not a science, aim to learn as you experiment.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Systematically Improving a Model</span>"
    ]
  },
  {
    "objectID": "systematic_model_improvement.html#monitoring-continuous-improvement",
    "href": "systematic_model_improvement.html#monitoring-continuous-improvement",
    "title": "18¬† Systematically Improving a Model",
    "section": "18.2 Monitoring & Continuous Improvement",
    "text": "18.2 Monitoring & Continuous Improvement\nModern machine learning models are excellent at identifying and compressing patterns in their training data. A good model will compress enough patterns that it can start to interpolate between them, this feature allows them to deal with ‚Äòsimilar but slightly different‚Äô novel data. However, input data will change over time, and the further the distribution of the new input data gets from the training data‚Äôs distribution, the worse the model will perform. If we want our model to maintain performance over time, we have to label new data and re-train the model with all previous data and the new data. To save time, we should aim to select the new data directly from the projects we used the model on, and the samples we hand labelled.\nOne way to do this is to combine labelling new data with our sampling of model outputs. For any project or dataset that we use the model on, we simply have to verify that it is performing as expected. This means looking at our data - taking samples of our model‚Äôs predictions and confirming or rejecting the predictions. We can then score our model on our samples, and approximate how well the model is calibrated for the dataset.\nIn the process you will find samples that are definitely correct, definitely incorrect, and borderline. Correct/confirm the labels and then save them somewhere that they can be added to the corpus for future training and evaluation runs.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Systematically Improving a Model</span>"
    ]
  },
  {
    "objectID": "systematic_model_improvement.html#footnotes",
    "href": "systematic_model_improvement.html#footnotes",
    "title": "18¬† Systematically Improving a Model",
    "section": "",
    "text": "Visit the model_evaluation document for a primer on loss functions‚Ü©Ô∏é\na lot more data‚Ü©Ô∏é\n‚ÄòBad practices‚Äô like training the model with 5 different seeds have a ‚Äòfairly reliable‚Äô positive impact on performance‚Ü©Ô∏é",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Systematically Improving a Model</span>"
    ]
  },
  {
    "objectID": "model_inference.html",
    "href": "model_inference.html",
    "title": "19¬† Model Inference",
    "section": "",
    "text": "Use this page for the info on model inference (based on Aoife‚Äôs work on how it‚Äôs different conceptually to training, how to speed up inference, the code needed, the services one uses etc)",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Model Inference</span>"
    ]
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "20¬† Our Packages",
    "section": "",
    "text": "20.1 ParseR\nParseR is the collective name for the techniques SAMY uses for text analysis. It‚Äôs primarily based on the tidytext philosophy and the analysis is normally carried out in R.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#connectr",
    "href": "packages.html#connectr",
    "title": "20¬† Our Packages",
    "section": "20.2 ConnectR",
    "text": "20.2 ConnectR\n\nConnectR is our package for network analysis. It helps the user find important individuals by graphing retweets and important communities by graphing mentions.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#segmentr",
    "href": "packages.html#segmentr",
    "title": "20¬† Our Packages",
    "section": "20.3 SegmentR",
    "text": "20.3 SegmentR\n\nSegmentR is the collective name for the techniques SAMY uses to find latent groups in data.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#bertopicr",
    "href": "packages.html#bertopicr",
    "title": "20¬† Our Packages",
    "section": "20.4 BertopicR",
    "text": "20.4 BertopicR\nBertopicR is our package which allows access to BERTopic‚Äôs modelling suite in R via reticulate.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#landscaper",
    "href": "packages.html#landscaper",
    "title": "20¬† Our Packages",
    "section": "20.5 LandscapeR",
    "text": "20.5 LandscapeR\n\nLandscapeR is our package for exploring text data which has been transformed into a navigable landscape. The package makes use of cutting-edge language models and their dense word embeddings, dimensionality reduction techniques, clustering and/or topic modelling as well as Shiny for an interactive data-exploration & cleaning UI.\nIf the conversation has been mapped appropriately, you will find that mentions close together in the Shiny application/UMAP plot have similar meanings, posts far apart have less similar meanings. This makes it possible to understand and explore thousands, hundreds of thousands, or even millions of posts at a level which was previously impossible.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#limpiar",
    "href": "packages.html#limpiar",
    "title": "20¬† Our Packages",
    "section": "20.6 LimpiaR",
    "text": "20.6 LimpiaR\n\nLimpiaR is an R library of functions for cleaning & pre-processing text data. The name comes from ‚Äòlimpiar‚Äô the Spanish verb‚Äôto clean‚Äô. Generally when calling a LimpiaR function, you can think of it as ‚Äòclean‚Ä¶‚Äô.\nLimpiaR is primarily used for cleaning unstructured text data, such as that which comes from social media or reviews. In its initial release, it is focused around the Spanish language, however, some of its functions are language-ambivalent.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#displayr",
    "href": "packages.html#displayr",
    "title": "20¬† Our Packages",
    "section": "20.7 DisplayR",
    "text": "20.7 DisplayR\nDisplayR is our package for data visualization, offering a wide array of functions tailored to meet various data visualization needs. This versatile package aims to improve data presentation and communication by providing visually engaging and informative graphics.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#helpr",
    "href": "packages.html#helpr",
    "title": "20¬† Our Packages",
    "section": "20.8 HelpR",
    "text": "20.8 HelpR\n\nHelpR is SAMY‚Äôs R package for miscellaneous functions that can come in handy across a variety of workflows.\nAs you progress through your data science journey, you may take an interest in developing your own package. Depending on your previous experience developing software, this might be daunting, but don‚Äôt worry none of us had much experience building packages when we joined; we all learned on the job - and so can you. If you want to.\nSee the Package Development document for more information.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "package_development.html",
    "href": "package_development.html",
    "title": "21¬† Package Development",
    "section": "",
    "text": "22 R\nHere we‚Äôll look at how to get off the ground in R using the R package stack - {usethis}, {pkgdown}, {devtools}, {testthat} and {roxygen2}.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Package Development</span>"
    ]
  },
  {
    "objectID": "package_development.html#building-your-own-package",
    "href": "package_development.html#building-your-own-package",
    "title": "21¬† Package Development",
    "section": "22.1 Building your own package",
    "text": "22.1 Building your own package\nThe (nearly) DIY way:\nThis check-list should get you most of the way there, but it‚Äôs always possible that we‚Äôve forgotten something or there has been a disturbance in the force a change in the package ecosystem. When this happens, open up an issue or submit a PR and help the next person who comes along.\n\nCreate a new repository on GitHub\nClone the repository\n\n\n\n\n\n\n\nFolder management\n\n\n\n\n\nCreate a folder at your home directory named ‚Äògit_repos‚Äô and store all of your repositories here:\nmkdir ~/git_repos\ncd ~/git_repos\ngit clone 'repo.url'\n\n\n\n\n\nOpen RStudio and call usethis::create_package()\nusethis::use_git() in the console and commit files\nCheck git has been set up with usethis::git_sitrep() (or git status in the terminal)\nSet an upstream branch e.g.¬†git remote add upstream &lt;link_to_repo_main&gt; in the terminal\nusethis::use_vignette() to add a vignettes folder and start your first vignette\nAdd individual functions/scripts with use_r(\"script_name\") - these appear in your R/ folder\nDocument each function following roxygen2‚Äôs structure Roxygen2 guidelines\nCall use_package() whenever you use an external package inside your package.\n\n\n\n\n\n\n\nDESCRIPTION\n\n\n\n\n\nYour package will now have a DESCRIPTION file, add external packages your package requires to Imports. Add additional packages used in vignettes to Suggests. - But be careful, it‚Äôs generally not advisable to use packages just for vignettes!\nYou can use the usethis::use_latest_dependencies() to add recent versions to your packages, but beware this can be restrictive. Ideally you would add the minimum package version necessary to run your code.\n\n\n\n\nusethis::use_*_license() - default to usethis::use_mit_license()\nusethis::use_testthat() and use_test(\"script_name\") to start writing units tests for your functions and add testthat to suggests.\nCall usethis::use_readme_rmd() to allow for markdown code chunks in your readme - just remember to devtools::build_readme() when you‚Äôre done.\nCall usethis::use_news_md()\nWhen you‚Äôre ready to add a website, call usethis::use_pkgdown() pkgdown::init_site(), pkgdown::build_home_index(), pkgdown::build_search(), pkgdown::build_reference(), pgkdown::build_articles(), and then pkgdown::build_site()\nAdd each function to pkgdown.yml‚Äôs reference section (we recommend viewing a working yml file from one of the other packages to get you started).\n\nThe Easy Way (Tidy functions) Read through the usethis-setup guide and then use the usethis::create_tidy_package() to create a package with some guardrails.\n\n\n\n\n\n\nGuardrails or no guardrails?\n\n\n\nThe usethis::create_tidy_package() function is a helpful abstraction, but it will be better for your long-term development if you know how to do this stuff without the abstraction. That way, when you need to fix something, or do something slightly different than the prescribed way, you‚Äôll have a better chance of success.\n\n\nWant to go deeper? Check out the R Packages Book, we recommend skimming first and then using it as a reference manual.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Package Development</span>"
    ]
  },
  {
    "objectID": "package_development.html#development-workflow",
    "href": "package_development.html#development-workflow",
    "title": "21¬† Package Development",
    "section": "22.2 Development workflow",
    "text": "22.2 Development workflow\n\n\nOnce you‚Äôve built the package there are some things you will want to do regularly to ensure your package stays in good shape. This is by no means an exhaustive list - be sure to add your tips & tricks as you amass them.\n\n\n\n\n\n\n\n\nRun testthat::test_package() often to check for regressions in your code\nRun devtools::check() occasionally to make sure you haven‚Äôt made any obvious mistakes - try to keep notes, warnings and errors to 0!\nUse devtools::load_all() to reload the package when you‚Äôve made changes. (devtools::document() also calls load_all() when called)\nRun roxygen2::roxygenise(clean = TRUE) if your documentation doesn‚Äôt look as you expect after\nUse pkgdown::build_site() when you expect to see changes in your package‚Äôs website\nUse pkgdown::clean_site() and pkgdown::build_site() when expected changes aren‚Äôt reflecting in your preview",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Package Development</span>"
    ]
  },
  {
    "objectID": "package_development.html#contributing-to-existing-packages",
    "href": "package_development.html#contributing-to-existing-packages",
    "title": "21¬† Package Development",
    "section": "22.3 Contributing to existing packages",
    "text": "22.3 Contributing to existing packages\n\nPull current state of repo/package from origin\nCreate a new branch, can use usethis::pr_init() from usethis to make this a bit easier, otherwise git checkout -b \"branch_name\"\nRun devtools::test() devtools::check() at regular intervals, keep errors, warnings, notes down to minimum\nBuild out logic for new changes, add to R/ where necessary. usethis::use_r() function to add new scripts properly\nBuild out tests for new logic in tests/\nEnsure function-level documentation is added to any new logic, including @title, @description, @details, @param, @returns, @examples and @export if function is to be exported, or @internal otherwise. Let roxygen2 take care of @usage.\nKeep re-running tests and check!\nIf you‚Äôre introducing something new, update package-level documentation e.g.¬†vignettes and/or readme explaining what you‚Äôve introduced and how it should be used. Provide examples where possible. If you‚Äôre building out a new capability you may need a whole new vignette, use the usethis::use_vignette() function.\n\n\n\n\n\n\n\nWhat are vignettes?\n\n\n\nVignettes are long-form guides that provide in-depth documentation for your package. They go beyond the basic function documentation and explain how to use the package to solve specific problems, often with detailed examples and code. Vignettes showcase your package‚Äôs full range of capabilities and help users understand how to effectively utilise its features\n\n\n\nIf you‚Äôre updating legacy code, check that vignettes are up-to-date with the changes you‚Äôve made - we want to avoid code-documentation drift where possible.\nAdd your function to the reference section in _pkgdown,yml if it‚Äôs being exported.\nAdd data objects, .Rhistory, *.Rproj, .Rprofile, .DS_Store, to .gitignore\nRun pkgdown::clean_site() and pkgdown::build site(), visually inspect each section of the site\n\nPull request when ready.\n\nCode\nGenerally code should sit in the R/ folder, you can choose between a script per function or use scripts as modules, where a module is a particular use case, or logical unit. Historically we sided on the former, but as a package grows it can become difficult to manage/navigate, and there can be a decoupling of logic. Ultimately this is a matter of taste in R.\n\nExercises - code\n\n\n\n\n\n\nExercises\n\n\n\n\n\nYou may need to consult external resources to answer the exercises, we‚Äôve tried to provide links to help you along the way, but we encourage you to embrace the joy of discovery and find relevant sources/fill in the gaps where necessary!\n\n\n\n\nWhat are the practical differences between .gitignore and .Rbuildignore?\n\n\nWhat objects should go in .gitignore but not .Rbuildignore, and vice versa?\n\n\nWhat does the DESCRIPTION file do?\nWrite your own description for each of the following packages, detailing what they are for where they sit in the R package stack:\n\n\ntestthat\nroxygen2\ndevtools\npkgdown\nusethis\n\n\n\n\nTests\n\nIn a perfect world, every dog implementation detail would have a home test and every home test would have an implementation detail.\n\nThere is a balance to be struck between testing absolutely everything and testing what needs to be tested. Before we get into the finer details, let‚Äôs establish why we‚Äôre writing tests in the first place. The first reason for writing tests is to help you write software that works. The second reason is to help you do this fast, and with confidence.\nTesting is not to prove that your code has no bugs, or cannot have any bugs in the future. Whenever you do find a bug, or someone reports one, write a test as you fix the issue.\nFor more information and another opinion, check out the R Packages testing section, and the Testing document\n\n\n\n\n\n\nTip\n\n\n\nDon‚Äôt let testing paralyse your development process, they‚Äôre there to help not hinder. As a rule-of-thumb, if your tests for a function are more complex than your function, you‚Äôve gone too far.\n\n\n\n\nDocumentation\n\n\nWe use {roxygen2} tags to document our functions. Visit the documenting functions article for a primer.\nUsing the roxygen2 skeleton promotes consistent documentation, check out a function‚Äôs help page (e.g.¬†?ParseR::count_ngram) to see how rendered documentation looks - do this regularly with your own functions.\nWe tend to find our documentation could always be better, more complete. You can‚Äôt hope to cover everything a user could do with your function, but make sure it‚Äôs clear from the documentation what your function is for and what its primary uses are.\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMost people will scroll straight past the @description and @details and go directly to your code examples.\n\n\n\nGuidelines for commonly-used Roxygen tags\n\n\n\n\n\n\n\nTag\nDescription\n\n\n\n\n@title\nOne-line description of what your function does\n\n\n@description\nA paragraph elaborating on your title\n\n\n@details\nA more detailed description of the function e.g.¬†explaining how its arguments interact, or other key implementation details.\n\n\n@param\nA description of the function‚Äôs parameters\n\n\n@return\nA description of the function‚Äôs return value\n\n\n@examples\nExamples of how to use the function\n\n\n@export\nWhether the function is exported or not\n\n\n\n\n\nExercises - Documentation\n\nWhat is the title of {dplyr}‚Äôs mutate() function?\n@examples must be self-contained, create an example that is not self-contained, and one that is.\nWhich package(s) (any programming language) stick out in your mind as being well-documented and easy to use, what did the creators do well?\nAudit SAMY‚Äôs R packages, find a function with sub-par documentation and upgrade it. Then fire in a Pull Request!\n\n\n\n\nData\nYou‚Äôre probably going to need some package-level data for your @examples or your vignettes. Before going off and finding or creating a new data set:\n\nCheck whether you can demonstrate what you need with existing datasets - call data() in your console\nMake sure the dataset you have chosen comes from a package your package explicitly Imports or Suggests\n\nIf you still can‚Äôt find the right dataset, create one!\n\nLoad the dataset into memory\nCall usethis::use_data(dataset_variable_name)\nDocument the columns\n\nIf you choose this route, some interesting problems may lie in wait. Skip to Exercises 1.\nTo go deeper view the R Packages Dataset Section\n\nDatasets from the {datasets} package come with base R\n\n\nExercises - Data\n\nWhy might you add your data artefacts to .Rbuildignore or .gitignore?\nWhich package does the diamonds dataset ship with?\n\nYou‚Äôre probably going to need some data‚Ä¶ existing data‚Ä¶ adding new usethis::use_data() usethis::use_data_raw()\n\n\n\nWebsite\npkgdown .nojekyll\n\nExercises - Website\n\nExplain in your own words what .nojekyll is for.\n\n\nWhere should it be placed in your package?\nWhat problems arise when you don‚Äôt have one?",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Package Development</span>"
    ]
  },
  {
    "objectID": "package_development.html#folder-setup",
    "href": "package_development.html#folder-setup",
    "title": "21¬† Package Development",
    "section": "23.1 Folder Setup",
    "text": "23.1 Folder Setup\nThe first step in creating a Python package is setting up the project structure. Create a new directory for your project and organize it with the following subdirectories and files:\n\nyour_package_name/: The main package directory containing your Python modules and code. Make sure to add a __init__.py file to the source and any modules.\n\n\n\n\n\n\n\ninit.py\n\n\n\n\n\nCreate a directory for your package and place an empty init.py file inside it. If your package has sub-packages or modules, create additional directories for them and place init.py files in each directory. Import your package or its modules in your Python scripts using the import statement or the from package import module syntax.\n\n\n\n\ntests/: Directory for test files to ensure the correctness of your package‚Äôs functionality.\ndocs/: Directory for storing comprehensive documentation files.\nsetup.py: File specifying package metadata, dependencies, and build instructions.\nMANIFEST.in: File listing the files to include in the package distribution.\nrequirements.txt: File listing the package dependencies for easy installation.\nREADME.md: File providing an overview, installation instructions, and usage examples for your package.\nLICENSE: File specifying the license under which your package is distributed.\n.gitignore: File specifying files and directories to ignore in version control.\nInitialize a Git repository in your project directory and create a new repository on GitHub for collaboration and issue tracking.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Package Development</span>"
    ]
  },
  {
    "objectID": "package_development.html#git---terminal",
    "href": "package_development.html#git---terminal",
    "title": "21¬† Package Development",
    "section": "23.2 Git - Terminal",
    "text": "23.2 Git - Terminal\nTo set up a Git repository for your Python package from the terminal, follow these steps:\n\nOpen your terminal and navigate to the root directory of your Python package using the cd command. For example: cd /path/to/your/package\nInitialize a new Git repository by running the git init command: git init\nAdd all the files in your package directory to the Git staging area using the git add command: git add .\nCreate an initial commit to save the current state of your package by running the git commit command with a meaningful commit message: git commit -m \"Initial commit\"\nAdd the remote repository URL to your local Git repository using the git remote add command: git remote add origin &lt;repo_url&gt;\nPush your local commits to the remote repository using the git push command: git push -u origin main",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Package Development</span>"
    ]
  },
  {
    "objectID": "package_development.html#vignettes",
    "href": "package_development.html#vignettes",
    "title": "21¬† Package Development",
    "section": "23.3 Vignettes",
    "text": "23.3 Vignettes\n\nWrite the vignette content in a readable format such as Markdown (.md) or reStructuredText (.rst).\nPlace the vignette file in a dedicated directory within your package, typically named vignettes/ or docs/vignettes/\nUse a documentation tool like Quarto, Sphinx or MkDocs to convert the vignette file into HTML or PDF format. We advise using Quarto to keep it simple.\nConfigure your package‚Äôs setup.py file to include the vignette files in the package distribution. Add the vignette directory to the package_data argument of the setup() function.\nIf using Sphinx or mkdocs: Generate the package documentation by running the documentation tool‚Äôs build command, such as sphinx-build or mkdocs build. This will create the HTML or PDF files for your vignettes.\nPublish the generated vignette files along with your package distribution. Include them in the source distribution (sdist) and wheel distribution (bdist_wheel) that you upload to PyPI or conda.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Package Development</span>"
    ]
  },
  {
    "objectID": "package_development.html#tests-1",
    "href": "package_development.html#tests-1",
    "title": "21¬† Package Development",
    "section": "23.4 Tests",
    "text": "23.4 Tests\nThere are multiple viable frameworks, but for simplicity we recommend Pytest which functions quite similarly to {testthat}.\nPytest has great docs, work through the how-to guides to get up to speed.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Package Development</span>"
    ]
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "22¬† Testing in R",
    "section": "",
    "text": "22.1 Testable Functions\nis_even &lt;- function(number) {\n  \n  stopifnot(is.numeric(number), number != 0)\n  \n  return(number %% 2 == 0)\n}\n\n\nis_odd &lt;- function(number) {\n  stopifnot(is.numeric(number), number != 0)\n  \n  return(!is_even(number))\n}\nOur two functions should take a number as an input, and check that the number isn‚Äôt zero. is_even should return TRUE if the number is divisible by 2 and FALSE otherwise. is_odd calls is_even and then flips the truth value, so that if a number isn‚Äôt even, and it‚Äôs not 0, it‚Äôs odd.\nIt clearly makes sense to test is_even first, because is_odd depends on it. So what should we test? Echoing Einstein‚Äôs famous words on simplicity, tests should test everything the function does, nothing more and nothing less. Obviously we should check that our input validation is working, if we input 0 do we get an error, the same if we input a non-numeric. Then we should check a few return values, some that the function should return FALSE to and some that the function should return TRUE to. And then there‚Äôs a slightly less obvious test - our function has one argument and that argument is mandatory, i.e.¬†it has no default value; if we‚Äôve made this decision we should have made it for a reason, so we should test the function does error if no value is set.\nReminder that unit tests should:\nWriting tests may at times feel cumbersome, but only at the beginning. Once you‚Äôve got a good test suite up development becomes more enjoyable - less anxiety associated with each change you make or feature you implement - and faster (trust!). You should often feel like you‚Äôre insulting your own intelligence and that of your colleagues‚Äô by writing such a simple test ‚ÄúWell duh, of course it does that‚Ä¶‚Äù",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Testing in R</span>"
    ]
  },
  {
    "objectID": "testing.html#testable-functions",
    "href": "testing.html#testable-functions",
    "title": "22¬† Testing in R",
    "section": "",
    "text": "be simple: tests are not the place to show off what you can do, you should be able to understand at a glance what‚Äôs being tested and how the test works i.e.¬†favour writing each test out rather than wrapping a bunch in a vapply() or a map().\nbe lightweight: you‚Äôre usually going to want to write hundreds of them for a package and check them often. Each test should run in fractions of a second, a heavy test suite won‚Äôt be used and so becomes self-defeating\nbe self-contained: don‚Äôt pass data around between tests, each test_that block should be able to start and terminate in isolation\nbe informative: they should provide helpful error messages when they fail, so that you know precisely which part of your code‚Äôs logic is broken and where\nbe comprehensive: if your code should do something, write a test to show it does\nlook both ways: if your code shouldn‚Äôt do something, write a test to show it doesn‚Äôt",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Testing in R</span>"
    ]
  },
  {
    "objectID": "testing.html#our-first-tests",
    "href": "testing.html#our-first-tests",
    "title": "22¬† Testing in R",
    "section": "22.2 Our First Tests",
    "text": "22.2 Our First Tests\nThere are a number of other, more specific tests for more advanced users, but let‚Äôs stick to expect_error, expect_true, and expect_false for now.\n\nlibrary(testthat)\ntest_that(\"is_even has an argument called number and it requires an input\", {\n  \n  expect_true(names(formals(is_even)) == \"number\")\n  expect_error(is_even(),\n               regexp = 'argument \"number\" is missing')\n})\n\nWe‚Äôre going to make a change to is_even, to show that these tests can fail if the underlying logic of is_even changes resulting in changes in the function‚Äôs behaviour (this isn‚Äôt necessary except for explanatory purposes).\n\nis_even_inputs &lt;- function() {\n  test_that(\"is_even has an argument called number and it requires an input\", {\n    \n    expect_true(\n      names(formals(is_even)) == \"number\")\n    \n    expect_error(\n      is_even(),\n      regexp = 'argument \"number\" is missing'\n    )\n  })\n}\n\n\nis_even_inputs()\n\nOk, so the test passes. But what if I want to change the input is_even takes to ‚Äòx‚Äô which is a more common input?\n\nis_even &lt;- is_even &lt;- function(x) {\n  \n  stopifnot(is.numeric(x), x != 0)\n  \n  return(x %% 2 == 0)\n}\n\nis_even_inputs()\n\nWe see that we get a test failure: ‚Äì Failure: is_even has an argument called number and it requires an input ‚Äî‚Äì names(formals(is_even)) == ‚Äúnumber‚Äù is not TRUE\nThis is exactly what we wanted. We wrote a function, wrote some tests, changed the function‚Äôs behaviour and then running our tests told us that we‚Äôd altered the function‚Äôs behaviour. At this point we should either fix our function - if indeed we broke it - or update our tests. We‚Äôll fix the function as the tests are still doing what we want them to. Then we‚Äôll check our old tests still pass.\n\nis_even &lt;- function(number) {\n  \n  stopifnot(is.numeric(number), number != 0)\n  \n  return(number %% 2 == 0)\n}\n\nis_even_inputs()\n\nOk, so let‚Äôs carry on with testing the function. We‚Äôll establish that our function doesn‚Äôt take 0 as an input, and that if we feed it a string, or a string that could be coerced into a numeric that the function errors. This last one might seem like a funny test, but we haven‚Äôt explicitly asked our function to coerce its inputs, so we should check that it does not.\n\ntest_that(\"is_even errors if given a non-numeric input, or 0 as an input\", {\n  expect_error(is_even(0),\n               regexp = stringr::fixed('number != 0'))\n  \n  expect_error(\n    is_even(\"string\"),\n    regexp = \"is\\\\.numeric\"\n  )\n  \n  expect_error(\n    is_even(\"10\"),\n    regexp = \"is\\\\.numeric\"\n  )\n})\n\nAnd then finally we can test that the return values are what we expect:\n\ntest_that(\"is_even returns a logical, and that logical is TRUE if given an even input, and FALSE if given an odd.\", {\n  expect_true(\n    inherits(is_even(10), \"logical\")\n  )\n  \n  expect_true(\n    inherits(is_even(9), \"logical\")\n  )\n  \n  expect_true(\n    is_even(10) == TRUE\n  )\n  expect_false(\n    is_even(9) == TRUE\n    )\n  \n  #and another value, just to be sure...\n  expect_true(\n    is_even(10002) == TRUE\n  )\n})",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Testing in R</span>"
    ]
  },
  {
    "objectID": "testing.html#refactoring-is_odd",
    "href": "testing.html#refactoring-is_odd",
    "title": "22¬† Testing in R",
    "section": "22.3 Refactoring is_odd",
    "text": "22.3 Refactoring is_odd\nWe‚Äôve now tested that our is_even function does what it should, and doesn‚Äôt do what it shouldn‚Äôt. We could add more tests, like what happens if we input a data frame as number, or a factor? Or if 8938957 and 23665 are odds, but we feel quite confident that our current cases take care of those.\nWe haven‚Äôt tested is_odd yet, but let‚Äôs take another look at our function definitions and see if we can‚Äôt simplify the logic somehwat.\n\nis_even &lt;- function(number) {\n  \n  stopifnot(is.numeric(number), number != 0)\n  \n  return(number %% 2 == 0)\n}\n\nis_odd &lt;- function(number) {\n  stopifnot(is.numeric(number), number != 0)\n  \n  return(!is_even(number))\n}\n\nWe‚Äôve written a pretty lightweight and comprehensive test suite for is_even, so do we just go ahead and write the same tests for is_odd? We don‚Äôt really need to, because is_odd calls is_even anyway. So let‚Äôs simplify is_odd:\n\nis_odd &lt;- function(number) {\n  return(!is_even(number))\n}\n\nInformally test a few values:\n\nis_odd(\"string\")\nis_odd(0)\n\nSo we can see that is_odd is producing the errors we would expect it to, because the logic is cemented in is_even. Our tests for is_odd don‚Äôt really need to duplicate this logic, so we could test one each odd-signalling end digit, and each even-signalling end digit.\n\ntest_that(\"is_odd returns TRUE for odd numbers and FALSE for even numbers\", {\n    \n  expect_true(is_odd(11))\n  expect_true(is_odd(333))\n  expect_true(is_odd(555))\n  expect_true(is_odd(37))\n  expect_true(is_odd(49))\n  \n  \n  expect_false(is_odd(10))\n  expect_false(is_odd(4))\n  expect_false(is_odd(638))\n  expect_false(is_odd(132))\n  expect_false(is_odd(666))\n})\n\nIt‚Äôs overkill to do this, but there‚Äôs an important point to be made. You might look at this and think ‚Äòshouldn‚Äôt I just apply a list of numbers, rather than write each test out, to avoid duplication?‚Äô\n\ntest_that(\"is_odd returns TRUE for odd numbers and FALSE for even numbers\", {\n  odds &lt;- list(11, 333, 555, 37, 49)\n  lapply(odds, function(odd) {\n    expect_true(is_odd(odd))\n  })\n  \n  evens &lt;- list(10, 4, 638, 132, 666)\n  lapply(evens, function(even){\n    expect_false(is_odd(even))\n  })\n})",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Testing in R</span>"
    ]
  },
  {
    "objectID": "testing.html#keep-it-simple-stupid",
    "href": "testing.html#keep-it-simple-stupid",
    "title": "22¬† Testing in R",
    "section": "22.4 Keep it Simple, Stupid",
    "text": "22.4 Keep it Simple, Stupid\nWhilst this is generally good practice, it‚Äôs not ideal in the case of testing because when a test fails, our error messages are less informative. For brevity we‚Äôll add an odd value to our evens list, and apply that list over our tests:\n\ntest_that(\"is_odd returns FALSE when given even inputs\",{\n  evens &lt;- list(10, 4, 638, 132, 666, 17)\n  lapply(evens, function(even){\n    expect_false(is_odd(even))\n  })\n})\n\nWe see that the error message we get back doesn‚Äôt tell us which of our inputs failed, just that we expected a FALSE and we got a TRUE, somewhere. In this case it‚Äôs pretty obvious, but there are times when testing things like shiny UI components where it‚Äôs tempting to put all the UI tags into a list of tags and l/vapply them into an expect function to keep the testing code concise and avoid duplication. However, we want our tests to be informative more than we want them to adhere to Do Not Repeat Yourself principles.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Testing in R</span>"
    ]
  },
  {
    "objectID": "testing.html#whats-the-problem",
    "href": "testing.html#whats-the-problem",
    "title": "22¬† Testing in R",
    "section": "23.1 What‚Äôs the problem?",
    "text": "23.1 What‚Äôs the problem?\nIdeally we want to test the three main components - the UI, the server, and the call to combine the two. The first obstacle is the UI, it‚Äôs not a function like we‚Äôre used to testing. And we don‚Äôt need to write a test to check that it‚Äôs still not a function going forwards, we let the Shiny developers write their own tests.\n\ninherits(ui, \"function\")\n\nI think it‚Äôs quite common to build Shiny apps without really knowing what a shiny.tag.list is, and that‚Äôs what a UI is.\n\nS3Class(ui)\n\nAnd then there‚Äôs the slightly puzzling unnamed list which has 4 elements\n\nlength(ui); names(ui)\n\nWhat are the elements, and what kinds of test can we run on them?\n\nui[[1]]\nui[[2]]\nui[2]\nui[3]\nclass(ui[4][[1]][[1]])\n\nWe‚Äôll come back to this shortly.\nThe next problem is the server object - which is a function - but a slightly esoteric one.\n\nS3Class(server)\n\nIt takes three mandatory arguments - input, output, and session. The input argument is quite transparent - we use it to access inputs all the time when building Shiny apps, and similar for outputs with the output$ object. As we can index them with $ we think they‚Äôre probably named lists of some description. But session is a little more opaque.\n\nformals(server)\n\nSo we know that if we want to test the server function we‚Äôll need to add input, output, and session but we don‚Äôt really know what we should add there. Like the UI, we‚Äôll come back to this shortly.\nSo the shinyApp function, which has a more familiar look about it. It takes our ui and server as inputs, and builds the Shiny app for us. Source code:\n\nfunction (ui, server, onStart = NULL, options = list(), uiPattern = \"/\", \n    enableBookmarking = NULL) \n{\n    if (!is.function(server)) {\n        stop(\"`server` must be a function\", call. = FALSE)\n    }\n    uiPattern &lt;- sprintf(\"^%s$\", uiPattern)\n    httpHandler &lt;- uiHttpHandler(ui, uiPattern)\n    serverFuncSource &lt;- function() {\n        server\n    }\n    if (!is.null(enableBookmarking)) {\n        bookmarkStore &lt;- match.arg(enableBookmarking, c(\"url\", \n            \"server\", \"disable\"))\n        enableBookmarking(bookmarkStore)\n    }\n    appOptions &lt;- captureAppOptions()\n    structure(list(httpHandler = httpHandler, serverFuncSource = serverFuncSource, \n        onStart = onStart, options = options, appOptions = appOptions), \n        class = \"shiny.appobj\")\n}\n\nThere‚Äôs some input validation,\ninputs is a list of reactiveValues, output is a list of some values too. Session is‚Ä¶ a bit different. And how do we access it programmatically?\nLater - refactor to have min &gt; 0, as why would you want 0 breaks and to allow the erro?",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Testing in R</span>"
    ]
  },
  {
    "objectID": "testing.html#mastering-shiny---testing-chapter-21-presently",
    "href": "testing.html#mastering-shiny---testing-chapter-21-presently",
    "title": "22¬† Testing in R",
    "section": "25.1 Mastering Shiny - Testing (Chapter 21 presently)",
    "text": "25.1 Mastering Shiny - Testing (Chapter 21 presently)\nYou can use browser() inside testServer to see what‚Äôs going on with specific values/what your changes do and what will / won‚Äôt work‚Ä¶\nstopifnot(is.reactive(var)) - nice little trick for input validation in modules, e.g.¬†for highlighted_dataframe()\ntestServer - Unlike the real world, time does not advance automatically. So if you want to test code that relies on reactiveTimer() or invalidateLater(), you‚Äôll need to manually advance time by calling session$elapse(millis = 300).\ntestServer() ignores UI. That means inputs don‚Äôt get default values, and no JavaScript works. Most importantly this means that you can‚Äôt test the update* functions, because they work by sending JavaScript to the browser to simulates user interactions. You‚Äôll require the next technique to test such code.\nWrap testServer in test_that",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Testing in R</span>"
    ]
  },
  {
    "objectID": "testing.html#shiny-app-packages---testing-section-3",
    "href": "testing.html#shiny-app-packages---testing-section-3",
    "title": "22¬† Testing in R",
    "section": "25.2 Shiny App Packages - Testing (Section 3)",
    "text": "25.2 Shiny App Packages - Testing (Section 3)\nTesting the UI\n\nmod_bigram_network_ui(id = \"test\")",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Testing in R</span>"
    ]
  },
  {
    "objectID": "testing.html#gotchas-reminders",
    "href": "testing.html#gotchas-reminders",
    "title": "22¬† Testing in R",
    "section": "25.3 Gotchas + Reminders",
    "text": "25.3 Gotchas + Reminders\nBrowse[1]&gt; class(bigram_reactive) [1] ‚Äúreactive.event‚Äù ‚ÄúreactiveExpr‚Äù ‚Äúreactive‚Äù ‚Äúfunction‚Äù\nBrowse[1]&gt; x &lt;- bigram_reactive() Browse[1]&gt; class(x) [1] ‚Äúggraph‚Äù ‚Äúgg‚Äù ‚Äúggplot‚Äù\nbigram_reactive is the reactive expression, untriggered. bigram_reactive() is the actual ggraph/gg/ggplot object now triggered. Always good to remind onself of this and what that means when interacting with the objects at various points of the SWD process.\nCan use ui &lt;- mod_bigram_network_ui(id = \"test\") and type ui to see all of the shiny tags, and then type ui[[1]] to render the UI in a viewer object, maybe easier than end app, run_app() -&gt; click to app.",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Testing in R</span>"
    ]
  },
  {
    "objectID": "testing.html#testing-interaction-with-nested-modules",
    "href": "testing.html#testing-interaction-with-nested-modules",
    "title": "22¬† Testing in R",
    "section": "25.4 Testing interaction with nested modules",
    "text": "25.4 Testing interaction with nested modules\nthe mod_group_vol_time_server gets its filtered data out of the mod_daterange_input_server. This presents a challenge because each module is namespaced, so we can‚Äôt just setInputs(dateRange = list(as.Date(\"2023-01-03\"), as.Date(\"2023-01-09\")) because we‚Äôd be setting the value of dateRange inside the wrong namespace - the namespace of mod_group_vol_time_server.\nThis means when we try to access the group_date_range_vot$over_time_data() to generate our groupued volume over time char, we get an error that the dateRange isn‚Äôt set. So with the help of ui &lt;- mod_group_vol_time_ui(id = \"test\") we can look for the correct input to set dateRange to, which is dateRangeGroupVol-dateRange. We‚Äôve unearthed a general truth about nested modules. Our parent module is dateRangeGroupVol, our child is dateRange, so we join the two with dateRangeGroupVol-dateRange, if dateRange had a child module called dateRangeChild, we‚Äôd join the three with dateRangeGroupVol-dateRange-dateRangeChild!\n\nsession$setInputs(\n  # ns(\"dateRange\") = list(as.Date(\"2023-01-03\"), as.Date(\"2023-01-09\")),\n  dateBreak = \"day\",\n  height = 600,\n  width = 400,\n  nrow = 2,\n  #So to pass stuff into the modules that need them we can pre-prepend the namespace with syms\n  `dateRangeGroupVol-dateRange` = list(as.Date(\"2023-01-03\"), as.Date(\"2023-01-09\"))\n)",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Testing in R</span>"
    ]
  },
  {
    "objectID": "testing.html#emulating-a-reactive-with-the-return-values-specified",
    "href": "testing.html#emulating-a-reactive-with-the-return-values-specified",
    "title": "22¬† Testing in R",
    "section": "25.5 Emulating a reactive with the return values specified:",
    "text": "25.5 Emulating a reactive with the return values specified:\nbecause in our module we try to: label_ids &lt;- as.numeric(selected_range()$key)\nWe can‚Äôt just send in selected_range as c(1, 2, 3). We need to send it in as an object which imitates a reactive, with a return value of list(key = c(‚Ä¶)) so that when we call it in the module later on, we get the current value, and we can access the key. Tricky.\nselected_range = function(){ return(list(key = c(1, 2, 3))) }\nIf wanting to change the length of the generate_dummy_data for whatever reason: function(){return(generate_dummy_data(length = 20))}",
    "crumbs": [
      "Development",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Testing in R</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html",
    "href": "code_best_practices.html",
    "title": "23¬† Coding best practices",
    "section": "",
    "text": "23.1 Why are we here?\nAt SAMY, code is the language of both our research and our development, so it pays to invest in your coding abilities. There are many great (and many terrible) resources on learning how to code. This document will focus on practical tips on how to structure your code to reduce cognitive strain and do the best work you can.\nLet‚Äôs be clear about what coding is: coding is thinking not typing, so good coding is simply good thinking and arranging our code well will help us to think better.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#reproducible-analyses",
    "href": "code_best_practices.html#reproducible-analyses",
    "title": "23¬† Coding best practices",
    "section": "23.2 Reproducible Analyses",
    "text": "23.2 Reproducible Analyses\nAbove everything else, notebooks must be reproducible. What do we mean by reproducible? You and your collaborators should be able to get back to any place in your analysis simply by executing code in the order it occurs in your scripts and notebooks. Hopefully the truth of this statement is self-evident. But if that‚Äôs the case, why are we talking about it?\nFor some projects you‚Äôll get away with a folder structure which looks something like this:\n\n\n./example_folder\n‚îú‚îÄ‚îÄ code\n‚îÇ   ‚îî‚îÄ‚îÄ analysis.Rmd\n‚îî‚îÄ‚îÄ data\n    ‚îú‚îÄ‚îÄ clean_data.csv\n    ‚îî‚îÄ‚îÄ raw_data.csv\n\n\nHowever, in weeks-long or even months-long research projects, if you‚Äôre not careful your project will quickly spiral out of control (see the lovely surprise below for a tame example), your R Environment will begin to store many variables, and you‚Äôll begin to pass data objects around between scripts and markdowns in an unstructured way, e.g.¬†you‚Äôll reference a variable created inside ‚Äòwrangling.Rmd‚Äô inside the ‚Äòcolab_cleaning.Rmd‚Äô, such that colab_cleaning.Rmd becomes unreproducible.\n\n\nA lovely surprise\n\n\n\n./example_folder_complex\n‚îú‚îÄ‚îÄ code\n‚îÇ   ‚îú‚îÄ‚îÄ colab_cleaning.Rmd\n‚îÇ   ‚îú‚îÄ‚îÄ edited_functions.R\n‚îÇ   ‚îú‚îÄ‚îÄ images\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ outline_image.png\n‚îÇ   ‚îú‚îÄ‚îÄ initial_analysis.Rmd\n‚îÇ   ‚îú‚îÄ‚îÄ quick_functions.R\n‚îÇ   ‚îú‚îÄ‚îÄ topic_modelling.Rmd\n‚îÇ   ‚îî‚îÄ‚îÄ wrangling.Rmd\n‚îî‚îÄ‚îÄ data\n    ‚îú‚îÄ‚îÄ clean\n    ‚îÇ   ‚îú‚îÄ‚îÄ all_data_clean.csv\n    ‚îÇ   ‚îú‚îÄ‚îÄ all_data_clean_two.csv\n    ‚îÇ   ‚îú‚îÄ‚îÄ all_data_cleaner.csv\n    ‚îÇ   ‚îú‚îÄ‚îÄ data_topics_clean.csv\n    ‚îÇ   ‚îî‚îÄ‚îÄ data_topics_newest.csv\n    ‚îî‚îÄ‚îÄ raw\n        ‚îú‚îÄ‚îÄ sprinklr_export_1.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_10.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_11.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_12.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_13.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_14.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_15.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_16.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_17.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_18.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_19.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_2.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_20.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_21.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_22.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_23.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_24.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_25.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_26.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_27.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_28.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_29.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_3.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_30.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_4.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_5.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_6.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_7.xlsx\n        ‚îú‚îÄ‚îÄ sprinklr_export_8.xlsx\n        ‚îî‚îÄ‚îÄ sprinklr_export_9.xlsx\n\n\n\n\nLiterate Programming\n\n‚Äúa script, notebook, or computational document that contains an explanation of the program logic in a natural language (e.g.¬†English or Mandarin), interspersed with snippets of macros and source code, which can be compiled and rerun. You can think of it as an executable paper!‚Äù\n\nNotebooks have become the de factor vehicles for Literate Programming and reproducible research. They allow you to couple your code, data, visualisations, interpretations and analysis. You can and should use the knit/render buttons regularly (found in the RStudio IDE) to keep track of whether your code is reproducible or not - follow the error messages to ensure reproducibility.\n\nHave I turned off the restore .RData setting in tools ‚Äì&gt; global options?\nHave I separated raw data and clean data?\nHave I recorded in code my data cleaning & transformation steps?\nDo my markdowns and notebooks render?\nAm I using relative or absolute filepaths within my scripts & notebooks?\n[ ]\n[ ]",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-flow-and-focus",
    "href": "code_best_practices.html#on-flow-and-focus",
    "title": "23¬† Coding best practices",
    "section": "23.3 On flow and focus",
    "text": "23.3 On flow and focus\nMost of us cannot do our best work on the most difficult challenges for 8 hours per day. In fact, conservative estimates suggest we have 2-3 hours per day, or 4 hours on a good day, where we can work at maximum productivity on challenging tasks. Knowing that about ourselves, we should proactively introduce periods of high and low intensity to our days.\nIn periods of high intensity we‚Äôll be problem solving - inspecting our data, selecting cleaning steps, running small scale experiments on our data: ‚ÄòWhat happens if I‚Ä¶‚Äô and recording and interpreting the results. When the task is at the correct difficulty, you‚Äôll naturally fall into a flow state. Try your best to prevent interruptions during this time. Protect your focus - don‚Äôt check your work emails, turn Slack off etc.\nWhilst these high-intensity periods are rewarding and hyper-productive, at the other end there is often a messy notebook or some questionable coding practices. Allocate time each and every day to revisit the code, add supporting comments, write assertions and tests, rename variables to be more descriptive, tidy up unused data artefacts, study your visualisations to understand what the data can really tell you etc. or anything else you can do to let your brain rest, recharge and come back stronger tomorrow. You‚Äôll sometimes feel like you don‚Äôt have time to do these things, but it‚Äôs quite the opposite - you don‚Äôt have time not to do them.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-managing-complexity",
    "href": "code_best_practices.html#on-managing-complexity",
    "title": "23¬† Coding best practices",
    "section": "23.4 On managing complexity",
    "text": "23.4 On managing complexity\n\n‚Äú‚Ä¶let‚Äôs think of code complexity as how difficult code is to reason about and work with.‚Äù\n\nThere are many heuristics for measuring code complexity, the most basic being ‚Äòlines of code‚Äô which is closely linked to ‚Äòvertical complexity‚Äô - the more code we have the longer our scripts and markdowns will be, the harder it is to see all of the relevant code at any one time, the more strain we put on our working memories. A naive strategy for reducing complexity is to reduce lines of code. But if we reduce the number of lines of code by introducing deeply nested function calls, the code becomes more complex not less as the number of lines decreases.\nAs a rough definition, let‚Äôs think of code complexity as ‚Äòhow difficult code is to reason about and work with.‚Äô A good test of code complexity is how long it takes future you to remember what each line, or chunk, of code is for.\nWe‚Äôll now explore some tools and heuristics for fighting complexity in our code.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-navigation",
    "href": "code_best_practices.html#on-navigation",
    "title": "23¬† Coding best practices",
    "section": "23.5 On navigation",
    "text": "23.5 On navigation\n\n\n\n\n\ngraph LR\n    A[Raw Data] --&gt; B(Cleaning)\n    B --&gt; C(Transformation)\n    C --&gt; D(Visualisation)\n    D --&gt; E(Modelling)\n    E --&gt; F(Communicating Results)\n    E --&gt; B(Cleaning)\n\n\n\n\n\n\nLet‚Äôs go out on a limb and say that the data science workflow is never linear, you will always move back and forth between cleanin data, inspecting it, and modelling it. Structuring your projects and notebooks with this in mind will save many headaches.\n\nReadme\nFor each project, add aREADME.md or README.Rmd, here you can outline what and who the project is for and guide people to notebooks, data artefacts, and any important resources. You may find it useful to maintain a to-do list here, or provide high-level findings - it‚Äôs really up to you, just keep your audience in mind.\n\n\nSection Titles\n\n\nSection titles help order your thoughts - when done well they let you see the big picture of your document. They will also help your collaborators to navigate and understand your document, and they‚Äôll function as HTML headers in your rendered documents. When in the RStudio IDE the outline tab allows click-to-navigate with your section titles.\n\n\n\n\n\n\nTip\n\n\n\nSet the toc-depth: in your quarto yaml to control how many degrees of nesting are shown in your rendered document‚Äôs table of contents.\n\n\n\n\n\n\n\n\nRstudio Outline\n\n\n\n\n\n\nCode chunks\nYou wrote the code in the chunk. So you know what it does, or at least you should. However, when rendering your document (which you should do regularly) it‚Äôs handy to have named chunks so that you know precisely which chunk is taking a long time to render, or has a problem. Furthermore, that 8-line pipe inside the chunk might not be as easy to understand at a glance in the future, and it certainly won‚Äôt be for your collaborators. It‚Äôs much easier to understand what a descriptively named chunk is doing than 8 piped function calls.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-comments",
    "href": "code_best_practices.html#on-comments",
    "title": "23¬† Coding best practices",
    "section": "23.6 On comments",
    "text": "23.6 On comments\nWhen following the literate programming paradigm, coding comments (# comment...) should be included in code chunks with echo = False unless you explicitly want your audience to see the code and the comments - save the markdown text for what your audience needs to see.\nGenerally code comments should be used sparingly, if you find yourself needing a lot of comments it‚Äôs a sign the code is too complex, consider re-factoring or abstracting (more on abstractions later).",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-repeating-yourself-1---variables",
    "href": "code_best_practices.html#on-repeating-yourself-1---variables",
    "title": "23¬† Coding best practices",
    "section": "23.7 On repeating yourself #1 - Variables",
    "text": "23.7 On repeating yourself #1 - Variables\nStoring code in multiple places tends to be a liability - if you want to make changes to that piece of code, you have to do it multiple times. More importantly than the time lost making the changes, you need to remember that the code has been duplicated and where all the copies are.\nWithout variables coding would be ‚Äònasty, brutish and short long.‚Äô. It‚Äôs difficult to find the Goldilocks zone between ‚Äòmore variables than I can possibly name‚Äô and ‚ÄòYOLO the project title is typed out 36 times‚Äô.\nMagrittr‚Äôs pipe operator (%&gt;% or command + shift + m) can save you from having to create too many variables. It would be quite ugly if we had to always code like this:\n\nmpg_horesepower_bar_chart &lt;- ggplot(mtcars, aes(x = mpg, y = hp))\n\nmpg_horesepower_bar_chart  &lt;- mpg_horesepower_bar_chart  + geom_point()\n\nmpg_horesepower_bar_chart  &lt;- mpg_horesepower_bar_chart  + labs(title = \"666 - Peaks & Pits - Xbox Horsepower vs Miles per Gallon\")\n\nmpg_horesepower_bar_chart\n\nInstead of this:\n\nmtcars %&gt;%\n  ggplot(aes(x = mpg, y = hp)) +\n  geom_point() +\n  labs(title = \"666 - Peaks & Pits - Xbox Horsepower vs Miles per Gallon\")\n\nPlace strings you‚Äôll use a lot in variables at the top of your notebook, and then use the paste function, rather than cmd + c, to use the contents of the variable where necessary. This way, when you need to change the title of the project you won‚Äôt have to mess around with cmd + f or manually change each title for every plot.\n\nproject_title &lt;- \"666 - Peaks & Pits - Xbox:\"\n\nmtcars %&gt;%\n  ggplot(aes(x = mpg, y = hp)) +\n  geom_point() +\n  labs(title = paste0(project_title, \" Horsepower vs Miles per Gallon\"))\n\n\nGive your variables descriptive names and use your IDE‚Äôs tab completion to help you access long names.\n\nLet‚Äôs say you‚Äôre creating a data frame that you‚Äôre not sure you‚Äôll need. Assume you will need it and delete after if not, don‚Äôt fall into the trap of naming things poorly\ntmp_df ‚ùå\nscreen_name_counts ‚úÖ",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-naming",
    "href": "code_best_practices.html#on-naming",
    "title": "23¬† Coding best practices",
    "section": "23.8 On naming",
    "text": "23.8 On naming\nThe primary objects for which naming is important are variables, functions, code chunks, section titles, and files. Give each of these clear names which describe precisely what they do or why they are there.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-repeating-yourself-2---abstractions",
    "href": "code_best_practices.html#on-repeating-yourself-2---abstractions",
    "title": "23¬† Coding best practices",
    "section": "23.9 On repeating yourself #2 - Abstractions",
    "text": "23.9 On repeating yourself #2 - Abstractions\nDo Not Repeat Yourself, so the adage goes. But some repetition is natural, desirable, and harmless whereas attempts to avoid all repetition can be the opposite. As a rule-of-thumb, if you write the same piece of code three times you should consider creating an abstraction.\nReasonable people disagree on the precise definition of ‚Äòabstraction‚Äô when it comes to coding & programming. For our needs, we‚Äôll think about it as simplifying code by hiding some complexity. A good abstraction helps us to focus only on the important details, a bad abstraction hides important details from us.\nThe main tools for creating abstractions are:\n\nFunctions\nClasses\nModules\nPackages\n\nWe‚Äôll focus on functions and packages.\n\nOn functions\nMake them! There are lots of reasons to write your own functions and make your code more readable and re-usable. We can‚Äôt hope to cover them all here, but we want to impress their importance. Writing functions will help you think better about your code and understand it on a deeper level, as well as making it easier to read, understand and maintain.\nFor a more comprehensive resource, check in with the R4DS functions section\nAlso see the Tidyverse Design Guide for stellar advice on building functions for Tidyverse functions.\n\n\nOn anonymous functions\nFunctions are particularly useful when you want to use iterators like {purrr}‚Äôs map family of functions or base R‚Äôs apply family. Often these functions are one-time use only so it‚Äôs not worth giving them a name or defining them explicitly, in which case you can use anonymous functions.\nAnonymous functions can be called in three main ways:\n\nUsing function() e.g.¬†function(x) x + 2 will add 2 to every input\nUsing the new anonymous function notation: \\x x + 2\nUsing the formula notation e.g.¬†map(list, ~.x + 2)\n\nYou will see a mixture of these, with 3. being used more often in older code, and 2. in more recent code.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-packages",
    "href": "code_best_practices.html#on-packages",
    "title": "23¬† Coding best practices",
    "section": "23.10 On packages",
    "text": "23.10 On packages\nDepending on how many functions you‚Äôve created, how likely you are to repeat the analysis, and how generalisable the elements of your code are, it may be time to create a package.\nAt first building a package is likely to seem overwhelming and something that ‚Äòother people do‚Äô. However, in reality the time it takes to create a package reduces rapidly the more you create them. And the benefits for sharing your code with others are considerable. Eventually you‚Äôll be able to spin up a new package for personal use in a matter of minutes, over time it will become clear which packages should be developed, left behind, or merged into an existing SAMY package.\nVisit the Package Development document for practical tips and guidelines for developing R packages\nsee also: Package Resources",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-namespaces-and-function-conflicts",
    "href": "code_best_practices.html#on-namespaces-and-function-conflicts",
    "title": "23¬† Coding best practices",
    "section": "23.11 On namespaces and function conflicts",
    "text": "23.11 On namespaces and function conflicts\nR manages functions via namespaces. Depending on the order that you import your packages, you may find a function doesn‚Äôt behave as expected. For example, the {stats} package has a filter() function, but so does {dplyr}. By default R will use the most recently-imported package‚Äôs namespace to avoid any conflicts, so filter() will now refer to {dplyr}‚Äôs implementation.\nIf you‚Äôre experiencing weirdness with a function, you may want to restart your R session, change the order of your imports to prevent the same weirdness occurring again. However, a more straightforward approach is to use the package:: notation to explicitly refer to the function you intended to use e.g.¬†dplyr::filter() will avoid any potential conflicts and confusion.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-version-control",
    "href": "code_best_practices.html#on-version-control",
    "title": "23¬† Coding best practices",
    "section": "23.12 On version control",
    "text": "23.12 On version control\nBy default your projects should be stored on Google Drive inside the ‚Äúdata_science_project_work‚Äù folder, in the event of disaster (or minor inconvenience) this means your code and data artefacts should be backed up. However, it‚Äôs still advisable to use a version control system like git - using branches to explore different avenues, or re-factor your code, can be a real headache preventer and efficiency gain.\nAim to commit your code multiple times per day, push to a remote branch (not necessarily main or master) once a day and merge + pull request when a large chunk of work has been finished. Keep your work code and projects in a private repository, add .Rhistory to .gitignore and make sure API keys are stored securely, i.e.¬†not in scripts and notebooks.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-managing-dependencies",
    "href": "code_best_practices.html#on-managing-dependencies",
    "title": "23¬† Coding best practices",
    "section": "23.13 On Managing Dependencies",
    "text": "23.13 On Managing Dependencies\nApplying the Anna Karenina principle to virtual environments:\n\n‚ÄúAll happy virtual environments are alike; each unhappy virtual environment is unhappy in its own way‚Äù\n\n\nR\nThe R ecosystem - led by CRAN and posit (formerly RStudio) - does a great job in managing package-level dependencies. It‚Äôs rare to end up in dependency hell R. However, there is still scope for ‚Äòworks on my machine‚Äô when working with collaborators who have different versions of a package, e.g.¬†person 1 upgrades their {dplyr} version before person 2, and now that .by argument person 1 has used is breaking code on person 2‚Äôs machine.\nTo avoid this, we advise using something like the {renv} package to manage package versions.\n\n\n\n\n\n\nOn using renv collaboratively\n\n\n\n\n\nrenv helps us keep track of package & R versions, which makes deployment 10x easier than without it. However, it can get tricky if we‚Äôre using renv in different ways.\nStart a renv project off withrenv::init(), this will essentially remove all of your packages. You could choose to take your current packages with you, but it‚Äôs not advised. These packages are linked to your RStudio project when using RStudio, which means other RStudio projects will have your current packages if they‚Äôre not themselves using renv.\nrenv::init() creates a renv lockfile, you‚Äôll need to keep this up to date as you work through the project - especially important if collaborating with other people on a project that uses renv. Once installed inside your local project, you can add single packages to the lockfile by renv::record(\"package_name\"). This is preferable to adding a bunch of packages at a time with renv::snapshot() particularly when collaborating. Generally it will be better to give one person control of the lockfile and to communicate about adding packages as and when.\nIf you‚Äôre working in a Quarto Doc or an RMarkdownfile to develop things that don‚Äôt need to pushed to the repo, you can create a .renvignore file like .Rbuildignore, .gitignore etc. and add the folder where the markdowns sit to make sure renv doesn‚Äôt try to sync itself with the packages you‚Äôre experimenting with.\nAt any time you can check your environment is still synced with renv::status(), if your project is out of sync, you may want to renv::clean(), if you‚Äôve got a bunch of packages that are like this:\nThe following package(s) are in an inconsistent state:\npackage       installed recorded used\nbackports     y         y        n\nblob          y         y        n\nbroom         y         y        n\ncallr         y         y        n\ncellranger    y         y        n\nThen you‚Äôll need to renv::clean(actions = \"unused.packages\"), which should get you in working order. There‚Äôs a lot more to {renv} when collaborating but these steps will do a lot to keep your environment in sync and allow collaborators to use your code.\n\n\n\n\n\nPython\n\nUnlike R, it‚Äôs pretty easy to get into deep, deep trouble when working with Python environments. We advise using miniconda, keeping your base environment completely free of packages, and creating virtual environments for projects or large, but commonly-used and important packages like torch.\n\n\n\n\n\n\nTip\n\n\n\nJust remember to activate the correct environment every time you need to install a package!",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-llm-generated-code",
    "href": "code_best_practices.html#on-llm-generated-code",
    "title": "23¬† Coding best practices",
    "section": "23.14 On LLM-generated code",
    "text": "23.14 On LLM-generated code\nGitHub Copilot, ChatGPT, Claude and other LLM-based code generators can be extremely useful, but they are a double-edged sword and should be used responsibly. If you find yourself relying on code you don‚Äôt understand, or couldn‚Äôt re-build yourself, you‚Äôre going to run in to trouble somewhere down the line. You have the time and space to learn things deeply here, so do read the docs, do reference textbooks, and do ask for help internally before relying on LLM-generated code which often looks right but is outdated or subtly incorrect/buggy.\n\n\n\n\n\n\nTip\n\n\n\nYou‚Äôre here because you can problem solve and pick up new skills when you need them - don‚Äôt be afraid to spend extra time understanding a concept or a library.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#great-coding-resources",
    "href": "code_best_practices.html#great-coding-resources",
    "title": "23¬† Coding best practices",
    "section": "23.15 Great Coding Resources",
    "text": "23.15 Great Coding Resources\n\nGoogle SWE Book\nHands on programming with R\nReproducible Analysis, Jenny Bryan",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#exercises",
    "href": "code_best_practices.html#exercises",
    "title": "23¬† Coding best practices",
    "section": "23.16 Exercises",
    "text": "23.16 Exercises\n\nIn your own words, summarise what makes an analysis reproducible.\n\n\nWrite a line in favour and against the claim ‚ÄòCode is an asset not a liability.‚Äô\n\n\nSet up a private github repo on a project inside data_science_project_work/internal_projects and create a new branch then commit, push and pull request a change.\n\n\nAdd your own best practices to this document!",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "calling_apis.html",
    "href": "calling_apis.html",
    "title": "24¬† Calling APIs",
    "section": "",
    "text": "24.1 What is an API?\nAPI, or Application Programming Interface, is a mechanism that allows for two pieces of software to interact and exchange data. APIs function through sending requests from an endpoint. In ‚Äòreal life‚Äô practice, this might mean we as a team send a request to a model for information in return. We either get the requested information or an error message in return.\nAs APIs may provide access to confidential or sensitive information, providers need to able to verify from whom a request is being sent, to ensure they can safely fulfil an ask (and know where to charge for the request!). This is where API keys come in.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#openai-api-overview",
    "href": "calling_apis.html#openai-api-overview",
    "title": "24¬† Calling APIs",
    "section": "24.2 OpenAI API Overview",
    "text": "24.2 OpenAI API Overview\nOpenAI‚Äôs API is a REST (Representational State Transfer) API. Simply put, this allows a client (such as our program) to request data or perform actions on a server (which hosts the AI models), where it retrieves or manipulates resources (e.g., model outputs such as generated text).\n\nHow the API Works\nOpenAI‚Äôs API works on the standard HTTP protocol, which structures communication between the client and server. In this system:\n\nEndpoints are specific paths on the server where the API can be accessed. For example, /v1/chat/completions is an endpoint that allows us to send prompts to a GPT model and receive completions.\nRequests are the actions taken by our application. We send requests with specific inputs (like text prompts), and the API processes them.\nResponses are the API‚Äôs outputs, such as text from a GPT model, an image from DALL-E, or speech-to-text conversions from Whisper.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#practical-use-of-openais-api",
    "href": "calling_apis.html#practical-use-of-openais-api",
    "title": "24¬† Calling APIs",
    "section": "24.3 Practical Use of OpenAI‚Äôs API",
    "text": "24.3 Practical Use of OpenAI‚Äôs API\nWe use the OpenAI API similarly to other public APIs: sign up for an account, obtain an API key, and use it to make API calls to specific models using HTTP requests.\n\nStep One - Obtain API Key and Authentication\nTo start using OpenAI‚Äôs API, you‚Äôll need an API key for authentication. Follow these steps:\n\nGo to platform.openai.com and create an account using your SHARE email address.\nMike will add you to the ‚ÄúSHARE organization‚Äù within the platform, allowing you to access the set aside usage credits we have as a company.\nThen make your way to the api-keys section of the platform and click the green Create new secret key in the top corner.\n\n\n\n\nCreate new secret key by clicking the button in the top right hand corner\n\n\n\nRename the key to something useful, such as the name and number of the project that they key will be used for, and keep the OpenAI project as ‚ÄúDefault project‚Äù and Permissions as ‚ÄúAll‚Äù.\nYou will then be provided with the opportunity to copy the provided API key, this is the one chance you will get to obtain it- after you click off this pop up you won‚Äôt be able to view the full API key again and you‚Äôll need to request a new one. Because of this, make sure you copy the key and add it to this private Google Sheet where the DS team keeps the API Keys. Remember that using the API costs money, so if this key is used by others we risk someone using up all of our API credits! Please see below for some more best practices relating to API key security.\n\n\n\nStep Two - Managing API Keys Securely\nAs outlined above, when working with APIs it‚Äôs essential to manage our API keys securely. An API key grants access to services, and if exposed, others could misuse it, leading to security breaches, unauthorised usage, or unexpected costs. Here are some key principles to follow:\n\nNever Hard-Code API Keys Avoid storing API keys directly in your code as hard-coded variables. This exposes them to anyone with access to your codebase.\nUse Environment Variables Store API keys in environment variables to keep them separate from the code. This ensures sensitive data isn‚Äôt exposed, and it‚Äôs easier to manage keys across different environments if required (development, production, etc.).\nVersion Control Precautions Make sure to add environment files that contain sensitive information (like .env, .Renviron, and .Rhistory) to .gitignore so they don‚Äôt get uploaded to version control systems like GitHub. Exposing API keys in public repositories is a common mistake, and it can be a serious security risk.\n\n\n\n\n\n\n\nExample implementations\n\n\n\n\n\nRStudio\n\nAdd API Key to .Renviron\n\nUse the usethis package to edit the .Renviron file where environment variables are stored. Add the API key like this:\n\nusethis::edit_r_environ(scope = \"project\")\n\nThis will open the .Renviron file in your editor. Note that scope = \"project\" scope means that the .Renviron file will be created in your specific R project folder. This means the environment variables (like your API key) will only be available when you are working inside that project. It‚Äôs a good way to keep project-specific configuration separate from other projects.\nThen add the following line to store your API key (replace your-api-key-here with the actually API key)\n\n# Write this within the .Renviron file and save it\nOPENAI_API_KEY=your-api-key-here\n\n\nAccess the API Key in your R Script\n\nYou can access the API key in your R scripts using Sys.getenv()\n\napi_key &lt;- Sys.getenv(\"OPENAI_API_KEY\")\n\nor if you need to call the API key in a function (such as BERTopicR) it could be\n\nrepresentation_openai &lt;- bt_representation_openai(fitted_model,\n                                                  documents,\n                                                  openai_model = \"gpt-4o-mini\",\n                                                  nr_repr_docs = 10,\n                                                  chat = TRUE,\n                                                  api_key = Sys.getenv(\"OPENAI_API_KEY\"))\n\n\nAdd .Renviron to .gitignore\n\nObviously this is only relevant if you are deploying a repo/project to GitHub, but we can make sure to exclude the .Renviron file to our .gitignore file\n\n# Exclude .Renviron file\n.Renviron\n\nPython\n\nCreate a .env file\n\nIn the root directory of your project, create a .env file. The best way to do this is using command line tools (touch and nano)\nWithin the terminal create an empty .env file by running\n\ntouch .env\n\nand then edit it by running\n\nnano .env\n\nand finally within the nano editor, type the following to add your API key (replace your-api-key-here with the actually API key)\n\nOPENAI_API_KEY=your-api-key-here\n\n\nUse the python-dotenv library\n\nInstall python-dotenv by running\n\npip install python-dotenv\n\n\nAccess the API Key in your script\n\nIn your Python script, load the .env file and access the API key\n\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Access the API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n\nAdd .env to .gitignore\n\nSimilar to the RStudio implementation above, add .env to your .gitignore\n\n# Exclude .env file\n.env\n\n\n\n\n\n\nStep Three - Making Requests to the API\nTo actually make requests to the OpenAI API we use python, and specifically the official OpenAI SDK. You can install it to your python environment simply via pip by running\n\npip install openai\n\nThe documentation on-line surrounding calling the OpenAI API is extremely extensive and generally good, however the API and underlying models do get updated quite often and this can cause code to become redundant or not act as one may expect. This can be particularly unwelcome when you run a previously working script to ping the API, get charged, but don‚Äôt receive an output that is useful.\nThe simple way to call the API and obtain a ‚Äòhuman-like response‚Äô to a prompt is with this code adapted from the OpenAI API tutorial:\n\nfrom openai import OpenAI\nclient = OpenAI(api_key = OPENAI_API_KEY)\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Write a short poem about RStudio.\"}\n    ]\n)\n\nprint(completion.choices[0].message)\n\nDon‚Äôt worry about what everything means, we‚Äôll explain this in a bit more detail below. But firstly, one thing to realise is that this code above is effectively the same as going onto the ChatGPT website and typing into the input box ‚ÄúYou are a helpful assistant. Write a short poem about RStudio.‚Äù for the model gpt-4o-mini. So effectively this code calls the API once, with an input and receives an output from the model.\n\nChat Completions\nTo use one of the text models, we need to send a request to the Chat Completions API containing the inputs and our API key, and receive a response containing the model‚Äôs output.\nThe API accepts inputs via the messages parameter, which is an array of message objects. Each message object has a role, either system, user, or assistant.\n\nThe system message is optional and can be used to set the behaviour of the assistant\nThe user messages provide requests or comments for the assistant to respond to\nAssistant messages store previous assistant responses, but can also be written by us to give examples of desired behaviour (however note we can also provide examples within the user message- which is what we tend to do in our workflows)\n\nFor example:\n\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n  ]\n)\n\nWhilst this chat format is designed to work well with multi-turn conversations, in reality we use it for single-turn tasks without a full conversation. So we would normally have something more like:\n\nfrom openai import OpenAI\nclient = OpenAI(api_key = OPENAI_API_KEY)\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant who specialised in sentiment analysis\"},\n        {\"role\": \"user\", \"content\": \"What is the sentiment of the following text: 'I love reading this Handbook'\"}\n    ]\n)\n\nprint(completion.choices[0].message)\n\nThe response (defined as completion in the code above) of the Chat Completions API looks like the following:\n\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"The sentiment of the text 'I love reading this Handbook' is positive. The use of the word 'love' indicates a strong positive emotion towards the Handbook.\",\n        \"role\": \"assistant\"\n      },\n      \"logprobs\": null\n    }\n  ],\n  \"created\": 1677664795,\n  \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n  \"model\": \"gpt-4o-mini\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 26,\n    \"prompt_tokens\": 13,\n    \"total_tokens\": 39,\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0\n    }\n  }\n}\n\nWe can see there is a lot of information here, such as the model used and the number of input tokens. You will notice the response is a dictionary and made up of key-value pairs to help organise the relevant information. However, we are mostly focussed on the models output (that is, the assistants reply), which we can extract by running:\n\nmessage = completion.choices[0].message.content",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#throughput",
    "href": "calling_apis.html#throughput",
    "title": "24¬† Calling APIs",
    "section": "24.4 Throughput",
    "text": "24.4 Throughput\nAn important part of using APIs is understanding the throughput - how many requests the API can handle efficiently within a given period.\nBroadly, we need to be able to balance cost, model selection, and efficient request handling.\n\nUnderstanding Tokens and Model Usage\nAPIs like OpenAI‚Äôs typically have costs associated with their usage, and this is often measured in tokens. When you input text or data into an API, it is broken down into tokens, which are individual units of language (like parts of words or characters).\n\nInput Tokens: These are the tokens sent to the API (e.g., your prompt to GPT). Every word, punctuation mark, or whitespace counts toward your input tokens.\nOutput Tokens: These are the tokens returned from the API as a response (e.g., the AI‚Äôs reply). The longer and more complex the output, the more tokens are consumed.\n\nManaging tokens is crucial because they directly impact the cost of API usage. Different models have different costs per token, with more advanced models being more expensive but often providing better results. For example, as of writing this document (October 2024), the pricing for gpt-4o-mini is $0.150/1M input tokens and $0.500/1M output tokens compared to $5.00/1M input tokens and $15.00/1M output tokens for gpt-4o. Or in other words, gpt-4o-mini is ~30x cheaper than gpt-4o! Check out the model pricing information to see the latest costs, and the model pages to see the differences in model capabilities (i.e.¬†context windows, maximum output tokens, training data).\n\nBased on our experience with the cost-performance trade-off, we strongly recommend always starting with gpt-4o-mini for initial development and testing. Only consider upgrading to gpt-4o at the very end of your workflow, and only if absolutely necessary. This means you should use gpt-4o-mini until you are 100% satisfied with your prompt, input data, and downstream analyses. In most cases, you‚Äôll likely find that gpt-4o-mini meets all your requirements, making the switch to gpt-4o unnecessary.\n\n\n\n\n\n\n\nTokens to Words\n\n\n\n\n\n\nA token is typically about 4 characters of English text.\n100 tokens are roughly equivalent to 75 words.\n\n\n\n\n\nHow to be more efficient with costs\nDespite these costs, there are some strategies we can implement to ensure we make the most of the API usage without unnecessary spending:\n\nRemove Duplicate Data: Ensure your dataset is free from duplicates before sending it to the API. Classifying the same post multiple times is a waste of resources. A simple deduplication process can help reduce unnecessary API calls and cut down on costs, if we then remember to join this filtered dataset (with the model output) back to the original data frame.\nClean and Pre-filter Data: Before sending data to the API, clean it to remove irrelevant or low-value entries. For instance, if you‚Äôre classifying sentiment on social media posts about mortgages, posts that are clearly not related to the subject matter should be filtered out beforehand. As a rule of thumb it is probably best to run data through the OpenAI API as one of the final analysis steps.\nSet a Max Token Limit: Define a max_tokens value in your API request to avoid long or unnecessary responses, especially when you only need a concise output (such as a sentiment label or a topic classification). For tasks like classification, where the output is typically short, limiting the tokens ensures the model doesn‚Äôt generate verbose or off-topic responses, thus reducing token usage.\nUse the Appropriate Model: Choose the model that best fits your use case. More advanced models like gpt-4o can be expensive, but simpler models like gpt-4o-mini may provide adequate performance at a fraction of the cost. Always start with the least expensive model that meets your needs and only scale up if necessary.\n\nOptimise Input Length: Reduce the length of the input prompts where possible. Long prompts increase the number of input tokens and, therefore, the cost. Make your prompts as concise as possible without losing the clarity needed to guide the model effectively.\nBatch Processing: Consider grouping multiple similar requests together when appropriate. While asynchronous requests can optimise speed, batching can further reduce overhead by consolidating similar requests into fewer calls when applicable. Additionally, the OpenAI Batch API can offer cost savings in specific use cases.\n\n\n\nEfficient request handling\nIn addition to costs and model capabilities, there are also rate limits associated with APIs.\nThese limits are measured in 5 ways that we need to be mindful of:\n\nRPM - Requests per minute (how many times we call the API per minute). For our current subscription for the vast majority of models we can perform 10,000 RPM.\nRPD - Requests per day (how many times we call the API per day). For our current subscription for the vast majority of models we can perform 14,400,000 RPD (10k x 1440 minutes).\nTPM - Tokens per minute. For our current subscription for the vast majority of models we can perform take in 10,000,000 TPM.\nTPD - Tokens per day. For our current subscription for the vast majority of models we can perform 1,000,000,000 TPD in batch mode.\nIPM - images per minute. For our current subscription dall-e-2 can take 100 images per minute, and dall-e-3 can take 15 images per minute.\n\nIn reality if it is only a single user calling the API at any one time, it is highly unlikely any of these limits will be reached. However, often there are multiple users calling the API at the same time (working on different projects, workflows etc) and even if we use different API keys, the rate limits are calculated at an organisation level.\nThere are a couple of ways we can overcome this. The first is to use the (Batch API)[https://platform.openai.com/docs/guides/batch], which enables us to send asynchronous groups of requests. This is actually 50% cheaper than the regular synchronous API, however you are not guaranteed to get the results back (each batch completes with 24 hours). Secondly, we can automatically retry requests with an exponential backoff (performing a short sleep when the rate limit is hit, and then retrying the unsuccessful request). There are a few implementations in Python for this, including the tenacity library, the backoff library, or implementing it manually. Examples for these are in the Batch API docs so we will not go into the implementation of them here.\n\n\nOptimising speed and throughput\nIn addition to managing rate limits, another critical aspect of API usage is optimising the speed of requests. When handling large datasets or numerous API calls, the time taken for individual requests can add up quickly, especially if each request is handle sequentially. To improve the efficiency of our workflows, we can use asynchronous calling.\nAsynchronous calling allow multiple requests to be sent concurrently rather than waiting for one to finish before sending the next. This approach is especially useful when processing tasks that are independent of each other, such as classifying individual social media posts.\nWhile asynchronous calling can greatly reduce the time taken to process large datasets, they do not circumvent API rate limits. Rate limits such as Requests Per Minute (RPM) and Tokens Per Minute (TPM) still apply to the total volume of requests, whether sent asynchronous or not. This means that even with asynchronous requests, you need to be mindful of the number of requests and tokens you are sending per minute to avoid hitting rate limits.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#structured-outputs",
    "href": "calling_apis.html#structured-outputs",
    "title": "24¬† Calling APIs",
    "section": "24.5 Structured Outputs",
    "text": "24.5 Structured Outputs\nAs per an update in August 2024, the API introduced a feature called Structured Outputs. This feature ensures the model will always generate responses that match a defined schema. While the explanation of how they work is beyond the scope of this handbook (there are good resources online from OpenAI), we will discuss why they are important and briefly provide a simple example to show how to implement them in a workflow.\n\nWhy Not Normal Output?\nThe output of LLMs is ‚Äúnatural language‚Äù, which, as text analytics practitioners, we know isn‚Äôt always in a machine-readable format or schema to be applied to downstream analyses. This can cause us headaches when we want to read the output of an LLM into R or python.\nFor example, say we wanted to use gpt-4o-mini‚Äôs out-of-the-box capabilities to identify emotion in posts. We know that a single post can have multiple emotions associated, so this would be a multilabel classification (data point can be assigned to multiple categories, and the labels are not mutually exclusive) problem. We would normally have to include a detailed and complex prompt which explains how we want the response to be formatted, for example provide each output emotion separated by a semi-colon, such as \"joy; anger; surprise\". Despite this, given enough input data the model will inevitably provide an output that does not follow this format, and provide something like this (one line per input post):\n\njoy; anger\njoy\nThis post contains sections of joy and some bits of anger\nsurprise; sadness\n\nWe can see the third output here has not followed the prompt instructions. Whilst the other three outputs can be easily read into R using something like delim = \";\" within the read_delim() function, the incorrect output would cause a lot more issues to parse (or we might even decide to just retry these incorrectly formatted responses, costing more time and money).\nSimilarly, we might be trying to perform a simple sentiment analysis using a GPT model and ask it to classify posts as either positive, negative, or neutral. The output from the API could easily be something like this (one line per input post):\n\nneutral\nNeutral\npositive\nNegative\n\nAgain, we can see a lack of consistency in how the responses are given, despite the prompt showing we wanted the responses to be lowercase.\n\n\nBenefits of Structured Outputs\nSo hopefully you can see that the benefits of Structured Outputs include:\n\nSimple prompting - we don‚Äôt need to be overly verbose when specifying how the output should be\nDeterministic names and types - we are able to guarantee the name and type of an output (i.e.¬†a number if needed for confidence score, and a classification label that is one of ‚Äúneutral‚Äù, ‚Äúpositive‚Äù, ‚Äúnegative‚Äù). There is no need to validate or retry incorrectly formatted responses.\nEasy post-processing and integration - it becomes easier to integrate model responses into further workflows or systems.\n\n\n\nSimple Example Implementation\nTo showcase an example, let‚Äôs say we want to classify posts into emotions (joy, anger, sadness, surprise, and fear) in a multilabel setting, to ensure the response is consistently formatted. If you‚Äôre familiar with coding in python you might recognise Pydantic, which is a widely used data validation library for Python.\n\nDefine the schema\n\nWe define a schema that includes only the emotion labels. This schema ensures that the model returns a list of emotions it detects from the text, following the structure we define. We do this by creating a Pydantic model that we call EmotionClassification which has one field, emotions. This field is a list that accepts only predefined literal values, allowing multiple emotions to be included in the list when detected.\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nfrom typing import Literal\n\nclass EmotionClassification(BaseModel):\n    emotions: list[Literal[\"joy\", \"anger\", \"sadness\", \"surprise\", \"fear\"]]  # List of detected emotions\n\n\nCall the model\n\nWe then call the model as before, but importantly we include our schema via the response_format parameter.\n\n\n\n\n\n\nNote\n\n\n\n\n\nHere we use client.beta.chat.completions.parse rather than client.chat.completions.create because Structured Output is only available using the .beta class.\n\n\n\n\n# Sample text to classify\ntext_to_classify = \"I was so happy when I got the job, but later I felt nervous about starting.\"\n\nclient = OpenAI(api_key = OPENAI_API_KEY)\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are an emotion classifier. For the provided text, classify the emotions.\"},\n        {\"role\": \"user\", \"content\": text_to_classify}\n    ],\n    response_format=EmotionClassification,\n)\n\nemotion_results = completion.choices[0].message.parsed\n\nIf we view the output of this we see we get a nice output in JSON format:\nEmotionClassification(emotions=['joy', 'fear'])",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#the-playground",
    "href": "calling_apis.html#the-playground",
    "title": "24¬† Calling APIs",
    "section": "24.6 The Playground",
    "text": "24.6 The Playground\nThe Playground is a tool from OpenAI that you can use to learn how to construct prompts, so that you can get comfortable using querying the model and learning how the API works. Note however that using the Playground incurs costs. While it‚Äôs unlikely you will rack up a large bill be analysing large corpora within the playground, it is still important to be mindful of usage.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "25¬† Resources",
    "section": "",
    "text": "25.1 Data Visualisation",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#data-visualisation",
    "href": "resources.html#data-visualisation",
    "title": "25¬† Resources",
    "section": "",
    "text": "Capture Cookbook\nR Graph Gallery",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#literate-programming",
    "href": "resources.html#literate-programming",
    "title": "25¬† Resources",
    "section": "25.2 Literate Programming",
    "text": "25.2 Literate Programming\n\nQuarto Guide\nQuarto Markdown Basics\nRStudio R Markdown guide\nVisual R Markdown",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#package-development",
    "href": "resources.html#package-development",
    "title": "25¬† Resources",
    "section": "25.3 Package Development",
    "text": "25.3 Package Development\n\nR Packages Book\npkgdown\nroxygen2\nusethis\ndevtools\ntestthat\nTidyverse Design Guide\nHappy Git with R",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#shiny",
    "href": "resources.html#shiny",
    "title": "25¬† Resources",
    "section": "25.4 Shiny",
    "text": "25.4 Shiny\n\nMastering Shiny\nEngineering Enterprise-grade Shiny Applications with Golem\nOutstanding Shiny Interfaces\nModularising Shiny Apps, YT\nPresenting Golem, YT\nStyling Shiny, Joe Chen, YT",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#text-analytics",
    "href": "resources.html#text-analytics",
    "title": "25¬† Resources",
    "section": "25.5 Text Analytics",
    "text": "25.5 Text Analytics\n\nText Mining with R (Tidytext)\nSupervised Machine Learning for Text Analysis in R\nSpeech & Language Processing\nNatural Language Processing with Transformers - we have a physical copy floating around the office",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#youtube-channels",
    "href": "resources.html#youtube-channels",
    "title": "25¬† Resources",
    "section": "25.6 YouTube Channels",
    "text": "25.6 YouTube Channels\nThese channels will help you see the power and flexibility of data analysis & science in R:\n\nJulia Silge\nDavid Robinson",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#what-is-an-api",
    "href": "calling_apis.html#what-is-an-api",
    "title": "24¬† Calling APIs",
    "section": "",
    "text": "Note\n\n\n\nThink of an API like ordering food at a restaurant. You, (the intrepid DS team member) make a request to the waiter (the API), who takes it to the kitchen (the server) and brings back your meal (the data) or tells you it‚Äôs not available on the menu (an error).",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#why-do-we-use-api-keys",
    "href": "calling_apis.html#why-do-we-use-api-keys",
    "title": "24¬† Calling APIs",
    "section": "24.2 Why Do We Use API Keys?",
    "text": "24.2 Why Do We Use API Keys?\nThere are a number of use cases when we‚Äôll need to request the services of models, be it in client-facing projects or internal work. We may face a problem or research question that requires us to request access to specific models through an API.\nGenerally speaking, we‚Äôll find ourselves using something like the OpenAI API for tasks, especially when we are in need of their advanced text models (GPT) for tasks such as classification, or even for specific stages of a workflow, like topic modelling. API keys thus help provide access to these tools, and consequently, are a great resource when we have a particular research problem to solve.\nOne of the key advantages of using an API instead of downloading and running these models locally (or utilising open-source models) is that it allows us to leverage the computational power and optimisation of the models without needing expensive hardware or vast computational resources.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#managing-api-keys-dos-donts",
    "href": "calling_apis.html#managing-api-keys-dos-donts",
    "title": "24¬† Calling APIs",
    "section": "24.3 Managing API Keys: Dos & Don‚Äôts",
    "text": "24.3 Managing API Keys: Dos & Don‚Äôts\nAPI keys are foundational for access to leverage these AI models, however, as one fictional mentor-figure once advised, ‚Äúwith great power comes great responsibility‚Äù.\nWe know API keys have the potential of granting access to sensitive information or services, and if leaked, this could spell trouble down the line as people abuse requests or ramp up unexpected costs. We need to be particularly prudent in how we handle API key ‚Äòsecurity hygiene‚Äô.\n1. Never ‚ÄòHard-Code‚Äô API Keys\nAvoid storing API keys directly in your code as hard-coded variables. This exposes them to anyone with access to your codebase.\n2. Use Environment Variables\nStore API keys in environment variables to keep them separate from the code. This ensures sensitive data isn‚Äôt exposed, and it‚Äôs easier to manage keys across different environments if required (development, production, etc.). We will see later in this document that EndpointR offers a tenable solution to this concern. You can also store them as managed secrets by providers like GitHub or Google Colab, for use outside of R/Rstudio/VScode.\n3. Version Control Precautions\nMake sure to add environment files that contain sensitive information (like .env, .Renviron, and .Rhistory) to .gitignore so they don‚Äôt get uploaded to version control systems like GitHub. Exposing API keys in public repositories is a common mistake, and it can be a serious security risk.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#creating-your-first-api-key",
    "href": "calling_apis.html#creating-your-first-api-key",
    "title": "24¬† Calling APIs",
    "section": "24.4 Creating Your First API Key",
    "text": "24.4 Creating Your First API Key\nWe use the OpenAI API similarly to other public APIs: sign up for an account, obtain an API key, and use it to make API calls to specific models using HTTP requests. You may find there are some use cases whereby you‚Äôd need to obtain a HuggingFace API key (dependent on the project or task), in which case, the steps are fairly similar to those of OpenAI.\nThe principle to navigating public APIs remain pretty consistent across the board: sign up for an account (be it OpenAI API, HuggingFace etc), obtain your API key, and then use it to make API calls to specific models. In the case of OpenAI‚Äôs API, these calls use HTTP requests.\n\nObtain API Key & Authentication: OpenAI API\nLet‚Äôs say you‚Äôre wanting to work on a text model to help with classification, or you want to use OpenAI‚Äôs advanced models to help label some data (and your prompt is top-notch). You‚Äôll need this key access to their API, much like any fearless, fantasy RPG protagonist needs a way to unlock useful treasures.\nFor OpenAI, you will need to ‚Äúunlock‚Äù access to their models by acquiring their key for authentication. By performing the following:\n1. Go to platform.openai.com, and create an account using your SAMY email address.\n2. Mike will then add you to the SAMY organisation within the platform. This allows you to access the usage credits we have a company for tasks which require API requests.\n3. You will then want to reach the api-keys section. Click the green Create new secret key which you‚Äôll find in the top corner.\n4. You‚Äôll have the option to rename the key to something useful - the name of the project or internal work will be helpful - and keep the OpenAI project as ‚ÄúDefault project‚Äù and Permissions as ‚ÄúAll‚Äù.\n5. You will then be provided with the opportunity to copy the provided API key, this is the one chance you will get to obtain it- after you click off this pop up you won‚Äôt be able to view the full API key again and you‚Äôll need to request a new one. Because of this, make sure you copy the key and add it to this private Google Sheet where the DS team keeps the API Keys. Remember that using the API costs money, so if this key is used by others we risk someone using up all of our API credits! We‚Äôve discussed above the Dos and Don'ts of managing API keys, so if in any doubt, please refer to that section at anytime.\n\n\n\n\n\n\nNote\n\n\n\nThe OpenAI API documentation is pretty solid (albeit at times fiddly to get your head around), so do take the time to have a read through some of their use cases for making API requests.\n\n\n\n\nHuggingFace API Key\nMuch like creating your own API key for OpenAI‚Äôs API, you can follow similar steps if you need to use a HuggingFace model for a particular workflow:\n1. You will need to sign up for an account on HuggingFace with your SAMY email, and as before, request permission to have access to the organisational token permissions.\n2. After creating your account, log in and go to your Account Settings. You can find this in the top-right corner of the screen.\n3. You can find a dropdown option to access Access Tokens - this is where you can generate and manage your API keys.\n4. You can create your new API key by clicking on Generate New API Key. Again, give it an appropriate name based on your research question or project. Make sure to adjust your permissions to Read access before generating the key so you will be able to access the API (more on permissions in the HuggingFace documentation on tokens.\n5. Again, be sure to copy your new API key before leaving the page, as this is the only time it will be visible for you. The DS team pastes and tracks our API keys here, where you can note the date of token creation, the relevant project, and paste your key into a cell.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#huggingface-based-models-task-specific",
    "href": "calling_apis.html#huggingface-based-models-task-specific",
    "title": "24¬† Calling APIs",
    "section": "25.1 HuggingFace Based Models (Task-Specific)",
    "text": "25.1 HuggingFace Based Models (Task-Specific)\n\n\n\n\n\n\nNote\n\n\n\nFor more on HuggingFace-specific workflows, you can read more about embedding text data and fetching dedicated endpoints, you can see Jack‚Äôs vignette here\n\n\nFor specialized tasks like named entity recognition, sentiment classification, or specific domain models, we often use HuggingFace models through EndpointR. Here, we will be using a HuggingFace Inference API with which to embed our data.These models are typically designed for specific tasks and require minimal prompting.\nAgain, you will need to create your API keys and set them into your workflow prior to these stages, which we have discussed in the previous sections.\n\n\n\n\n\n\nMini-Case Study: Embedding & Classifying Text\n\n\n\n\n\nFirstly, we‚Äôll need to set up our workflow to work with EndpointR and wrangle our text:\n\nlibrary(EndpointR)\nlibrary(dplyr)\nlibrary(httr2)\nlibrary(tibble)\n\nUnlike a prompt-based, OpenAI workflow, we simply provide an inference URL when working with HF.\n\n# inference api url for embeddings\nembed_url &lt;- \"https://router.huggingface.co/hf-inference/models/sentence-transformers/all-mpnet-base-v2/pipeline/feature-extraction\"\n\nYou can (and most definitely should!) trial out the model‚Äôs functionality by sampling a text/handful of sample texts as opposed to the fully fledged data frame. For the sake of this case study, we‚Äôll jump to classifying a data frame, as that‚Äôs going to be a pretty common occurrance for our workflows, but don‚Äôt forget to experiment with a randomised sample first in real world cases.\n\nembedding_result &lt;- hf_embed_df(\n  df = my_data,\n  text_var = text,      # column with your text\n  id_var = id,          # column with unique ids\n  endpoint_url = embed_url,\n  key_name = \"HF_API_KEY\"\n)\n\nFor a classification workflow, it works much the same; we‚Äôre just changing the inference API URL to one that better suits our workflows.\n\nclassify_url &lt;- \"https://router.huggingface.co/hf-inference/models/distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n\nclassification_result &lt;- hf_classify_df(\n  df = my_data,\n  text_var = text,\n  id_var = id,\n  endpoint_url = classify_url,\n  key_name = \"HF_API_KEY\"\n)\n\nYou will need to evaluate the output carefully, thinking about whether it 1) makes sense from a ‚Äúas a human, I‚Äôd label this too‚Äù perspective, and 2) consider how you‚Äôd wrangle the output for further, downstream analyses and steps.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#openai-based-models-prompt-based",
    "href": "calling_apis.html#openai-based-models-prompt-based",
    "title": "24¬† Calling APIs",
    "section": "25.2 OpenAI Based Models (Prompt-Based)",
    "text": "25.2 OpenAI Based Models (Prompt-Based)\nFor more complex, flexible tasks requiring natural language instructions, we use OpenAI‚Äôs models. This is where prompting and structured outputs become crucial.\nPrompting involves crafting clear instructions that guide the model‚Äôs behavior. Schemas ensure consistent, machine-readable outputs by defining the exact format we want returned (more on structured outputs and schemas below). Again, we want to be mindful of both LLM costs and workflow rigour, so always test your prompt on a sample/dummy data before going full steam ahead with your main dataset.\n\n\n\n\n\n\nMini-Case Study: Classification with OpenAI API\n\n\n\n\n\nLet‚Äôs say you get assigned a research project for a cat food brand who want to better understand how people feed their beloved furry friend. You pre-process and clean your data before deciding you will need the help of LLMs to better understand the broader ‚Äòcategories‚Äô of brand discussions within a large dataset. Sure, you could use some regex to try filter out key terms around cats and their food being mentioned, but ultimately, this is a limited tool for the ask at hand. We may need to identify the overall attributes (i.e.¬†characteristics) of cat food and cats‚Äô eating preferences, and due to the qaulitative nature of the data, this is difficult to articulate into a regex.\nWe can, therefore, 1) use our prompt which will (hopefully) guide the model to label each mention to an appropriate attribute, and 2) define a schema to guarantee our structured output which provides a cleaner, JSON format for downstream analyses.\nFirst, let‚Äôs create a brief prompt:\n\nattribute_prompt = \"Classify the text based on brand attributes.\n\nThe accepted labels are Cat_Preference, Cat_Breed, Cat_Digestion_Issues, Catfood_Cost.\n\nCat_Preference refers to texts discussing owners' cats having likes or dislikes towards food types.\n\nCat_Breed refers to the eating habits across different cat breeds.\n\nCat_Digestion texts contain discussions around cats' stomach issues as a result of the food they eat.\n\nCatfood_Cost refers to texts discussing the price of varying cat foods.\"\n\nThen, we want to ensure our output is structured.\n\n\n\n\n\n\nNote\n\n\n\nFor this example, I‚Äôve stuck with schema_enum, as we only have a few attributes to label, but you may find as the number of attributes increase, you‚Äôll find schema_boolean a better choice to produce a much more manageable output.\n\n\n\n# Define our desired output structure\nattributes_schema &lt;- create_json_schema(\n  name = \"attribute_analysis\",\n  schema = schema_object(\n  attributes = schema_enum(\n    c(\"Cat_Preference\", \"Cat_Breed\", \"Cat_Digestion_Issues\", \"Catfood_Cost\"))\n  )\n)\n\nWe would then call this schema when making our request to the model, ensuring we have a sensible (and deterministic) output. This is much better for our work downstream.\n\ncatfood_df &lt;- oai_complete_df(\n  catfood_descriptions,\n  text_var = text,\n  id_var = id,\n  schema = attributes_schema, # we ensure our output is structured\n  output_file = NULL,\n  system_prompt = attribute_prompt,\n  concurrent_requests = 2, \n  chunk_size = 5\n)\n\n\n\n\n\nBest Practices for Team Usage: Making Requests to the API\nSomething you‚Äôll find in your data science work is the importance of not just pressing a ‚Äòrun code‚Äô button and assuming the work is done - we need the time and space to recognise whether something looks ‚Äònot quite right‚Äô, or whether what we‚Äôre doing is statistically or empirically viable!\nIn the case of making API requests, not only can spamming API asks be costly in usage when accessing LLM models, but we run the risk of missing pivotal insight.\nFor example, not sense-checking our test output after running a prompt through the model can cause us more problems down the line if we later see the output looks ‚Äòoff‚Äô. Hence why workflows can be iterative and require food for thought between sending requests to text models.\nTo save both money and potential issues down the line when making requests to the API, the emphasis should be on mini-experimentation - or testing- throughout the recourse of a workflow. Create dummmy data initially to check things are working - both from the sense of ‚Äòthe code works‚Äô in addition to ‚Äòthe model output looks like something a human would label‚Äô, then begin to build up the prompt as needed as you run a sample of your working data.\nBeing deliberate and measured in this initial part of your workflow can feel as though you‚Äôre being ‚Äúslow‚Äù. In reality, not only is this far more likely to create higher quality output, but this is where the fun part of data science really kicks into gear: creativity, critical thinking, and employing the scientific method to prove our assumptions.\n\n\n\n\n\n\nNote\n\n\n\nIt can feel a bit tricky juggling the demands of workflows, deadlines, and producing quality output all at the same time. I advise even writing bullet points or notes as you go through each stage of testing a prompt on dummy/sampled data. Are there any types of data the model struggles to handle? What about edge cases? Have you included some ‚Äògotchas‚Äô into your prompt to ensure the model can handle these more subtle cases? Anything of interest spring to mind? Asking these questions keeps you engaged throughout the work process.\n\n\nThere will be use cases when you might be tempted to reach out to the LLM approach right off the bat, say, when you‚Äôre expected to extract entities or explore mentions which contain specific brands or terms. Whilst there are certainly times when LLMs can help us in our research question, often simpler approaches prove to be quicker, cheaper, and just as fruitful in their output. In this instance, creating a simple regex can save us both time and money whilst helping us explore the data landscape.\nIn other words, before reaching out automatically to signal out to our overlord LLMs for assistance, take a step back and consier what other Data Science methods we have in our toolbox that can do the job just as well - far more efficiently.\n\n\n\n\n\n\nExample Use Case: Dummy Data & Prompt Building\n\n\n\n\n\nWe‚Äôll load up EndpointR in case we haven‚Äôt already.\n\nlibrary(EndpointR)\n\nAs a reminder, we‚Äôll use those lovely functions which simplify API calling massively. In this example, we‚Äôll assume we‚Äôve already set the API key. So we just need to retrieve it as an environment variable:\n\nget_api_key(\"OPENAI_API_KEY\")\n\nFor this sample workflow, we‚Äôll be using OpenAI‚Äôs API to perform some simple sentiment analysis. Because we want to check it works right off the bat, we can get a completion for a single text only:\n\nsentiment_system_prompt = \"Classify the text into sentiment categories.\nThe accepted categories are 'positive', 'negative', 'neutral', and 'mixed'.\nA 'mixed' text contains elements of both positive and negative.\"\n\ntext &lt;- \"I love cats, they're so cute and full of mischief.\"\n\noai_complete_text(\n  text = text,\n  system_prompt = sentiment_system_prompt,\n  model = \"gpt-4.1-nano\" # EndpointR uses the cheaper model but you can change this as your needs suit\n)\n\nOnce we‚Äôve identified that the code works, and the model isn‚Äôt spouting total nonsense, we can move onto create a dummy data frame - here, you can put in edge cases or gotchas and see how well your prompt accounts for these - or generate a randomised sample from your real-world data. Again, we want to be prudent enough not to run 100k + rows immediately in case we need to go back and tweak things - remember, API request costs add up!\nNotice here we use a different function when dealing with dataframes:\n\nreview_df &lt;- data.frame(\n  id = 1:5,\n  text = c(\n    \"Absolutely fantastic service! The staff were incredibly helpful and friendly.\",\n    \"Terrible experience. Food was cold and the waiter was rude.\",\n    \"Pretty good overall, but nothing special. Average food and service.\",\n    \"Outstanding meal! Best restaurant I've been to in years. Highly recommend!\",\n    \"Disappointed with the long wait times. Food was okay when it finally arrived.\"\n  )\n)\n\noai_complete_df(\n  review_df,\n  text_var = text,\n  id_var = id,\n  output_file = NULL, # leave this to 'auto' to have your results written to a file in your current working directory\n  system_prompt = sentiment_system_prompt,\n  concurrent_requests = 2, \n  chunk_size = 5\n)\n\nWe will then need to continuously review the output in order to see whether 1) our prompt is pretty robust and 2) the model is able to fulfil our research question sufficiently. If we cannot answer these, it‚Äôs time to consider other approaches or ways we can solve our problem.\nOnce we‚Äôre happy with our testing phase, we can then repeat these steps above with our data proper.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#understanding-throughput",
    "href": "calling_apis.html#understanding-throughput",
    "title": "24¬† Calling APIs",
    "section": "25.3 Understanding Throughput",
    "text": "25.3 Understanding Throughput\nThe throughput measures how many requests the API can handle efficiently within a time period.\nAs implied through this document, there‚Äôs a knack to balancing the cost, model selection (and research question demands), and efficient request handling as we call APIs.\n\nTokens & Model Usage\nAPIs generally have costs with their usage (there are freebie options, however this may come at the cost of output robustness). We can measure this usage via tokens. When we input text data into an API, it then breaks our data down into tokens, which we can define as individual units of a language.\nNaturally, this means we need to keep tabs on the amount of input and output tokens which are produced in a workflow. Be sure to check out the model pricing information to see the latest costs of the model you‚Äôd like to use, in addition to the general differences in model capabilities - we don‚Äôt always need the most ‚Äòadvanced‚Äô model to answer our research question, and consequently, can save on resources.\n\n\n\n\n\n\nNote\n\n\n\nTokens are generally about 4 words of English text. 100 tokens are approximately 75 words.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Calling APIs</span>"
    ]
  }
]