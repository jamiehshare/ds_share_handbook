# Machine Learning modelling

Machine learning models (specifically language models) play an important role in extracting meaningful insights from the vast amounts of data we work with on a daily basis.

Whilst our job mainly focussed on using these ML models to perform classification tasks (consider sentiment analysis, named entity recognition, emotion detection, peaks and pits etc), ML in text analytics goes beyond classification and includes text generation, translation, summarisation, question answering to name a few implementations.

For us data scientists, our goal is often to decide the best approach to answer a research questions- be it using predefined OOTB (out of the box) models or bespoke models that we need to train or fine-tune ourselves to produce a suitable approach to a client's needs.

This page aims to provide a comprehensive overview of the process involved in training and fine-tuning NLP-based machine learning models for classification tasks. It covers everything from data acquisition and labelling to model evaluation and determining when a model is suitable for inference. While technical in nature, this guide remains code-agnostic to focus on concepts and best practices that are universally applicable.


## Understanding the Problem: Defining the Research Question

Before diving into data collection or whirring up a GPU for model training, it's crucial to have a clear and well-defined research question from the client.The question provided by the client will shape every aspect of the project, from the selection of data sources to the design of labelling schemes and, ultimately, the type of model that is used

Cassie Kozyrkov (former Chief Decision Scientist at Google) brilliantly describes machine learning as "A thing labeller" - a tool that helps make a series of small decisions within the data. For example: Is this email spam? Should we invest more in this campaign? If our task involves labelling, categorizing, or making a decision based on patterns in data, then machine learning may be a good fit. However, if the correct answer can be looked up each time, or if decisions don't involve subtle patterns in large datasets, ML is likely unnecessary.

### Does the Task Need Machine Learning?

One of the most common misconceptions is that ML is a universal solution, often seen as ‚Äúmagical‚Äù by those unfamiliar with its limitations. However, ML isn't always the right tool for the job. Before we consider building models, we need to be able to clearly articulate what success looks like. If the goals and metrics are not defined, or if there isn't a need for learning from complex patterns in large amounts of data, ML may not be the answer. ML is not magic, though in many businesses (and non data savvy people, unlike yourself) it is considered okay to skip thinking about what it means to do the task well.

For ML to add value, you should be able to answer the following questions before starting:

* **What does it mean to do the task correctly?** If the task's outcome isn't measurable or there's no clear definition of what success looks like, an ML solution will struggle to be effective.
* **Which mistakes are worse than which other mistakes?** In many ML tasks, errors are inevitable, but not all errors are equally harmful. Before building a model, it's essential to rank the mistakes and decide which ones are more tolerable than others.
* **How will we measure the performance at scale?** Imagine 1,000 units of work are completed, some of them imperfectly. You need a well-defined method to evaluate the overall performance of that work. Without an evaluation metric, it's impossible to gauge success or failure meaningfully.

#### Best Practices for Deciding Whether ML is Needed

	1.	Is the problem deterministic?
If a task can be solved by looking up known answers or using rule-based logic, you don't need ML. ML is beneficial when patterns need to be learned from data, particularly when those patterns are complex, subtle, or change over time.
	2.	Can a non-ML solution address the problem efficiently?
Consider whether simpler approaches such as basic statistics, heuristics, or existing automation tools can solve the problem. ML should only be used when it offers a clear advantage over these simpler methods.
	3.	Do we have enough data, and is it labelled correctly?
ML models require data, and lots of it, especially for tasks like classification. Moreover, labelled data (with clear examples of the desired output) is crucial for supervised learning. If the data is scarce or poorly labelled, you may need to reconsider whether ML is a viable approach.
	4.	Can we measure success?
Defining evaluation metrics is a must before starting any ML project. If it's impossible to clearly measure how well a model performs (e.g., accuracy, F1 score, precision-recall, etc.), reconsider whether ML is necessary or if the problem is well-formed.

`automating the ineffable`

## Data Labelling and Collection

## Model Training and Fine-Tuning

Now that you have your data, the next step is selecting the appropriate model. When working with predefined categories (e.g. sentiment), starting with out-of-the-box (OOTB) models that has already been fine-tuned by others to perform the task at hand might be sufficient. However, given the nature of both social media and our specific research, these models may not fully understand specific nuances (e.g., sarcasm, platform-specific language) without additional fine-tuning.

For this section, let's assume we need to perform sentiment classification, and the reason we cannot use an OOTB sentiment classification model is because we are classifying data from a novel social media platform that has very unique language style (for example, this could be from a gaming forum where the phrase "that is sick" is actually positive, or from a thread on a new song that is described as "savage"). Whilst these are overly simplified example, the sentiment (ü•Å) still stands.

I am going to explore three distinct approaches to text classification, each offering different levels of complexity, resource requirements, and suitability based on the size of your dataset and the task at hand.

1. **Logistic Regression:** A simple, interpretable classification model that is trained from scratch using basic feature extraction techniques like TF-IDF or Bag-of-Words. Logistic regression is well-suited for straightforward classification tasks and small datasets where interpretability is important.
2. **SetFit:** A framework designed for few-shot learning, where limited labelled data is available. SetFit leverages pre-trained sentence transformers to generate embeddings, fine-tuning a lightweight classifier on top of these embeddings, making it ideal when you need quick results with minimal data.
3. **Vanilla Fine-Tuning with Hugging Face Trainer:** The most powerful of the three, this approach fine-tunes large pre-trained language models like BERT on task-specific datasets. It's best used when you have access to larger datasets and need high accuracy and deep contextual understanding.

### Fine-Tuning a Model

Fine-tuning involves taking a pre-trained model and retraining it on your specific dataset. How we go about this can be approached differently depending on the amount of labelled data and the complexity of the problem.

#### Method 1: Logistic Regression

Logistic regression is a statistic method used for binary or multi-class classification problems. In the context of NLP it involves:

1. Feature extraction: Transform textual data into numerical features
2. Model training: Fitting a logistic regression model to these features
3. Prediction: Using the trained model to classify new and unseen text instances

The core idea behind using logistic regression for text classification is to find the best-fitting model to describe the relationship between a categorical dependent variable and one or more independent variables (features extracted from text).

* **Simplicity:** Logistic regression is straightforward and interpretable.
* **Baseline Performance:** Often used as a baseline to compare against more complex models.
* **Feature-Based:** Relies heavily on the quality of feature extraction.

#### Method 2: SetFit

SetFit is a method for fine-tuning Sentence Transformers for classification tasks with limited labelled data. It involves:

1. Pre-trained Sentence Transformer: Starting with a model like SBERT (Sentence-BERT).
2. Few-Shot Learning: Using a small number of labelled examples per class.
3. Contrastive Learning: Fine-tuning the model using contrastive loss functions.
4. Classification Head: Adding a simple classifier on top of the embeddings.

Benefits
* Resource Efficient: Less computationally intensive that full fine-tuning of large models
* Quick iterations: Faster training times allow for rapid experimentation

Limitations
* Performance ceiling: May not match the performance of models fine-tuned on large datasets
* Dependence of pre-trained model quality: The quality of embeddings is tied to the pre-trained model used. 

#### How to do it?

Start by installing SetFit

```{python}
#| eval: false
pip install setfit
```

Then initialise a SetFit model using a Sentence Transformer model of our choice. For this example we will use `BAAI/bge-small-en-v1.5`:

```{python}
#| eval: false
from setfit import SetFitModel

model = SetFitModel.from_pretrained("BAAI/bge-small-en-v1.5")
```

Then we will load in our data. The benefit of SetFit is being able to perform model fine-tuning with very few labelled data. As such, we will load in data from the SetFit library, but will sample it so we only keep 8 (yes 8!) instances of each label for fine-tuning. Note the dataset provided is already split up into training, testing, and validation sets (and it is the training set we will be sampling). The testing set is left unaffected for better evaluation.

```{python}
#| eval: false
from datasets import load_dataset

dataset = load_dataset("SetFit/sst2")

train_dataset = sample_dataset(dataset["train"], label_column="label", num_samples=8)
train_dataset

test_dataset = dataset["test"]
test_dataset
```

We also apply labels from the dataset to the model, so the predictions output are readable (rather than something like 0 or 1)

```{python}
#| eval: false
model.labels = ["negative", "positive"]
```

Now we prepare `TrainingArguments` for training- the most frequently used arguments (hyperparamters) are `num_epochs` and `max_steps` which affect the number of total training steps. We then initialise the `Trainer` and perform the training

```{python}
#| eval: false
from setfit import TrainingArguments

args = TrainingArguments(
    batch_size=32,
    num_epochs=10,
)

from setfit import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
)

trainer.train()
```

Finally we can evaluate using the testing dataset

```{python}
#| eval: false
trainer.evaluate(test_dataset)
```

Now we can save (or load) the model as needed

```{python}
#| eval: false
model.save_pretrained("setfit-bge-small-v1.5-sst2-8-shot") # Save to a local directory

model = SetFitModel.from_pretrained("setfit-bge-small-v1.5-sst2-8-shot") # Load from a local directory
```

Once a SetFit model has been trained, it can be used for inference straight away using `SetFitModel.predict()`

```{python}
#| eval: false
preds = model.predict([
    "It's a charming and often affecting journey.",
    "It's slow -- very, very slow.",
    "A sometimes tedious film.",
])

preds
```

#### Method 3: Hugging Face Trainer API

The Hugging Face Trainer API is a high-level interface for training and fine-tuning Transformer models. The process involves:

1. Model Selection: Choosing a pre-trained Transformer model (e.g., BERT, RoBERTa).
2. Data Preparation: Tokenizing and formatting data according to model requirements.
3. Configuration: Setting up training parameters and hyperparameters.
4. Training Loop: Using the Trainer API to handle the training process.
5. Evaluation: Assessing model performance using built-in methods.

The Hugging Face Trainer API simplifies the fine-tuning of large Transformer models by abstracting the training loop and providing utilities for data handling, optimisation, and evaluation.

Benefits
* Ease of Use: Simplifies complex training procedures.
* Flexibility: Supports a wide range of models and tasks.
* Comprehensive: Handles distributed training, mixed precision, and more.

Limitations
* Computational Resources: Training large models requires significant computational power.
* Complexity: Despite simplifications, understanding underlying mechanics is beneficial.
* Data Requirements: Performs best with larger datasets.

* **When to Fine-Tune LLMs?** 
  * When you have more labelled data, it's worth using a full transformer model and fine-tuning it using the Huggingface Trainer API
  * Models such as BERT, DistilBERT, or RoBERTa can be adapted for specific classification tasks like sentiment analysis, emotion detection, or custom categories.
  
#### How to do it?

We will first load in our dataset. Similar to the SetFit example above, we will load from the datasets library the sst2 dataset (Stanford Sentiment Treebank)

```{python}
#| eval: false
from datasets import load_dataset

dataset = load_dataset("stanfordnlp/sst2")

dataset["train"][100]
```

We then need to tokenize the text to be able to process the text for fine-tuning (as well as adding padding and truncation to enable us to handle variable sequence lengths). We can do this with one step using `map`:

```{python}

```


#### Logistic Regression


  
### Choosing the Right Pre-Trained Model

Selecting an appropriate starting point is crucial.

* Model Selection: Choose models known to perform well in NLP tasks (e.g., BERT, RoBERTa).
* Domain Relevance: Prefer models pre-trained on data similar to your domain if available.

## Evaluation

## Systematically improving