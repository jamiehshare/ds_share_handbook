{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  eval: false\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Method 2: SetFit\n",
        "\n",
        "When working with limited labelled data^[See the page on data labelling for our advice on knowing when you have limited data - rule of thumb here is < 100 posts per class], [traditional fine-tuning methods](./vanilla_finetuning.qmd) can be resource-intensive and less effective. SetFit is a framework that offers an efficient and prompt-free approach for few-shot text classification. It leverages sentence transformers and contrastive learning to achieve high accuracy with minimal data.\n",
        "\n",
        "At a high level, the SetFit process involves:\n",
        "\n",
        "1. **Model Selection**: Choosing a pre-trained sentence transformer (e.g., paraphrase-mpnet-base-v2).\n",
        "2. **Data Preparation**: Formatting your labelled examples.\n",
        "3. **Contrastive Fine-Tuning**: Generating sentence pairs and fine-tuning the model using contrastive learning.\n",
        "4. **Classifier Training**: Training a classification head on the embeddings produced by the fine-tuned model.\n",
        "5. **Evaluation**: Assessing model performance on a validation and finally test sets.\n",
        "\n",
        "SetFit is a method for fine-tuning Sentence Transformers for classification tasks with limited labelled data. It involves:\n",
        "\n",
        "1. Pre-trained Sentence Transformer: Starting with a model like SBERT (Sentence-BERT).\n",
        "2. Few-Shot Learning: Using a small number of labelled examples per class.\n",
        "3. Contrastive Learning: Fine-tuning the model using contrastive loss functions.\n",
        "4. Classification Head: Adding a simple classifier on top of the embeddings.\n",
        "\n",
        "**Benefits**\n",
        "\n",
        "* Data Efficiency: Achieves good performance with as few as 8 examples per class.\n",
        "* Computationally Light: Fine-tunes quickly and requires less computational power.\n",
        "* No Prompts Needed: Eliminates the need for hand-crafted prompts.\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "* Performance ceiling: May not match the performance of models fine-tuned on large datasets\n",
        "* Dependence of pre-trained model quality: The quality of embeddings is tied to the pre-trained model used. \n",
        "\n",
        "**When to Use SetFit?**\n",
        "\n",
        "* If you have a small amount of labelled data.\n",
        "* For quick prototyping and iterative development (if time allows, give SetFit a go first and if it looks promising then it's worth labelling up more data to perform [vanilla fine-tuning](./vanilla_finetuning.qmd)).\n",
        "\n",
        "## How to fine-tune a model with SetFit?\n",
        "\n",
        "Let's dive into fine-tuning a model using SetFit. This section will get you started quickly. Feel free to run the code, experiment, and learn by doing. After this walkthrough, we'll provide a more detailed explanation of each step.\n",
        "\n",
        "Start by installing the required packages/modules...\n"
      ],
      "id": "b3b4de23"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install setfit datasets"
      ],
      "id": "3ab863f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... before loading in our dataset. For this example, we'll use the `sst2` (Stanford Sentiment Treebank) dataset, which is great for sentiment analysis as it is single sentences extracted from movie reviews that have been annotated as either positive or negative. \n"
      ],
      "id": "1378c941"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"SetFit/sst2\")"
      ],
      "id": "dd949765",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare the data\n",
        "\n",
        "Now we have loaded in the data, let's prepare it for the SetFit framework. The benefit of SetFit is being able to perform model fine-tuning with very few labelled data. As such, we will load in data from the SetFit library, but will sample it so we only keep 8 (yes 8!) instances of each label for fine-tuning to simulate a few-shot learning scenario. Note the dataset provided is already split up into training, testing, and validation sets (and it is the training set we will be sampling). The testing set is left unaffected for better evaluation.\n"
      ],
      "id": "15cfc1dc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use 8 examples per class for training\n",
        "train_dataset = sample_dataset(dataset[\"train\"], label_column=\"label\", num_samples=8)\n",
        "\n",
        "# Obtain the validation and test datasets\n",
        "validation_dataset = dataset[\"validation\"]\n",
        "test_dataset = dataset[\"test\"]"
      ],
      "id": "24bc3df1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading a Pre-trained SetFit Model\n",
        "\n",
        "Then initialise a SetFit model using a Sentence Transformer model of our choice. For this example we will use `BAAI/bge-small-en-v1.5`:\n"
      ],
      "id": "d2638071"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from setfit import SetFitModel\n",
        "\n",
        "model = SetFitModel.from_pretrained(\"BAAI/bge-small-en-v1.5\",\n",
        "labels=[\"negative\", \"positive\"])"
      ],
      "id": "56cad4f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we prepare `TrainingArguments` for training- the most frequently used arguments (hyperparamters) are `num_epochs` and `max_steps` which affect the number of total training steps. We then initialise the `Trainer` and perform the training\n"
      ],
      "id": "07279dd0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from setfit import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    batch_size=32,\n",
        "    num_epochs=10,\n",
        ")\n",
        "\n",
        "from setfit import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "id": "bc645882",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating the Model\n",
        "\n",
        "After training, evaluate the model on the validation dataset.\n"
      ],
      "id": "db40d12f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "metrics = trainer.evaluate(validation_dataset)\n",
        "print(metrics)"
      ],
      "id": "7091f5b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally once we are happy with model performance based on the validation data, we can evaluate using the testing dataset.\n"
      ],
      "id": "cc1cdf2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "id": "0e73fc55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can save (or load) the model as needed\n"
      ],
      "id": "cfce6563"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.save_pretrained(\"setfit-bge-small-v1.5-sst-8-shot\") # Save to a local directory\n",
        "\n",
        "model = SetFitModel.from_pretrained(\"setfit-bge-small-v1.5-sst-8-shot\") # Load from a local directory"
      ],
      "id": "6bd807b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once a SetFit model has been trained, it can be used for inference straight away using `SetFitModel.predict()`\n"
      ],
      "id": "9904833c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "texts = [\n",
        "    \"I love this product! It's fantastic.\",\n",
        "    \"Terrible customer service. Very disappointed.\",\n",
        "]\n",
        "\n",
        "predictions = trainer.model.predict(texts)\n",
        "print(predictions)"
      ],
      "id": "aaaabcb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Congratulations! You've fine-tuned a SetFit model for sentiment analysis. Feel free to tweak the code, try different datasets, and explore further.\n",
        "\n",
        "## Detailed overview\n",
        "\n",
        "Now that you've got a taste of how SetFit works, let's delve deeper into each step.\n",
        "\n",
        "### Setting Up the Environment\n",
        "\n",
        "Despite SetFit being lightweight, we still recommend you running it in a cloud environment like Google Colab to access the GPUs\n",
        "\n",
        "As such, make sure you are connected to a GPU, we recommend T4 as it's a good balance between speed and cost.\n",
        "\n",
        "::: {.callout-tip collapse=\"true\"}\n",
        "# How do I do this?\n",
        "\n",
        "To use a GPU in Colab, go to `Runtime` >` Change runtime type` and select a GPU under the hardware accelerator option\n",
        ":::\n",
        "\n",
        "### Install the required packages and modules\n"
      ],
      "id": "a20d4a7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%capture\n",
        "# Install  necessary packages\n",
        "!pip install setfit datasets evaluate\n",
        "\n",
        "# Imports \n",
        "from datasets import load_dataset"
      ],
      "id": "c3111fa7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = load_dataset(\"SetFit/sst2\")"
      ],
      "id": "5b768089",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "rag_hf_tutorial",
      "language": "python",
      "display_name": "Python (RAG_hf_tutorial)",
      "path": "/Users/jamiehudson/Library/Jupyter/kernels/rag_hf_tutorial"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}